[
["index.html", "Science des données biologiques Préambule", " Science des données biologiques Philippe Grosjean &amp; Guyliann Engels avec des contributions de Raphael Conotte 2020-08-10 Préambule Cet ouvrage a été écrit pour le cours de science des données I : inférence et visualisation à Mons et le cours de Bio-informatique et Sciences des Données à Charleroi pour l’année académique 2019-2020 (UMONS). Afin de trouver la dernière version disponible de cet ouvrage suivez le lien suivant : - http://biodatascience-course.sciviews.org/sdd-umons/ Cet ouvrage interactif est le premier volume d’une série de trois ouvrages traitant de la science des données biologiques. L’écriture de cette suite de livres a débuté au cours de l’année académique 2018-2019. Pour l’année académique 2019-2020, cet ouvrage interactif est le support des cours suivants : Science des données I : Visualisation et inférence, UMONS dont le responsable est Grosjean Philippe Bio-informatique et sciences des données, UMONS-ULB dont le responsable est Conotte Raphael Cet ouvrage est conçu pour être utilisé de manière interactive en ligne. En effet, nous y ajoutons des vidéos, des démonstrations interactives ainsi que des exercices sous forme de questionnaires interactifs. Ces différents éléments ne sont, bien évidemment, utilisables qu’en ligne. Le matériel dans cet ouvrage est distribué sous licence CC BY-NC-SA 4.0. "],
["vue-generale-des-cours.html", "Vue générale des cours", " Vue générale des cours Le cours de Science des données I: visualisation et inférence est dispensé aux biologistes de second Bachelier en Faculté des Sciences de l’Université de Mons à partir de l’année académique 2018-2019. Le cours de Bio-informatique et sciences des données, UMONS-ULB est dispensé au biologistes de second Bachelier en Faculté des Sciences à Charleroi en co-diplomation entre l’Université de Mons (UMONS) et l’Université Libre de Bruxelles (ULB) à partir de l’année académique 2019-2020. La matière est divisée en 12 modules de sessions de 6h chacuns en présentiel. Il nécessitera environ un tiers de ce temps (voir plus, en fonction de votre rythme et de votre technique d’apprentissage) en travail à domicile. Une première séance de 2h précèdera ces 12 modules afin d’installer les logiciels (SciViews Box, R, RStudio, Github Desktop), et de se familiariser avec eux. "],
["materiel-pedagogique.html", "Matériel pédagogique", " Matériel pédagogique Le matériel pédagogique rassemblé dans ce syllabus interactif est aussi varié que possible. Vous pourrez ainsi piocher dans l’offre en fonction de vos envies et de votre profil d’apprenant pour optimiser votre travail. Vous trouverez: le présent ouvrage en ligne, des tutoriaux interactifs (réalisés avec un logiciel appelé learnr) que vous pourrez exécuter directement sur votre ordinateur, et qui vous donnerons accès à des pages Web réactives contenant des explications, des exercices et des quizzs en ligne, des slides de présentations, des dépôts Github Classroom dans la section BioDataScience-Course (vous apprendrez ce que c’est très rapidement dès le premier module) pour réaliser et documenter vos travaux personnels. des renvois vers des documents externes en ligne, types vidéos youtube ou vimeo, des ouvrages en ligne en anglais ou en français, des blogs, des tutoriaux, des parties gratuites de cours Datacamp ou équivalents, des questions sur des sites comme “Stackoverflow” ou issues des “mailing lists” R, … Tout ce matériel est accessible à partir du site Web du cours, du présent syllabus interactif (et de Moodle pour les étudiants de l’UMONS et de Charleroi). Ces derniers ont aussi accès au dossier SDD sur StudentTemp en Intranet à l’UMONS. Les aspects pratiques seront à réaliser en utilisant la ‘SciViews Box’, une machine virtuelle préconfigurée que nous installerons ensemble lors du premier cours1. Il vous faudra donc avoir accès à un ordinateur (sous Windows, MacOS, ou Linux peu importe, suffisamment puissant et connecté à Internet ou à l’Intranet UMONS). Enfin, vous pourrez poser vos questions par mail à l’adresse sdd@sciviews.org. Il existe tout de même des outils plus pointus pour obtenir de l’aide sur le logiciel R comme rseek.org, rdocumentation.org ou rdrr.io. Rien ne sert de chercher ’R’ dans Goggle.↩ "],
["comment-apprendre.html", "Comment apprendre?", " Comment apprendre? fortunes::fortune(&quot;brain surgery&quot;) # # I wish to perform brain surgery this afternoon at 4pm and don&#39;t know where # to start. My background is the history of great statistician sports # legends but I am willing to learn. I know there are courses and numerous # books on brain surgery but I don&#39;t have the time for those. Please direct # me to the appropriate HowTos, and be on standby for solving any problem I # may encounter while in the operating room. Some of you might ask for # specifics of the case, but that would require my following the posting # guide and spending even more time than I am already taking to write this # note. # -- I. Ben Fooled (aka Frank Harrell) # R-help (April 1, 2005) Version courte: en pratiquant et en faisant des erreurs ! Version longue: aujourd’hui –et encore plus à l’avenir– les données sont complexes et ne se manipulent plus simplement avec un tableur comme Microsoft Excel. Vous apprendrez donc à maitriser des outils professionnels très puissants mais aussi relativement complexes. La méthode d’apprentissage que nous vous proposons a pour objectif prioritaire de vous faciliter la tâche, quelles que soient vos aptitudes au départ. Envisagez votre voyage en science des données comme l’apprentissage d’une nouvelle langue. C’est en pratiquant, et en pratiquant encore sur le long terme que vous allez progresser. Pour vous aider dans cet apprentissage progressif et sur la durée, la formation s’étale sur quatre années, et est répartie en cinq cours de difficulté croissante. N’hésitez pas à expérimenter, tester ou essayer des nouvelles idées (même au delà de ce qui sera demandé dans les exercices) et surtout, n’ayez pas peur de faire des erreurs. Vous en ferez, … beaucoup … nous vous le souhaitons! La meilleure manière d’apprendre, c’est en faisant des erreurs et en mettant ensuite tout en oeuvre pour les comprendre et les corriger. Donc, si un message d’erreur, ou un “warning” apparait, ne soyez pas intimidé. Prenez une bonne respiration, lisez-le attentivement, essayez de le comprendre, et au besoin faites-vous aider: la solution est sur le Net, ‘Google1 est votre ami’! System information sessioninfo::session_info() # ─ Session info ────────────────────────────────────────────────────────── # setting value # version R version 3.5.3 (2019-03-11) # os Ubuntu 18.04.2 LTS # system x86_64, linux-gnu # ui X11 # language (EN) # collate en_US.UTF-8 # ctype en_US.UTF-8 # tz Europe/Brussels # date 2020-08-10 # # ─ Packages ────────────────────────────────────────────────────────────── # package * version date lib source # assertthat 0.2.1 2019-03-21 [2] CRAN (R 3.5.3) # bookdown 0.9 2018-12-21 [2] CRAN (R 3.5.3) # brew 1.0-6 2011-04-13 [2] CRAN (R 3.5.3) # cli 1.1.0 2019-03-19 [2] CRAN (R 3.5.3) # codetools 0.2-16 2018-12-24 [2] CRAN (R 3.5.3) # colorspace 1.4-1 2019-03-18 [2] CRAN (R 3.5.3) # crayon 1.3.4 2017-09-16 [2] CRAN (R 3.5.3) # DiagrammeR 1.0.0 2018-03-01 [2] CRAN (R 3.5.3) # digest 0.6.18 2018-10-10 [2] CRAN (R 3.5.3) # downloader 0.4 2015-07-09 [2] CRAN (R 3.5.3) # dplyr 0.8.0.1 2019-02-15 [2] CRAN (R 3.5.3) # evaluate 0.13 2019-02-12 [2] CRAN (R 3.5.3) # farver 1.1.0 2018-11-20 [2] CRAN (R 3.5.3) # fortunes 1.5-4 2016-12-29 [2] CRAN (R 3.5.3) # gganimate 1.0.3 2019-04-02 [2] CRAN (R 3.5.3) # ggplot2 3.1.1 2019-04-07 [2] CRAN (R 3.5.3) # glue 1.3.1 2019-03-12 [2] CRAN (R 3.5.3) # gridExtra 2.3 2017-09-09 [2] CRAN (R 3.5.3) # gtable 0.3.0 2019-03-25 [2] CRAN (R 3.5.3) # hms 0.4.2 2018-03-10 [2] CRAN (R 3.5.3) # htmltools 0.3.6 2017-04-28 [2] CRAN (R 3.5.3) # htmlwidgets 1.3 2018-09-30 [2] CRAN (R 3.5.3) # igraph 1.2.4 2019-02-13 [2] CRAN (R 3.5.3) # influenceR 0.1.0 2015-09-03 [2] CRAN (R 3.5.3) # inline 0.3.15 2018-05-18 [2] CRAN (R 3.5.3) # jsonlite 1.6 2018-12-07 [2] CRAN (R 3.5.3) # knitr 1.22 2019-03-08 [2] CRAN (R 3.5.3) # lazyeval 0.2.2 2019-03-15 [2] CRAN (R 3.5.3) # magick 2.0 2018-10-05 [2] CRAN (R 3.5.3) # magrittr 1.5 2014-11-22 [2] CRAN (R 3.5.3) # munsell 0.5.0 2018-06-12 [2] CRAN (R 3.5.3) # pillar 1.3.1 2018-12-15 [2] CRAN (R 3.5.3) # pkgconfig 2.0.2 2018-08-16 [2] CRAN (R 3.5.3) # plyr 1.8.4 2016-06-08 [2] CRAN (R 3.5.3) # prettyunits 1.0.2 2015-07-13 [2] CRAN (R 3.5.3) # progress 1.2.0 2018-06-14 [2] CRAN (R 3.5.3) # purrr 0.3.2 2019-03-15 [2] CRAN (R 3.5.3) # R6 2.4.0 2019-02-14 [2] CRAN (R 3.5.3) # RColorBrewer 1.1-2 2014-12-07 [2] CRAN (R 3.5.3) # Rcpp 1.0.1 2019-03-17 [2] CRAN (R 3.5.3) # readr 1.3.1 2018-12-21 [2] CRAN (R 3.5.3) # rgexf 0.15.3 2015-03-24 [2] CRAN (R 3.5.3) # rlang 0.3.4 2019-04-07 [2] CRAN (R 3.5.3) # rmarkdown 1.12 2019-03-14 [2] CRAN (R 3.5.3) # Rook 1.1-1 2014-10-20 [2] CRAN (R 3.5.3) # rstudioapi 0.10 2019-03-19 [2] CRAN (R 3.5.3) # scales 1.0.0 2018-08-09 [2] CRAN (R 3.5.3) # sessioninfo 1.1.1 2018-11-05 [2] CRAN (R 3.5.3) # stringi 1.4.3 2019-03-12 [2] CRAN (R 3.5.3) # stringr 1.4.0 2019-02-10 [2] CRAN (R 3.5.3) # tibble 2.1.1 2019-03-16 [2] CRAN (R 3.5.3) # tidyr 0.8.3 2019-03-01 [2] CRAN (R 3.5.3) # tidyselect 0.2.5 2018-10-11 [2] CRAN (R 3.5.3) # tweenr 1.0.1 2018-12-14 [2] CRAN (R 3.5.3) # viridis 0.5.1 2018-03-29 [2] CRAN (R 3.5.3) # viridisLite 0.3.0 2018-02-01 [2] CRAN (R 3.5.3) # visNetwork 2.0.6 2019-03-26 [2] CRAN (R 3.5.3) # withr 2.1.2 2018-03-15 [2] CRAN (R 3.5.3) # xfun 0.6 2019-04-02 [2] CRAN (R 3.5.3) # XML 3.98-1.19 2019-03-06 [2] CRAN (R 3.5.3) # yaml 2.2.0 2018-07-25 [2] CRAN (R 3.5.3) # # [1] /home/sv/R/x86_64-pc-linux-gnu-library/3.5 # [2] /usr/local/lib/R/site-library # [3] /usr/lib/R/site-library # [4] /usr/lib/R/library Il existe tout de même des outils plus pointus pour obtenir de l’aide sur le logiciel R comme rseek.org, rdocumentation.org ou rdrr.io. Rien ne sert de chercher ’R’ dans Goggle.↩ "],
["intro.html", "Module 1 Introduction", " Module 1 Introduction Objectifs Appréhender ce qu’est la science des données et les (bio)statistiques. S’initier à des outils de base (SciViews Box, RStudio, Markdown, Git, GitHub). "],
["donnees.html", "1.1 Le monde il y a 25 ans", " 1.1 Le monde il y a 25 ans Il y a 25 ans, pas d’internet, pas de smartphone. Essayez d’imaginer ce que serait votre vie aujourd’hui si ces outils qui font partie de votre quotidien n’existaient pas. Les révolutions industrielles: 1770 (1756) révolution 1: mécanisation 1870 révolution 2: maîtrise de l’énergie 1970 (1979) révolution 3: informatique 1990 révolution 4: internet (1990 Web, 1992 ISOC = Internet society, 1993 = premier navigateur web) 2000 révolution 5: numérique. GAFA = Google - Apple - Facebook - Amazon + Microsoft = GAFAM aux USA et BATX en Chine = Baidu - Alibaba - Tencent - Xiaomi. Aussi NATU = Netflix - Airbnb - Tesla - Uber. 2010 révolution 6: NBIC = nanotechnologies - biotechnologies - informatique - sciences cognitives. 2020 = date prévue pour que l’ordinateur ait la même puissance de traitement de l’information que le cerveau humain 2030 = trans-humanisme: ordinateur plus puissant que l’homme et le remplacera probablement dans de nombreuses tâches. Valeur estimée des données et informations mises à disposition par les utilisateurs du net: 1000 milliards de dollar par an (écrivez ce nombre en chiffres pour vous donner une meilleure idée de ce que cela représente) ! En 2020, quantité d’information ajoutée sur le net: 1000 milliards de milliards par semaine (écrivez ce nombre en chiffres également). Comparaison de puissance de traitement du cerveau humain versus un ordinateur: 89 milliards de neurones, mais travail en multitâche alors qu’un processeur est monotâche =&gt; difficile à comparer. Une étude a montré en 2017 que l’un des 5 ordinateurs les plus puissants a été capable de simuler le fonctionnement d’environ 1% du cerveau humain en 1 sec. Il lui a fallu 40 min de calcul pour y arriver. Intel estime que l’évolution permettra d’égaler le cerveau humain en terme de vitesse de traitement vers 2020. Consommation électrique du supercalculateur: se mesure en mégawatts, alors que le cerveau humain consomme 12-13W seulement! “Le transhumanisme est une approche interdisciplinaire qui nous amène à comprendre et à évaluer les avenues qui nous permettrons de surmonter nos limites biologiques par les progrès technologiques. Les trans-humanistes cherchent à développer les possibilités techniques afin que les gens vivent plus longtemps et …” Vous pouvez maintenant avoir un aperçu de l’importance d’avoir des outils performants afin d’appréhender les données dont le nombre croit de manière exponentielle. Pour ce cours de sciences des données, plusieurs outils puissants sont mis à votre disposition (Vous trouverez sur l’hyperlien suivant, un poster présentant la philosophie du cours https://github.com/BioDataScience-Course/RencontresRRennes2018) "],
["decouverte-des-outils.html", "1.2 Découverte des outils", " 1.2 Découverte des outils La science des données est complexe et requiert d’employer des outils performants que nous avons sélectionnés pour vous. 1.2.1 Machine virtuelle La SciViews Box est une machine virtuelle (un ordinateur complet, mais totalement indépendant du matériel -le hardware- et qui peut être déployé sur pratiquement n’importe quel ordinateur physique). Cette SciViews Box est complètement configurée et dédiée à la sciences des données biologiques. Elle contient tout ce qu’il faut pour importer et analyser vos données, et ensuite écrire des rapports ou d’autres documents prêts à publication ou à présentation. Elle vous servira également à collaborer avec d’autres chercheurs qui peuvent facilement utiliser exactement la même machine virtuelle (aspect reproductible de vos analyses). Des explications détaillées se trouvent dans l’annexe A dédiée à l’installation, la configuration et l’utilisation de la SciViews Box. Figure 1.1: Logo de la SciViews Box Une fois connecté au compte sv dans la machine virtuelle, réalisez l’activité : Découverte de la machine virtuelle https://github.com/BioDataScience-Course/sdd_lesson/blob/2019-2020/sdd1_01/presentations/sdd1_01_svbox.pdf Après avoir réalisé l’activité, un document récapitulatif est mis à votre disposition : https://htmlpreview.github.io/?https://github.com/BioDataScience-Course/sdd_lesson/blob/2019-2020/sdd1_01/exercises/sdd1_01_svbox.html 1.2.2 RStudio RStudio est l’outil au sein de la SciViews Box que vous allez utiliser le plus fréquemment durant ce cours. Il fournit un environnement complet et optimisé pour réaliser vos analyses, vos graphiques et vos rapports. RStudio travaille main dans la main avec le logiciel R qui effectue l’ensemble des traitements. L’interface utilisateur de RStudio est divisée en quatre zones importantes (A-D) avec une barre d’outils générale par dessus : A. Une zone d’édition B. Plusieurs onglets sont présents comme Environment, History ou encore Connections. Par exemple, les différents items (on parle d’objets) chargés en mémoire dans R sont visibles dans l’onglet Environment (mais pour l’instant, il n’y a encore rien). C. La Console est l’endroit où vous pouvez entrer des instructions dans R pour manipuler vos données D. Une zone multiusage où vous pouvez manipuler vos fichiers (Files), vos graphiques (Plots), les différents “addins” de R (on parle de Packages), accéder aux pages d’aide (Help) ou encore, visualiser le rendu final de vos rapports (Viewer). Des explications détaillées se trouvent dans l’annexe B.1 qui présente les bases de l’utilisation de RStudio. Vous avez également à votre disposition un aide-mémoire afin d’appréhender cette interface RStudio IDE Cheat Sheet. Pour en savoir plus RStudio. Site Web de RStudio comprenant un ensemble de ressources en anglais RStudio, un environnement de développement pour R. Brève explication de RStudio en français. RStudio : sa vie, son oeuvre, ses ressources. Un autre site Web consacré à RStudio en français. 1.2.3 Markdown Dans RStudio, les rapports sont rédigés en utilisant le langage Markdown dans la zone d’édition. Il permet de baliser le texte pour indiquer le sens des différentes parties (par exemple, pour indiquer les différents niveaux de titres) et de se concentrer sur l’écriture dans un premier temps en dissociant le fond de la mise en forme. En effet, vous vous préoccupez de l’aspect final du document dans un second temps, et même, vous pouvez changer radicalement d’avis pratiquement sans rien changer dans le texte (par exemple, il est possible de passer d’une page Web à un document PDF ou Word, ou même encore à une présentation). Markdown est relativement simple et intuitif à l’usage, même si un petit effort est nécessaire, naturellement, au début. Quels sont les commandes et instructions indispensables lorsque l’on rédige un rapport ? Des titres et sous-titres, une mise en évidence (texte en italique ou en gras), des listes,… Il ne faut au final que très peu de commandes pour réaliser un rapport de qualité avec une mise en page sobre et épurée qui caractérise les travaux professionnels. Vous avez à votre disposition deux aide-mémoires pour apprendre Markdown : R Markdown Cheat Sheet et R Markdown Reference Guide plus détaillé. Après avoir rédigé votre document, vous devez cliquer sur le bouton Preview ou Knit (selon le type de document édité) dans la barre d’outils de la zone d’édition pour obtenir la version finale formatée. Pour en savoir plus Markdown. Explication en anglais de l’intérêt d’employer Markdown ainsi que la syntaxe à employer. Rédigez en Markdown ! Un guide pour bien commencer avec Markdown Le Markdown comme langage d’écriture universel ? Comment écrire confortablement et professionnellement ? Le Markdown !. Utilisation de Markdown afin de revenir à l’essence de la rédaction. Écrire tout simplement – Introduction à Markdown. Pourquoi utiliser Markdown ? 1.2.4 Gestionnaire de version Lors de la rédaction de travaux un petit peu conséquents, comme un travail de fin d’étude, une publication scientifique ou un rapport volumineux, on se retrouve rapidement avec plusieurs fichiers correspondant à des états d’avancements du travail : TFE_final TFE_final1 TFE_final2 TFE_final3 TFE_final… TFE_final99 Lors de différents essais, on aura tendance à tout garder dans différents fichiers afin de ne rien supprimer d’important. Cette pratique bien que très courante comporte le gros désavantage de prendre énormément de place sur le disque de votre ordinateur et de n’être pas pratique. Les questions suivantes peuvent se poser : Que se cache-t-il dans la version TFE_finalX ? Après un mois sans travailler sur le projet, seriez-vous encore capable de faire facilement la différence entre TFE_final2 et le TFE_final3 ? Cela se complique encore plus lorsque plusieurs personnes collaborent sur un même projet. Ils vont, par exemple, s’échanger par email différentes versions du travail avec chacun qui y place ses commentaires et modifie différentes parties du texte. Cela peut donner quelque chose comme ceci : TFE_final TFE_final1 TFE_final1_jacques TFE_final1_pierre TFE_final2 TFE_final2_jules TFE_final… TFE_final99 Dans quel fichier se trouve la dernière version de chaque personne ayant collaboré sur le projet ? Un petit peu dans différents fichiers, sans doute. Différents outils informatiques existent pour faciliter le travail collaboratif comme : Le partage de fichiers en ligne (Dropbox, Google Drive, One Drive). Ces espaces de stockage sur le “cloud” ne règlent toujours pas le problème de collaboration sur le même fichier. L’utilisation d’un programme d’édition collaboratif en temps réel (Etherpad, Google Drive - Docs, Gobby). Il est possible de travailler en même temps sur un même fichier. Cette option ne règle pas le problème du retour vers une ancienne version. Lorsqu’une modification a été réalisée l’ancienne version est tout simplement écrasée. La meilleure combinaison pour gérer ses versions et collaborer : Git et GitHub. Ces outils sont plutôt considérés comme écrits par et pour des geeks. Cependant, ils permettent de gérer et collaborer de manière efficace sur un même projet contenant du code ou non, et des interfaces facilitant leur utilisation apparaissent comme GitHub Desktop, ou même, les outils Git intégrés dans RStudio. 1.2.4.1 Git La gestion de versions est gérée par Git. Cet outil remplacera les nombreuses copies d’un même fichier par une sorte d’arbre que l’on peut représenter schématiquement comme ci-dessous : Représentation de la gestion de fichiers via Git Comme vous pouvez le voir ci-dessus, on peut suivre la progression de notre projet via un nombre d’étapes successives représentées sur le schéma par des boules bleues. Chaque étape capture l’état de notre projet au moment où nous avons décidé de l’enregistrer. Pour enregistrer une nouvelle version de votre projet, vous réalisez un commit qui sera accompagné d’un message spécifiant les modifications apportées. Git comprend de nombreux outils très intéressant pour la gestion de versions que vous utiliserez par la suite. 1.2.4.2 GitHub Un réseau social a été conçu autour de Git pour sauvegarder vos projets sur le “cloud”, les partager et collaborer avec d’autres personnes. Ce système se nomme GitHub (tout comme Facebook ou LinkedIn). GitHub rassemble donc “Git”, la gestion de version et “Hub” relatif au réseau. D’autres réseaux équivalents existent comme Gitlab ou Bitbucket, mais dans ce cours, nous utiliserons GitHub ensemble, sachant que les notions apprises ici seront réutilisables ailleurs. Une description plus détaillée de GitHub est présente dans l’annexe B.2 Lorsque l’on travaille seul tout en utilisant GitHub, l’évolution de notre projet ressemblera à l’arbre ci-dessous : Représentation des versions successives d’un projet avec GitHub. On réalise un envoi (push) lorsque l’on souhaite synchroniser nos changements locaux avec la version sur le “cloud”. Plusieurs commits peuvent être envoyés avec un seul push sur le réseau, et c’est d’ailleurs généralement comme cela que l’on procède. L’inverse (rapatrier localement les changements que d’autres collaborateurs ont envoyés sur la version réseau de notre projet) s’appelle faire un “pull”. L’avantage principal de GitHub ne réside pas vraiment dans la possibilité de réaliser une sauvegarde en ligne mais plutôt dans la possibilité de collaborer avec d’autres personnes présentes sur ce réseau comme l’illustre la figure ci-dessous. Deux scientifiques (les versions représentées par des boules bleues et des boules vertes) collaborent sur un même projet que l’on appelle un dépôt (repository en anglais) lorsqu’il est en ligne. Le premier chercheur (boules bleues) va initier le dépôt et réaliser un “push”&quot; pour rendre son travail accessible sur le réseau (boules oranges). Son collaborateur (boules vertes) va clôner (clone en anglais) le dépôt sur son ordinateur afin d’y travailler également en local sur son PC. Après avoir fait des changements, il réalise également un push sur le réseau. Le premier scientifique, avant de travailler à nouveau sur le projet, va donc réaliser un pull afin d’obtenir en local l’ensemble des modifications fournies par son ou ses collaborateurs, et ensuite après modifications en local il effectuera à nouveau un “push”. Différentes versions d’un projet sur GitHub lorsque deux personnes différentes collaborent sur le même dépôt. Vous venez d’apprendre le B-A-BA de la terminologie nécessaire à la bonne compréhension de Git et GitHub : repository : espace de stockage sous gestion de version Git. commit : enregistrer une version du projet. clone : créer un double local d’un dépôt GitHub. push : envoyer ses modifications locales vers le dépôt GitHub. pull : rapatrier les modifications que les autres utilisateurs ont appliquées dans le dépôt GitHub vers sa propre version locale. Ceci n’est qu’une explication très succincte. Vous trouverez plus de détails dans les liens ci-dessous et dans les Appendices. Il est, par exemple, possible de travailler sur une version en parallèle d’un dépôt original pour lequel on n’a pas de droits en écriture. Dans ce cas, il faudra faire une copie dans notre propre compte GitHub du dépôt. Cela s’appelle faire un fork. Il n’est pas possible de faire un push vers le dépôt d’origine puisqu’on n’a pas les droits en écriture. Dans ce cas, on fera un pull request, suggérant ainsi à l’auteur d’origine que nous avons fait des modifications qui pourraient l’intéresser. Si c’est effectivement le cas, il pourra accepter notre “pull request” et intégrer nos suggestions dans le dépôt d’origine. Vous serez amenés à “forker” des dépôts GitHub pour vos exercices, et vous effectuerez également un “pull request” lorsque vous serez suffisamment aguerris avec les autres techniques de gestion de vos projets sous Git et GitHub. Pour en savoir plus Gérez vos codes sources avec Git. Explication en français sur l’utilisation de Git. Quel logiciel de gestion de versions devriez-vous utiliser ?. Explication en français sur l’utilisation des logiciels de gestion de versions. Git : comprendre la gestion de versions. Explication en français sur ce qu’est Git et comment cela s’utilise en pratique. Introduction en anglais de GitHub dans RStudio à l’aide d’une vidéo. Happy Git and GitHub for the useR. Complet, mais un peu technique et en anglais. Installation et première utilisation de Git et GitHub dans R. En anglais. Git. Site en anglais comprenant toute la documentation de Git. GitHub pour les nuls : pas de panique, lancez-vous !. 1.2.4.3 GitHub Classroom GitHub Classroom est une extension de GitHub qui facilite le travail avec GitHub dans le contexte d’exercices à réaliser dans le cadre d’un cours. Vous serez amené à cloner et modifier des dépôts issus de GitHub Classroom pour réaliser vos exercices. Ces dépôts seront privés. Cela signifie que, seuls vous-mêmes et vos enseignants auront accès à ces dépôts. A la fin de la formation, tous ces dépôts seront détruits. Donc, si vous voulez les conserver, il faudra les “forker” sur votre propre compte. Rassurez-vous : nous vous préviendrons avant de faire le ménage ! Maintenant que vous comprenez mieux avec quels outils informatiques nous allons travaillez, vous pouvez passer à votre premier exercice pour découvrir la SciViews Box, RStudio, Markdown, Git et GitHub : vous allez réaliser un site web professionnel en ligne… Avant de poursuivre, vous allez devoir effectuer une séance d’exercice de type learnR. Pour apprendre à utiliser ces tutoriels, reportez-vous à l’appendice C. Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir le tutoriel relatif aux différents outils que nous venons de voir et répondez ensuite aux questions qui vous sont posées : BioDataScience::run(&quot;01a_base&quot;) Quand vous avez terminé, cliquez dans la fenêtre Console de RStudio et appuyez sur la touche ESC pour reprendre la main dans R à la fin d’un tutoriel. (BioDataScience est un package R spécialement développé pour ce cours et que vous avez dû installer lors de la configuration de votre SciViews Box, voir Appendice A.3.3). "],
["premier-markdown.html", "1.3 Premier document en markdown", " 1.3 Premier document en markdown Dans le cadre de ce premier module, vous allez réaliser votre premier document écrit en markdown. Nous vous proposons pour ce faire, d’écrire une petite introduction d’un article scientifique sur un animal de votre choix. Vous trouverez le lien pour cette activité dans le rubrique “A vous de jouer” ci-dessous. L’accès a cette tâche, qui est un travail individuel, se fait en utilisant GitHub Classroom B.3. Lorsque votre assignation est réalisée, faites un clone local de votre dépôt et placez-le dans le sous-dossier projects de votre dossier partagé avec la SciViews Box shared. Vous aurez alors un nouveau projet RStudio B.1.1 A vous de jouer Maintenant que vous avez appréhendé les différents outils, lancez vous dans la création de votre premier document rédigé en markdown via l’adresse suivante : Les explications relatives à la tâche qui vous est assignée sont dans le fichier README.mddu dépôt accessible depuis : Pour l’année académique 2019-2020, les URLs à utiliser pour accéder à votre tâche sont les suivants : Cours de Bioinformatique et Sciences des données à Charleroi : https://classroom.github.com/a/GY_xtRk6 Cours de Sciences des données I à Mons : https://classroom.github.com/a/cfirRjgx Pour les autres utilisateurs de ce livre, veuillez faire un “fork” du dépôt markdown-decouverte. Si vous souhaitez accéder à une version précédente de l’exercice, sélectionner la branche correspondante à l’année que vous recherchez. Si vous n’êtes pas famillarisé avec Github Classroom, vous pouvez vous référer à l’Appendice B.3. "],
["visu1.html", "Module 2 Visualisation I", " Module 2 Visualisation I Objectifs Découvrir –et vous émerveiller de– ce que l’on peut faire avec le logiciel R (R Core Team 2019) Savoir réaliser différentes variantes d’un graphique en nuage de points dans R avec la fonction chart() Découvrir le format R Markdown (Allaire et al. 2019) et la recherche reproductible Intégrer ensuite des graphiques dans un rapport et y décrire ce que que vous observez Comparer de manière critique un flux de travail “classique” en biologie utilisant Microsoft Excel et Word avec une approche utilisant R et R Markdown ; Prendre conscience de l’énorme potentiel de R Prérequis Si ce n’est déjà fait, vous devez installer et vous familiariser avec la ‘SciViews Box’, RStudio, Markdown. Vous devez aussi maîtriser les bases de Git et de GitHub (avoir un compte GitHub, savoir cloner un dépôt localement, travailler avec GitHub Desktop pour faire ses “commits”, “push” et “pull”). L’ensemble de ces outils a été abordé lors de la création de votre site personnel professionnel du module 1. Avant de poursuivre, vous allez devoir découvrir les premiers rudiments de R afin de pouvoir réaliser par la suite vos premiers graphiques. Pour cela, vous aurez à lire attentivement et effectuer tous les exercices de deux tutoriels2. Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir le tutoriel concernant les bases de R : BioDataScience::run(&quot;02a_base&quot;) Ensuite, vous pouvez également parcourir le tutoriel qui vous permettra de découvrir R sur base d’une analyse concrète (cliquez dans la fenêtre Console de RStudio et appuyez sur la touche ESC pour reprendre la main dans R à la fin d’un tutoriel) : BioDataScience::run(&quot;02b_decouverte&quot;) (BioDataScience est un package R spécialement développé pour ce cours et que vous avez dû installer lors de la configuration de votre SciViews Box, voir Appendice A.3.3). Références "],
["nuage-de-points.html", "2.1 Nuage de points", " 2.1 Nuage de points Dès que vous vous sentez familiarisé avec les principes de base de R, vous allez pouvoir réaliser assez rapidement des beaux graphiques. Par exemple, si vous souhaitez représenter une variable numérique en fonction d’une autre variable numérique, vous pouvez exprimer cela sous la forme d’une formule3 \\[y \\sim x\\] que l’on peut lire “y en fonction de x”. Pour les deux variables numériques x et y, la représentation graphique la plus classique est le nuage de points (voir Fig. 2.1 pour un exemple). Figure 2.1: Exemple de graphique en nuage de points. Des éléments essentiels sont ici mis en évidence en couleurs (voir texte). Les éléments indispensables à la compréhension d’un graphique en nuage de points sont mis en évidence à la Fig. 2.1 : Les axes avec les graduations (en rouge), les labels et les unités des axes (en bleu). Les instructions dans R pour produire un tel nuage de point sont : # Chargement de SciViews::R SciViews::R # Importation du jeu de données urchin &lt;- read(&quot;urchin_bio&quot;, package = &quot;data.io&quot;, lang = &quot;fr&quot;) # Visualisation interactive du tableau de données DT::datatable(urchin , filter = &#39;top&#39;, class = c(&#39;compact&#39;, &#39;cell-border&#39;), rownames = FALSE, options = list(pageLength = 5, scrollX = TRUE), caption = &quot;Jeu de données sur la biométrie des oursins&quot;) # Réalisation du graphique chart(data = urchin, height ~ weight) + geom_point() Figure 2.2: Taille (hauteur du test) d’oursins en fonction de leur masse. La fonction chart() n’est pas accessible dans R de base, mais l’extension chargée via l’instruction SciViews::R rend cette fonction disponible. Elle requiert comme argument le jeu de donnée (data = urchin, c’est un objet dataframe ou tibble dans le langage de R), ainsi que la formule à employer dans laquelle vous avez indiqué le nom des variables que vous voulez sur l’axe des ordonnées à gauche et des abscisses à droite de la formule, les deux membres étant séparés par un “tilde” (~). Vous voyez que le jeu de données contient beaucoup de variables (les titres des colonnes du tableau en sortie). Parmi toutes ces variables, nous avons choisi ici de représenter height en fonction de weight, la hauteur en fonction de la masse des oursins. Jusqu’ici, nous avons spécifié ce que nous voulons représenter, mais pas encore comment (sous quelle apparence), nous voulons les matérialiser sur le graphique. Pour un nuage de points, nous voulons les représenter sous forme de … points ! Donc, nous devons ajouter la fonction geom_point() pour indiquer cela. 2.1.1 Le nuage de points en vidéo Vous trouverez une vidéo ci-dessous vous expliquant la création du nuage de points dans R sur ce jeu de données, en analysant d’autres variables. Cette vidéo ne vous a montré que les principaux outils disponibles lors de la réalisation de graphiques. Soyez curieux et expérimentez par vous-même ! A vous de jouer Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir le tutoriel concernant le nuage de points : BioDataScience::run(&quot;02c_nuage_de_points&quot;) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R à la fin d’un tutoriel) 2.1.2 Echelles de graphiques Vous devez être vigilant lors de la réalisation d’un nuage de point particulièrement sur l’étendue des valeurs présentées sur vos axes. Vous devez utilisez votre expertise de biologiste pour vous posez les deux questions suivantes : Est ce que l’axe représente des valeurs plausibles de hauteurs et de masses de ces oursins appartenant à l’espèce Paracentrotus lividus ? Quels est la précision des mesures effectuées ? Dans certains cas, la forme du nuage de points peut être distendu par la présence de valeurs aberrantes. Ce n’est pas le cas ici, mais nous pouvons le simuler en distendant artificiellement soit l’axe X, soit l’axe Y, soit les deux : Figure 2.3: Piège du nuage de points. A) graphique initial montrant la variation de la hauteur [mm] en fonction de la masse [g]. B) graphique A avec la modification de l’échelle de l’axe X. C) Graphique A avec une seconde modification de l’axe X. D) Graphique A avec modification simultanée des deux axes. Appliquez par vous même les modifications des échelles via l’application ci-dessous 2.1.3 Transformation des données Vous avez la possibilité d’appliquer une transformation de vos données (il est même conseillé de le faire) afin qu’elles soient plus facilement analysables. Par exemple, il est possible d’utiliser des fonctions de puissance, racines, logarithmes, exponentielles4 pour modifier l’apparence du nuage de points dans le but de le rendre plus linéaire (car il est plus facile d’analyser statistiquement des données qui s’alignent le long d’une droite). Par exemple, sur nos données de hauteurs et masses d’oursins, la transformation double-logarithmique (log(x) et log(Y)) fonctionne très bien pour rendre le nuage de points plus linéaire : # Réalisation du graphique de la hauteur en fonction de la masse a &lt;- chart(urchin, height ~ weight) + geom_point() # Application du logarithme sur les deux variables représentées b &lt;- chart(urchin, log10(height) ~ log10(weight)) + geom_point() + labs(x = &quot;log(Masse totale [g])&quot;, y = &quot;log(Hauteur du test [mm])&quot;) # Assemblage des graphiques combine_charts(list(a, b)) Figure 2.4: A) Hauteur [mm] en fonction de la masse [g] d’oursins violets. B) Logarithme en base 10 de la hauteur [mm] en fonction du logarithme en base 10 de la masse [g] de ces mêmes oursins. Appliquez par vous même les transformations des variables via l’application ci-dessous Pièges et astuces RStudio permet de récupérer rapidement des instructions à partir d’une banque de solutions toutes prêtes. Cela s’appelle des snippets. Vous avez une série de snippets disponibles dans la SciViews Box. Celui qui vous permet de réaliser un graphique en nuage de points s’appelle .cbxy (pour chart -&gt; bivariate -&gt; xy-plot). Entrez ce code et appuyez ensuite sur la tabulation dans un script R, et vous verrez le code remplacé par ceci dans la fenêtre d’édition : chart(data = DF, YNUM ~ XNUM) + geom_point() Vous avez à votre disposition un ensemble de snippets que vous pouvez retrouver dans l’aide-mémoire consacré à SciViews. Vous avez également à votre disposition l’aide-mémoire sur la visualisation des données (Data Visualization Cheat Sheet) qui utilise la fonction ggplot() plutôt que chart() et une interface légèrement différente pour spécifier les variables à utiliser pour réaliser le graphique (aes(x = ..., y = ...)). A vous de jouer Une nouvelle tâche vous est demandée ci-dessous en utilisant GitHub Classroom 1.2.4.3. Cette tâche est un travail individuel. Une fois votre assignation réalisée, faites un clone local de votre dépôt et placez-le dans le sous-dossier projects de votre dossier partagé avec la SciViews Box shared. Vous aurez alors un nouveau projet RStudio B.1.1 Les instructions R que vous expérimentez dans un learnR peuvent être employées également dans un script d’analyse. Sur base du jeu de données urchin_bio, explorez différents graphiques en nuages de points. Les explications relatives à la tâche qui vous est assignée sont dans le fichier README.mddu dépôt accessible depuis : Pour l’année académique 2019-2020, les URLs à utiliser pour accéder à votre tâche sont les suivants : Cours de Bioinformatique et Sciences des données à charleroi : https://classroom.github.com/a/p2m_RPt1 Cours de Sciences des données I à Mons : https://classroom.github.com/a/GxS0euIC Pour les autres utilisateurs de ce livre, veuillez faire un “fork” du dépôt sdd1_urchin_bio. Si vous souhaitez accéder à une version précédente de l’exercice, sélectionner la branche correspondante à l’année que vous recherchez. Inspirez-vous du script dans le dépôt sdd1_iris. Vous devez commencer par faire un “fork” du dépôt, puis un clone sur votre ordinateur en local pour pouvoir l’utiliser. https://github.com/BioDataScience-Course/sdd1_iris Prêtez une attention toute particulière à l’organisation d’un script R. En plus des instructions R, il contient aussi sous forme de commentaires, un titre , la date de la dernière mise à jour, le nom de l’auteur, et des sections qui organisent de façon claire le contenu du script. A ce sujet, vous trouverez des explications détaillées concernant l’utilisation des scripts R dans l’annexe B.1.2. Pour en savoir plus Visualisation des données dans R for Data Science. Chapitre du livre portant sur la visualisation des données, en anglais. ggplot2 nuage de point. Tutoriel en français portant sur l’utilisation d’un nuage de points avec le package ggplot2 et la fonction geom_point(). Fundamentals of Data Visualization. Un livre en anglais sur les fondamentaux de la visualisation graphique. R Graphics Cookbook - Chapter 5: Scatter Plots. Un chapitre d’un livre en anglais sur l’utilisation du nuage de points. geom_point(). La fiche technique de la fonction (en anglais). Testez vos acquis Dans la fenêtre Console de RStudio, entrez l’instruction suivante et puis appuyez sur la touche Entrée pour ouvrir le tutoriel de challenge concernant le nuage de points : BioDataScience::run(&quot;02d_np_challenge&quot;) N’oubliez pas de vous enregistrer (login GitHub et email UMONS) au début, et d’appuyer sur la touche ESC pour reprendre la main dans R à la fin d’un tutoriel. Dans R, une formule permet de spécifier les variables avec lesquelles on souhaite travailler, et leur rôle. Par exemple ici, la variable x sur l’axe des abscisses et la variable y sur l’axe des ordonnées.↩ Pour les proportions (prop) ou les pourcentages (perc) (valeurs bornées entre 0 et 1 ou 0 et 100%) la transformation arc-sinus est souvent utilisée : \\(prop′ = \\arcsin \\sqrt{prop}\\) ou \\(perc′ = \\arcsin \\sqrt{perc / 100}\\).↩ "],
["graphiques-dans-r-markdown.html", "2.2 Graphiques dans R Markdown", " 2.2 Graphiques dans R Markdown Un fichier R Markdown est un fichier avec une extension .Rmd. Il permet de combiner le langage Markdown que vous avez déjà abordé au premier module avec du code R, tel que celui utilisé dans la première partie de ce module 2. 2.2.1 R Markdown en vidéo La vidéo ci-dessous vous montre ce qu’est R Markdown, un format hybride entre Markdown et R bien pratique pour inclure vos graphiques directement dans un rapport. Elle vous montre aussi comment transformer un script R en document R Markdown (ou R Notebook, qui en est une variante). Les balises spéciales R Markdown à retenir sont les suivantes : en entrée de chunk R : ```{r} seul sur une ligne. Il est aussi possible de rajouter un nom, par exemple, ```{r graphique1} et/ou des options, par exemple, ```{r, echo=FALSE, results='hide'} pour cacher et le code et le résultat dans le rapport), en sortie de chunk R : ``` seul sur une ligne. Vous devez bien entendu avoir autant de balises d’entrée que de balises de sortie. Des explications plus détaillées se trouvent dans l’annexe B.1.3 dédiée au R Markdown. De plus, l’écriture d’un rapport d’analyse scientifique doit respecter certaines conventions. Vous trouverez des explications à ce sujet dans l’annexe D. Vous ne devez bien évidemment pas commencer avec un script R. Vous pouvez commencer d’emblée avec un R Markdown/R Notebook et écrire vos instructions R directement dedans. Il vous est toujours possible d’exécuter ces instructions ligne après ligne dans la fenêtre Console pour les tester tout comme à partir d’un script R. Pour en savoir plus Communicating results with R Markdown explique la même chose que dans la vidéo, avec plus de détails et des liens vers d’autres documents utiles (en anglais). What is R Markdown?. Vidéo en anglais + site présentant les différentes possibilités, par les concepteurs de R Markdown (RStudio). Introduction to R Markdown. Tutoriel en anglais, par RStudio. R Markdown : the definitive guide est le manuel par excellence pour R Markdown (en anglais uniquement, malheureusement). Aide-mémoire R Markdown: dans les menus de RStudio : Help -&gt; Cheatsheets -&gt; R Markdown Cheat Sheet Référence rapide à Markdown : dans les menus RStudio Help -&gt; Markdown Quick Reference Introduction à R Markdown. Présentation en français par Agrocampus Ouest - Rennes. Le langage R Markdown. Introduction en français concise, mais relativement complète. Reproducible reports with R Markdown. Une explication en anglais de la raison d’être de R Markdown. Why I love R Notebooks explique (en anglais) pourquoi le format R Notebook est particulièrement bien adapté à la science des données. A vous de jouer Vous allez maintenant manipuler un R Notebook pour construire de manière interactive une analyse en même temps que le rapport associé. Partez du projet sdd1_urchin_bio que vous avez obtenu via le lien GitHub Classroom dans la première partie de ce module. Votre objectif est de comprendre les données proposées, en utilisant des visualisations graphiques appropriées et en documentant le fruit de votre étude dans un rapport R Notebook. Utilisez le graphique en nuage de points que vous venez d’étudier, bien sûr, mais vous êtes aussi encouragés à expérimenter d’autres formes de visualisation graphique. Comparaions du flux de travail en biologie : R et Microsoft Word versus R et R Markdown. Une nouvelle tâche vous est demandée ci-dessous en utilisant GitHub Classroom 1.2.4.3. Cette tâche est un travail en équipe. Une fois votre assignation réalisée, faites un clone de votre dépôt et placez-le dans le dossier shared/projects. Comparez le workflow classique en biologie via Microsoft Office avec l’utilisation de R - R Markdown. Les explications relatives à la tâche qui vous est assignée sont dans le fichier README.mddu dépôt accessible depuis : Pour l’année académique 2019-2020, les URLs à utiliser pour accéder à votre tâche sont les suivants : Cours de Bioinformatique et Sciences des données à Charleroi : https://classroom.github.com/g/jvDR9nhM Cours de Sciences des données I à Mons : https://classroom.github.com/g/Q1-L0PhW Pour les autres utilisateurs de ce livre, veuillez faire un “fork” du dépôt sdd1_biometry. Si vous souhaitez accéder à une version précédente de l’exercice, sélectionner la branche correspondante à l’année que vous recherchez. "],
["visu2.html", "Module 3 Visualisation II", " Module 3 Visualisation II Objectifs Savoir réaliser différentes variantes de graphiques visant à montrer comment les données se distribuent tel que les histogrammes, les graphes de densité ou encore les diagrammes en violon dans R avec la fonction chart() Intégrer ensuite des graphiques dans un rapport et y décrire ce que vous observez Gérer des conflits dans GitHub Prérequis Pour réaliser les exercices dans ce module, vous devez être capables de travailler dans la SciViews Box et dans RStudio. Vous devez également maîtriser les bases de Git et GitHub. Tout ceci est enseigné dans le module 1. Vous devez également être familiarisés avec les graphiques dans R et R Markdown, une matière qui fait l’objet du module 2. Préparatifs Une nouvelle tâche vous est demandée ci-dessous en utilisant GitHub Classroom 1.2.4.3. Une fois votre assignation réalisée, faites un clone de votre dépôt et placez-le dans le dossier shared/projects. Pour cette tâche, vous démarrerez d’un projet RStudio B.1.1 que vous obtiendrez via une tâche GitHub Classroom. Pour cette activité, vous allez travailler en binôme sur les données d’un projet étudiant le zooplancton provenant de Madagascar. Les explications relatives à la tâche qui vous est assignée sont dans le fichier README.mddu dépôt accessible depuis : Pour l’année académique 2019-2020, les URLs à utiliser pour accéder à votre tâche sont les suivants : Cours de Bioinformatique et Sciences des données à Charleroi : https://classroom.github.com/g/qJZ0kmPG Cours de Sciences des données I à Mons : https://classroom.github.com/g/AkOJKC3n Pour les autres utilisateurs de ce livre, veuillez faire un “fork” du dépôt sdd1_zooplankton. Si vous souhaitez accéder à une version précédente de l’exercice, sélectionner la branche correspondante à l’année que vous recherchez. Vous utiliserez à la fois votre projet sur la biométrie des oursins (du module précédent) et ce nouveau projet sur le zooplancton5 pour découvrir les nouveaux outils graphiques décrits dans ce module. Le mot zooplancton ne se décline jamais au pluriel. On parle du zooplancton pour désigner une large communauté d’organismes zooplanctoniques, et non pas des zooplanctons.↩ "],
["histogramme.html", "3.1 Histogramme", " 3.1 Histogramme Vous souhaitez visualiser l’étalement de vos données sur un axe (on parle de distribution6 en statistique) pour l’une des variables étudiées. L’histogramme est l’un des outils pouvant vous apporter cette information. Ce graphique représente sous forme de barres un découpage en plusieurs classes7 d’une variable numérique. Figure 3.1: Exemple d’histogramme montrant la distribution de la taille d’un échantillon de zooplancton. Outre l’histogramme lui-même, représenté par des barres de hauteur équivalentes au nombre de fois que les observations ont été réalisées dans les différentes classes, les éléments suivants sont également indispensables à la compréhension du graphique (ici mis en évidence en couleur) Les axes avec les graduations (en rouge). Su l’axe des abscisses, les classes de tailles, et sur l’axe des ordonnées, le nombre d’occurrence les labels des axes et l’unité (pour l’axe des abscisses uniquement ici) (en bleu) Les instructions dans R afin de produire un histogramme à l’aide de la fonction chart() sont : # Importation du jeu de données (zooplankton &lt;- read(&quot;zooplankton&quot;, package = &quot;data.io&quot;, lang = &quot;FR&quot;)) # # A tibble: 1,262 x 20 # ecd area perimeter feret major minor mean mode min max std_dev # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 0.770 0.465 4.45 1.32 1.16 0.509 0.363 0.036 0.004 0.908 0.231 # 2 0.700 0.385 2.32 0.728 0.713 0.688 0.361 0.492 0.024 0.676 0.183 # 3 0.815 0.521 4.15 1.33 1.11 0.598 0.308 0.032 0.008 0.696 0.204 # 4 0.785 0.484 4.44 1.78 1.56 0.394 0.332 0.036 0.004 0.728 0.218 # 5 0.361 0.103 1.71 0.739 0.694 0.188 0.153 0.016 0.008 0.452 0.110 # 6 0.832 0.544 5.27 1.66 1.36 0.511 0.371 0.02 0.004 0.844 0.268 # 7 1.23 1.20 15.7 3.92 1.37 1.11 0.217 0.012 0.004 0.784 0.214 # 8 0.620 0.302 3.98 1.19 1.04 0.370 0.316 0.012 0.004 0.756 0.246 # 9 1.19 1.12 15.3 3.85 1.34 1.06 0.176 0.012 0.004 0.728 0.172 # 10 1.04 0.856 7.60 1.89 1.66 0.656 0.404 0.044 0.004 0.88 0.264 # # … with 1,252 more rows, and 9 more variables: range &lt;dbl&gt;, size &lt;dbl&gt;, # # aspect &lt;dbl&gt;, elongation &lt;dbl&gt;, compactness &lt;dbl&gt;, transparency &lt;dbl&gt;, # # circularity &lt;dbl&gt;, density &lt;dbl&gt;, class &lt;fct&gt; # Réalisation du graphique chart(data = zooplankton, ~ size) + geom_histogram(bins = 50) + ylab(&quot;Effectifs&quot;) Figure 3.2: Distribution des tailles au sein d’un échantillon de zooplancton # bins permet de préciser le nombre de classes souhaitées La fonction chart() requiert comme argument le jeu de donnée (zooplankton), ainsi que la formule à employer dans laquelle vous avez indiqué le nom de la variable que vous voulez sur l’axe des abscisses à droite de la formule, après le tilde ~. Vous voyez que le jeu de données contient beaucoup de variables (les titres des colonnes du tableau en sortie). Parmi toutes ces variables, nous avons choisi ici de représenter size, Jusqu’ici, nous avons spécifié ce que nous voulons représenter, mais pas encore comment (sous quelle apparence), nous voulons matérialiser cela sur le graphique. Pour un histogramme, nous devons ajouter la fonction geom_histogram(). L’argument bins dans cette fonction permet de préciser le nombre de classes souhaitées. Le découpage en classe se fait automatiquement dans R à partir de la variable size d’origine. Vous pouvez décrypter votre histogramme sur base des modes8 et de la symétrie9 de ces derniers. Un histogramme peut être unimodal (un seul mode), bimodal (deux modes) ou multimodal (plus de deux modes). En général, s’il y a plus d’un mode, nous pouvons suspecter que des sous-populations existent au sein de notre échantillon. Figure 3.3: Histogrammes montrant les modes et symétries : A. histogramme unimodal et symétrique, B. histogramme bimodal et asymétrique, C. histogramme unimodal et asymétrique, D. histogramme multimodal et symétrique. 3.1.1 Nombre de classes Vous devez être particulièrement vigilant lors de la réalisation d’un histogramme aux classes définies pour ce dernier. # Réalisation du graphique précédent a &lt;- chart(data = zooplankton, ~ size) + geom_histogram(bins = 50) + ylab(&quot;Effectifs&quot;) # Modification du nombre de classes b &lt;- chart(data = zooplankton, ~ size) + geom_histogram(bins = 20) + ylab(&quot;Effectifs&quot;) c &lt;- chart(data = zooplankton, ~ size) + geom_histogram(bins = 10) + ylab(&quot;Effectifs&quot;) d &lt;- chart(data = zooplankton, ~ size) + geom_histogram(bins = 5) + ylab(&quot;Effectifs&quot;) # Assemblage des graphiques combine_charts(list(a, b, c, d)) Figure 3.4: Choix des classes. A. histogramme initial montrant la répartition des tailles au sein d’organismes planctoniques. B., C., D. Même histogramme que A, mais en modifiant le nombres de classes. Comme vous pouvez le voir à la Fig. 3.4, le changement du nombre de classes peut modifier complètement la perception des données via l’histogramme. Le choix idéal est un compromis entre plus de classes (donc plus de détails), et un d’coupage raisonnable en fonction de la quantité de données disponibles. Si l’intervalle des classes est trop petit, l’histogramme sera illisible. Si l’intervalle des classes est trop grand, il sera impossible de visualiser correctement les différents modes. Dans la figure en exemple, les variantes A et B sont acceptables, mais les C et D manquent de détails. Pièges et astuces La SciViews Box propose un snippet RStudio pour réaliser un histogramme. Il s’appelle .cuhist (pour chart -&gt; univariate -&gt; histogram). Entrez ce code dans une zone d’édition R et appuyez ensuite sur la tabulation, et vous verrez le code remplacé par ceci : chart(data = DF, ~VARNUM) + geom_histogram(binwidth = 30) L’argument binwidth = permet de préciser la largeur des classes. C’est une autre façon de spécifier le découpage en classes, mais vous pouvez naturellement le remplacer par l’argument bins = si vous préférez. Vous avez à votre disposition un ensemble de snippets que vous pouvez retrouver dans l’aide-mémoire sur SciViews. N’oubliez pas que vous avez également à votre disposition l’aide-mémoire sur la visualisation des données (Data Visualization Cheat Sheet), via la fonction ggplot(). 3.1.2 Histogramme par facteur Lors de l’analyse de jeux de données, vous serez amené à réaliser un histogramme par facteur (c’est-à-dire, en fonction de différents niveaux d’une variable qualitative qui divise le jeu de données en sous-groupes). Par exemple, dans un jeu de données sur des fleurs d’iris, la variable species10 représente l’espèce d’iris étudiée (trois espèces différentes : I. setosa, I. versicolor et I. virginica). # Importation du jeu de données (iris &lt;- read(&quot;iris&quot;, package = &quot;datasets&quot;, lang = &quot;fr&quot;)) # # A tibble: 150 x 5 # sepal_length sepal_width petal_length petal_width species # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; # 1 5.1 3.5 1.4 0.2 setosa # 2 4.9 3 1.4 0.2 setosa # 3 4.7 3.2 1.3 0.2 setosa # 4 4.6 3.1 1.5 0.2 setosa # 5 5 3.6 1.4 0.2 setosa # 6 5.4 3.9 1.7 0.4 setosa # 7 4.6 3.4 1.4 0.3 setosa # 8 5 3.4 1.5 0.2 setosa # 9 4.4 2.9 1.4 0.2 setosa # 10 4.9 3.1 1.5 0.1 setosa # # … with 140 more rows # Réalisation de l&#39;histogramme par facteur chart(data = iris, ~ sepal_length %fill=% species) + geom_histogram(bins = 25) + ylab(&quot;Effectifs&quot;) + scale_fill_viridis_d() # palette de couleur harmonieuse Figure 3.5: Distribution des longueurs de sépales de trois espèces d’iris. Ici, nous avons tracé un histogramme unique, mais en prenant soin de colorier les barres en fonction de l’espèce. La formule fait toujours intervenir la variable numérique à découper en classes à la droite du tilde ~, ici sepal_length, mais nous y avons ajouté une directive supplémentaire pour indiquer que le remplissage des barres (%fill=%) doit se faire en fonction du contenu de la variable species. Nous avons ici un bon exemple d’histogramme multimodal lié à la présence de trois sous-groupes (les trois espèces différentes) au sein d’un jeu de données unique. Le rendu du graphique n’est pas optimal. Voici deux astuces pour l’améliorer. La premières consiste à représenter trois histogrammes séparés, mais rassemblés dans une même figure. Pour cela, nous utilisons des facettes (facets) au lieu de l’argument %fill=%. Dans chart(), les facettes peuvent être spécifiées an utilisant l’opérateur | dans la formule. chart(data = iris, ~ sepal_length | species) + geom_histogram(bins = 25) + ylab(&quot;Effectifs&quot;) Figure 3.6: Distribution de la longueur des sépales de trois espèces d’iris (en employant les facettes pour séparer les espèces). L’histogramme est maintenant séparé en trois en fonction des niveaux de la variable facteur species. Cela rend la lecture plus aisée. Une seconde solution combine les facettes avec | et l’argument %fill=%11. Il faut ensuite ajouter par derrière un histogramme grisé de l’ensemble des données. nbins &lt;- 25 chart(data = iris, ~ sepal_length %fill=% species | species) + # histogramme d&#39;arrière plan en gris ne tenant pas compte de la variable species geom_histogram(data = select(iris, -species), fill = &quot;grey&quot;, bins = nbins) + geom_histogram(show.legend = FALSE, bins = nbins) + ylab(&quot;Effectifs&quot;) + scale_fill_viridis_d() Figure 3.7: Distribution des longueurs de sépales de trois espèces d’iris (avec facettes et histogrammes complets grisés en arrière plans). Vous découvrez sans doute que les graphiques réalisables avec R sont modulables à souhait en ajoutant une série d’instructions successives qui créent autant de couches superposées dans le graphique. Cette approche permet de réaliser quasiment une infinité de graphiques différents en combinant seulement quelques dizaines d’instructions. Pour s’y retrouver, les fonctions qui ajoutent des couches commencent toutes par geom_, et celles qui manipulent les couleurs par scale_, par exemple. Vous découvrirez encore d’autres fonctions graphiques plus loin. La distribution des données en statistique se réfère à la fréquence avec laquelle les différentes valeurs d’une variable s’observent.↩ Une variable numérique est découpée en classes en spécifiant différents intervalles, et ensuite en dénombrant le nombre de fois que les observations rentrent dans ces classes.↩ Les modes d’un histogramme correspondent à des classes plus abondantes localement, c’est-à-dire que les classes à gauche et à droite du mode comptent moins d’occurrences que lui.↩ Un histogramme est dit symétrique lorsque son profil à gauche est identique ou très similaire à son profil à droite autour d’un mode.↩ Attention : le jeu de donnée iris est un grand classique dans R, mais lorsqu’il est chargé à l’aide de la fonction read() du package data.io, le nom de ses variables est modifié pour suivre la convention “snake-case” qui veut que seules des lettres minuscules soient utilisées et que les mots soient séparés par un trait souligné _. Ainsi, dans le jeu de données d’origine, les variables sont nommées Petal_Length ou Species. Ici, ces même variables se nomment petal_length et species.↩ Astuce proposée ici.↩ "],
["graphique-de-densite.html", "3.2 Graphique de densité", " 3.2 Graphique de densité L’histogramme n’est pas le seul outil à votre disposition. Vous pouvez également employer le graphique de densité qui se présente un peu comme un histogramme lissé. Le passage d’un histogramme vers un graphe de densité se base sur une estimation par noyaux gaussien12 Figure 3.8: A. Histogramme et B. graphique de densité montrant la distribution de la taille de zooplancton étudié par analyse d’image. Comme pour les autres graphiques, veillez à soigner les indications qui permettent d’interpréter le graphique. Outre la courbe de densité, il faut : Les axes avec les graduations (en rouge) les labels des axes, et l’unité pour l’axe des abscisses (en bleu) Les instructions en R pour produire un graphique de densité avec la fonction chart() sont : # Importation du jeu de données (zooplankton &lt;- read(&quot;zooplankton&quot;, package = &quot;data.io&quot;, lang = &quot;FR&quot;)) # # A tibble: 1,262 x 20 # ecd area perimeter feret major minor mean mode min max std_dev # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 0.770 0.465 4.45 1.32 1.16 0.509 0.363 0.036 0.004 0.908 0.231 # 2 0.700 0.385 2.32 0.728 0.713 0.688 0.361 0.492 0.024 0.676 0.183 # 3 0.815 0.521 4.15 1.33 1.11 0.598 0.308 0.032 0.008 0.696 0.204 # 4 0.785 0.484 4.44 1.78 1.56 0.394 0.332 0.036 0.004 0.728 0.218 # 5 0.361 0.103 1.71 0.739 0.694 0.188 0.153 0.016 0.008 0.452 0.110 # 6 0.832 0.544 5.27 1.66 1.36 0.511 0.371 0.02 0.004 0.844 0.268 # 7 1.23 1.20 15.7 3.92 1.37 1.11 0.217 0.012 0.004 0.784 0.214 # 8 0.620 0.302 3.98 1.19 1.04 0.370 0.316 0.012 0.004 0.756 0.246 # 9 1.19 1.12 15.3 3.85 1.34 1.06 0.176 0.012 0.004 0.728 0.172 # 10 1.04 0.856 7.60 1.89 1.66 0.656 0.404 0.044 0.004 0.88 0.264 # # … with 1,252 more rows, and 9 more variables: range &lt;dbl&gt;, size &lt;dbl&gt;, # # aspect &lt;dbl&gt;, elongation &lt;dbl&gt;, compactness &lt;dbl&gt;, transparency &lt;dbl&gt;, # # circularity &lt;dbl&gt;, density &lt;dbl&gt;, class &lt;fct&gt; # Réalisation du graphique chart(data = zooplankton, ~ size) + geom_density() + ylab(&quot;Densité&quot;) Figure 3.9: Distribution des tailles au sein de l’échantillon de zooplancton. Ici, nous utilisons donc la fonction geom_density(). L’opération effectuée pour passer d’un histogramme à une courbe de densité consiste effectivement à lisser les pics plus ou moins fort dans l’histogramme de départ.↩ "],
["diagramme-en-violon.html", "3.3 Diagramme en violon", " 3.3 Diagramme en violon Le graphique en violon est constitué de deux graphiques de densité en miroir. Le résultat fait penser un peu à un violon pour une distribution bimodale. Cette représentation est visuellement très convainquante lorsque la variable étudiée contient suffisamment d’observations pour permettre de déterminer précisément sa distribution (plusieurs dizaines ou centaines d’individus mesurés). Figure 3.10: Graphe en violon de la distribution de la taille en fonction des groupes taxonomiques dans un échantillon de zooplancton. Les instructions en R pour produire un diagramme en violon à l’aide de la fonction chart() sont : # Importation du jeu de données zooplankton &lt;- read(&quot;zooplankton&quot;, package = &quot;data.io&quot;, lang = &quot;FR&quot;) # Réduction du jeu de données zooplankton_sub &lt;- filter(zooplankton, class %in% c(&quot;Annélide&quot;, &quot;Calanoïde&quot;, &quot;Cyclopoïde&quot;, &quot;Décapode&quot;)) # Réalisation du graphique chart(data = zooplankton_sub, size ~ class) + geom_violin() Figure 3.11: Distribution des tailles pour 4 groupes taxonomiques de zooplancton. Ici, la formule fournie à chart() indique la variable numérique à représenter par un graphe de densité dans le terme de gauche, et la variable facteur qui découpe l’échantillon en classes à droite : YNUM (size) ~ XFACT (class). Pour réaliser un graphique de densité vous devez ensuite ajouter la fonction geom_violin(). Vous pouvez aussi utiliser %fill=% pour colorer vos différents graphes en fonction de la variable facteur également, comme dans la Fig. 3.10. Pièges et astuces Parfois, les labels sur l’axe des abscisses d’un diagramme en violon apparaissent trop rapprochés et se chevauchent, comme ci-dessous. chart(data = zooplankton, size ~ class) + geom_violin() Figure 3.12: Distribution de tailles des 17 classes d’organismes planctoniques (diagramme en violon). La fonction coord_flip() permute les axes. Ainsi les labels ne se chevauchent plus sur l’axe des ordonnées. chart(data = zooplankton, size ~ class) + geom_violin() + coord_flip() Figure 3.13: Distribution de tailles des 17 classes d’organismes planctoniques (diagramme en violon avec l’ajout de la fonction coord_flip()). Le package ggridges propose une seconde solution basée sur le principe de graphique de densité avec la fonction geom_density_ridges() qui crée un graphique en lignes de crêtes. Attention : remarquez que la notation est ici inverse du diagramme en violon, soit XFACT (class) ~ YNUM (size) ! chart(data = zooplankton, class ~ size) + ggridges::geom_density_ridges() Figure 3.14: Distribution des tailles des 17 classes d’organismes planctoniques (sous forme de graphique en lignes de crêtes). "],
["visualiser-des-distributions.html", "3.4 Visualiser des distributions", " 3.4 Visualiser des distributions En pratique, vous ne représenterez pas systématiquement tous ces types de graphiques pour toutes les variables. Il faudra choisir le graphique le plus adapté à la situation. La plupart du temps, cela se fait de manière itérative : vous essayez diverses variantes, vous les comparez, et vous gardez celle(s) qui visualisent le mieux les données dans le cas particulier de votre étude. A vous de jouer Reprenez vos différents projets et étudiez la distribution de variables numériques de différentes manières. Commentez vos différents graphiques par des paragraphes rédigés en Markdown. Précisez ceux qui vous semblent les plus appropriés et justifiez vos choix. Terminez ce module en vérifiant que vous avez bien compris les notions apprises jusqu’ici. Ouvrez RStudio dans votre SciViews Box, puis exécutez l’instruction suivante dans la fenêtre console : BioDataScience::run(&quot;03a_test&quot;) Pour en savoir plus Si vous avez encore du mal avec la compréhension de l’histogramme, voyez cette vidéo qui vous montre comment le construire à la main. Dans la section “How to build an histogram” dans cette page, vous verrez une animation qui visualise étape par étape la construction d’un histogramme (en anglais). Les histogrammes à classes de largeurs variables. "],
["visu3.html", "Module 4 Visualisation III", " Module 4 Visualisation III Objectifs Savoir réaliser différents graphiques pour représenter des variables facteurs comme le graphique en barres, ou le graphique en camembert dans R avec la fonction chart() Comprendre et utiliser la boîte de dispersion pour synthétiser la distribution de données numériques Arranger différents graphiques dans une figure unique Découvrir différents systèmes graphiques (graphiques de base, lattice, ggplot2) et les comparer entre eux Prérequis Assurez-vous de bien maîtriser les bases relatives à la représentation graphiques vues dans le module 2 et que vous êtes à l’aise dans l’utilisation de vos outils logiciels (SciViews Box, RStudio, R Markdown). "],
["graphique-en-barres.html", "4.1 Graphique en barres", " 4.1 Graphique en barres Le graphique en barres (on dit aussi graphique en bâtons) compare les effectifs pour différents niveaux (ou modalités) d’une variable qualitative ou facteur. La différence avec l’histogramme est donc subtile et tient au fait que, pour l’histogramme, nous partons d’une variable quantitative qui est découpée en classes. 4.1.1 Effectifs par facteur La question du nombre et/ou de l’intervalle des classes ne se pose pas dans le cas du graphique en barres. Par défaut, les barres seront séparées les unes des autres par un petit espace vide pour bien indiquer visuellement qu’il n’y a pas continuité entre les classes (dans l’histogramme, les barres sont accolées les unes aux autres pour matérialiser justement cette continuité). La formule que vous utiliserez, ici encore, ne fait appel qu’à une seule variable et s’écrira donc : \\[\\sim variable \\ facteur\\] Figure 4.1: Exemple d’un graphique en barres montrant le dénombrement des niveaux d’une variable facteur, avec les éléments importants du graphique mis en évidence en couleurs. Outre les barres elles-mêmes, prêtez toujours attention aux éléments suivants du graphique (ici mis en évidence en couleurs) : les axes avec les graduations (en rouge) les niveaux de la variable facteur (en rouge également) le label des axes (en bleu) Les instructions dans R pour produire un graphique en barres à l’aide de la fonction chart() sont : # Importation du jeu de données (zooplankton &lt;- read(&quot;zooplankton&quot;, package = &quot;data.io&quot;, lang = &quot;FR&quot;)) # # A tibble: 1,262 x 20 # ecd area perimeter feret major minor mean mode min max std_dev # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 0.770 0.465 4.45 1.32 1.16 0.509 0.363 0.036 0.004 0.908 0.231 # 2 0.700 0.385 2.32 0.728 0.713 0.688 0.361 0.492 0.024 0.676 0.183 # 3 0.815 0.521 4.15 1.33 1.11 0.598 0.308 0.032 0.008 0.696 0.204 # 4 0.785 0.484 4.44 1.78 1.56 0.394 0.332 0.036 0.004 0.728 0.218 # 5 0.361 0.103 1.71 0.739 0.694 0.188 0.153 0.016 0.008 0.452 0.110 # 6 0.832 0.544 5.27 1.66 1.36 0.511 0.371 0.02 0.004 0.844 0.268 # 7 1.23 1.20 15.7 3.92 1.37 1.11 0.217 0.012 0.004 0.784 0.214 # 8 0.620 0.302 3.98 1.19 1.04 0.370 0.316 0.012 0.004 0.756 0.246 # 9 1.19 1.12 15.3 3.85 1.34 1.06 0.176 0.012 0.004 0.728 0.172 # 10 1.04 0.856 7.60 1.89 1.66 0.656 0.404 0.044 0.004 0.88 0.264 # # … with 1,252 more rows, and 9 more variables: range &lt;dbl&gt;, size &lt;dbl&gt;, # # aspect &lt;dbl&gt;, elongation &lt;dbl&gt;, compactness &lt;dbl&gt;, transparency &lt;dbl&gt;, # # circularity &lt;dbl&gt;, density &lt;dbl&gt;, class &lt;fct&gt; # Réduction du jeu de données (copepoda &lt;- filter(zooplankton, class %in% c(&quot;Calanoïde&quot;, &quot;Cyclopoïde&quot;, &quot;Harpacticoïde&quot;, &quot;Poecilostomatoïde&quot;))) # # A tibble: 535 x 20 # ecd area perimeter feret major minor mean mode min max std_dev # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 0.770 0.465 4.45 1.32 1.16 0.509 0.363 0.036 0.004 0.908 0.231 # 2 0.815 0.521 4.15 1.33 1.11 0.598 0.308 0.032 0.008 0.696 0.204 # 3 0.785 0.484 4.44 1.78 1.56 0.394 0.332 0.036 0.004 0.728 0.218 # 4 0.361 0.103 1.71 0.739 0.694 0.188 0.153 0.016 0.008 0.452 0.110 # 5 0.832 0.544 5.27 1.66 1.36 0.511 0.371 0.02 0.004 0.844 0.268 # 6 1.23 1.20 15.7 3.92 1.37 1.11 0.217 0.012 0.004 0.784 0.214 # 7 0.620 0.302 3.98 1.19 1.04 0.370 0.316 0.012 0.004 0.756 0.246 # 8 1.19 1.12 15.3 3.85 1.34 1.06 0.176 0.012 0.004 0.728 0.172 # 9 1.04 0.856 7.60 1.89 1.66 0.656 0.404 0.044 0.004 0.88 0.264 # 10 0.725 0.412 7.14 1.90 0.802 0.655 0.209 0.008 0.004 0.732 0.202 # # … with 525 more rows, and 9 more variables: range &lt;dbl&gt;, size &lt;dbl&gt;, # # aspect &lt;dbl&gt;, elongation &lt;dbl&gt;, compactness &lt;dbl&gt;, transparency &lt;dbl&gt;, # # circularity &lt;dbl&gt;, density &lt;dbl&gt;, class &lt;fct&gt; # Réalisation du graphique chart(data = copepoda, ~ class) + geom_bar() + ylab(&quot;Effectifs&quot;) Figure 4.2: Abondances de quatres types de copépodes dans un échantillon de zooplancton. La fonction geom_bar() se charge d’ajouter les barres verticales dans le graphique. La hauteur de ces barres correspond au nombre d’observations rencontrées dans le jeu de données pour chaque niveau (ou classe, ou groupe) de la variable facteur représentée. 4.1.2 Effectifs par 2 facteurs # Importation des données biometry (biometry &lt;- read(&quot;biometry&quot;, package = &quot;BioDataScience&quot;, lang = &quot;FR&quot;)) # # A tibble: 395 x 7 # gender day_birth weight height wrist year_measure age # &lt;fct&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 H 1995-03-11 69 182 15 2013 18 # 2 H 1998-04-03 74 190 16 2013 15 # 3 H 1967-04-04 83 185 17.5 2013 46 # 4 H 1994-02-10 60 175 15 2013 19 # 5 F 1990-12-02 48 167 14 2013 23 # 6 F 1994-07-15 52 179 14 2013 19 # 7 F 1971-03-03 72 167 15.5 2013 42 # 8 F 1997-06-24 74 180 16 2013 16 # 9 H 1972-10-26 110 189 19 2013 41 # 10 H 1945-03-15 82 160 18 2013 68 # # … with 385 more rows # Conversion de la variable year_measure de numérique à facteur biometry$year_measure &lt;- as.factor(biometry$year_measure) label(biometry$year_measure) &lt;- &quot;Année de la mesure&quot; Différentes représentations sont possibles pour observer des dénombrements tenant compte de plusieurs variables facteurs. Par défaut, l’argument position = a pour valeur par défaut stack (donc, lorsque cet argument n’est pas précisé dans geom_bar()). a &lt;- chart(data = biometry, ~ gender) + geom_bar() + ylab(&quot;Effectifs&quot;) b &lt;- chart(data = biometry, ~ gender %fill=% year_measure) + geom_bar() + ylab(&quot;Effectifs&quot;) + scale_fill_viridis_d() combine_charts(list(a, b), common.legend = TRUE) Figure 4.3: Dénombrement des hommes (H) et des femmes (F) dans l’étude sur l’obésité en Hainaut en tenant compte des années de mesure pour (B). Il existe d’autres options en utilisant la valeur dodge ou fill pour l’argument position =. a &lt;- chart(data = biometry, ~ gender %fill=% year_measure) + geom_bar(position = &quot;stack&quot;) + ylab(&quot;Effectifs&quot;) + scale_fill_viridis_d() b &lt;- chart(data = biometry, ~ gender %fill=% year_measure) + geom_bar(position = &quot;dodge&quot;) + ylab(&quot;Effectifs&quot;) + scale_fill_viridis_d() c &lt;- chart(data = biometry, ~ gender %fill=% year_measure) + geom_bar(position = &quot;fill&quot;) + ylab(&quot;Fractions&quot;) + scale_fill_viridis_d() combine_charts(list(a, b, c), common.legend = TRUE) Figure 4.4: Dénombrement des hommes (H) et des femmes (F) dans l’étude sur l’obésité en Hainaut en tenant compte des années de mesure (différentes présentations). Soyez vigilant à la différence entre l’argument position = stack et position = fill qui malgré un rendu semblable ont l’axe des ordonnées qui diffère (dans le cas de fill, il s’agit de la fraction par rapport au total qui est représentée, et non pas des effectifs absolus dénombrés). Pièges et Astuces Réordonner la variable facteur par fréquence Vous pouvez avoir le souhait d’ordonner votre variable facteur afin d’améliorer le rendu visuel de votre graphique. Pour cela, vous pouvez employer la fonction fct_infreq(). chart(data = copepoda, ~ fct_infreq(class)) + geom_bar() + labs(x = &quot;Classe&quot;, y = &quot;Effectifs&quot;) Figure 4.5: Dénombrement des classes de copépodes du jeu de données zooplankton. Rotation des axes du graphique en barre Lorsque les niveaux dans la variable étudiée sont trop nombreux, les légendes en abscisse risquent de se chevaucher, comme dans la Fig. 4.6 chart(data = zooplankton, ~ class) + geom_bar() + ylab(&quot;Effectifs&quot;) Figure 4.6: Dénombrement des classes du jeu de données zooplankton. Avec la fonction coord_flip() ajoutée à votre graphique, vous pouvez effectuer une rotation des axes pour obtenir un graphique en barres horizontales. De plus, l’œil humain perçoit plus distinctement les différences de longueurs horizontales que verticales. Donc, de ce point de vue, le graphe en barres horizontal est considéré comme meilleur que le graphe en barres verticales. chart(data = zooplankton, ~ class) + geom_bar() + ylab(&quot;Effectifs&quot;) + coord_flip() Figure 4.7: Dénombrement des classes du jeu de données zooplankton (version avec barres horizontales). Pour en savoir plus Graphes en barres à l’aide de ggplot2. Un tutoriel en français utilisant la fonction ggplot(). L’annotation des barres est également présentée. Page d’aide de la fonction geom_bar() en anglais. Autres exemples de graphes en barres à l’aide de `ggplot(). 4.1.3 Valeurs moyennes Le graphique en barres peut être aussi employé pour résumer des données numériques via la moyenne. Il ne s’agit plus de dénombrer les effectifs d’une variable facteur mais de résumer des données numériques en fonction d’une variable facteur. On peut exprimer cette relation dans R sous la forme de \\[y \\sim x\\] que l’on peut lire : \\[y \\ en \\ fonction \\ de \\ x\\] Avec y une variable numérique et x une variable facteur. Considérez l’échantillon suivant : 1, 71, 55, 68, 78, 60, 83, 120, 82 ,53, 26 Calculez la moyenne sur base de la formule de la moyenne \\[\\overline{y} = \\sum_{i = 1}^n \\frac{y_i}{n}\\] # Création du vecteur x &lt;- c(1, 71, 55, 68, 78, 60, 83, 120, 82, 53, 26) # Calcul de la moyenne mean(x) # [1] 63.36364 Les instructions pour produire ce graphe en barres à l’aide de chart() sont : chart(data = copepoda, size ~ class) + stat_summary(geom = &quot;col&quot;, fun.y = &quot;mean&quot;) Figure 4.8: Exemple de graphique en barres représentant les moyennes de tailles par groupe zooplanctonique. Ici, nous faisons appel à une autre famille de fonctions : celles qui effectuent des calculs sur les données avant de les représenter graphiquement. Le graphe en barres pour représenter les moyennes est très répandu dans le domaine scientifique malgré le grand nombre d’arguments en sa défaveur et que vous pouvez lire dans la section pour en savoir plus ci-dessous. L’un des arguments le plus important est la faible information qu’il véhicule puisque l’ensemble des données n’est plus représenté que par une valeur (la moyenne) pour chaque niveau de la variable facteur. Pour un petit nombre d’observations, il vaut mieux toutes les représenter à l’aide d’un nuage de points. Si le nombre d’observations devient très grand (dizaines ou plus), le graphique en boites de dispersion est plus indiqué (voir plus loin dans ce module). Pour en savoir plus Beware of dynamite. Démonstration de l’impact d’un graphe en barres pour représenter la moyenne (et l’écart type) = graphique en “dynamite”. Dynamite plots : unmitigated evil? Une autre comparaison du graphe en dynamite avec des représentations alternatives qui montre que le premier peut avoir quand même quelques avantages dans des situations particulières. "],
["graphique-en-camembert.html", "4.2 Graphique en camembert", " 4.2 Graphique en camembert Le graphique en camembert (ou en parts de tarte, ou encore appelé diagramme circulaire, pie chart en anglais) vous permettra de visualiser un dénombrement d’observations par facteur, tout comme le graphique en barres. chart(data = copepoda, ~ factor(0) %fill=% class) + geom_bar(width = 1) + coord_polar(&quot;y&quot;, start = 0) + theme_void() + scale_fill_viridis_d() Figure 4.9: Exemple de graphique en camembert montrant les effectifs des niveaux d’une variable facteur. Ce graphique est plus difficile à réaliser à l’aide de chart() ou ggplot(). En fait, il faut ruser ici, et l’auteur du package ggplot2 n’avait tout simplement pas l’intention d’ajouter ce type de graphique dans la panoplie proposée. En effet, il faut savoir que l’œil humain est nettement moins bon pour repérer des angles que pour comparer des longueurs. Donc, le diagramme en barres est souvent meilleur pour comparer des effectifs par classes. Mais d’une part, le graphique en camembert est (malheureusement) un graphique très répandu et il faut savoir l’interpréter, et d’autre part, il peut s’avérer quand même utile dans certaines situations. Notez l’utilisation de la fonction theme_void() qui crée un graphique sans axes. Pièges et astuces Partons d’un exemple fictif pour vous convaincre qu’un graphique en barres est souvent plus lisible qu’un graphique en camembert. Combien d’observations comptez-vous pour la lettre H ? # Warning: `data_frame()` is deprecated, use `tibble()`. # This warning is displayed once per session. Figure 4.10: Arrivez-vous à lire facilement des valeurs sur un graphique en camenbert (une échelle y est ajoutée de manière exceptionnelle pour vous y aider). Maintenant, effectuez le même exercice sur base d’un graphique en barres, combien d’observations pour la lettre H ? Figure 4.11: Dénombrement des niveaux d’une variable facteur sur un graphique en barres. Dans ce dernier cas, c’est bien plus facile : il y a effectivement 24 observations relatives à la lettre H. Pour en savoir plus Graphique en camembert à l’aide de la fonction ggplot(). Explications en français des différentes étapes pour passer d’un graphique en barres à un graphique en camembert avec ggplot2. Autre explication en français, également accompagnée d’informations sur les bonnes pratiques en matière de graphique en camembert. Save the pies for dessert est une démonstration détaillée des méfaits du graphique en camembert (le graphique en camembert, un graphique puant ? Pourrait-on peut-être titrer en français). Les côtés positifs du graphe en camembert sont mis en évidence dans ce document (en anglais). "],
["boxplot.html", "4.3 Boite de dispersion", " 4.3 Boite de dispersion Vous souhaitez représenter graphiquement cette fois un résumé d’une variable numérique mesurée sur un nombre (relativement) important d’individus, soit depuis une dizaine jusqu’à plusieurs millions. Vous souhaitez également conserver de l’information sur la distribution des données, et voulez éventuellement comparer plusieurs distributions entre elles : soit différentes variables, soit différents niveaux d’une variable facteur. Nous avons déjà vu au module 3 les diagrammes en violon et en lignes de crêtes pour cet usage. Nous allons étudier ici les boites de dispersion (encore appelée boite à moustaches) comme option alternative intéressante. La boite de dispersion représentera graphiquement cinq descripteurs appelés les cinq nombres. Considérez l’échantillon suivant : 1, 71, 55, 68, 78, 60, 83, 120, 82 ,53, 26 Ordonnez-le de la plus petite à la plus grande valeur : # Créer du vecteur x &lt;- c(1, 71, 55, 68, 78, 60, 83, 120, 82, 53, 26) # Ordonner le vecteur par ordre croissant sort(x) # [1] 1 26 53 55 60 68 71 78 82 83 120 Le premier descripteur des cinq nombres est la médiane qui est la valeur se situant à la moitié des observations, donc, avec autant d’observations plus petites et d’observations plus grande qu’elle. La médiane sépare l’échantillon en deux. median(x) # [1] 68 Les quartiles séparent l’échantillon en quatre. Le premier quartile (Q1) sera la valeur pour laquelle 25% des observations seront plus petites. Elle se situe donc entre la valeur minimale et la médiane. Cette médiane est égale au second quartile (50% des observations plus petites). Le troisième quartile (Q3) est la valeur pour laquelle 75% des observations de l’échantillon sont plus petites13. Enfin, la valeur minimale et la valeur maximale observées dans l’échantillon complètent ces cinq nombres qui décrivent de manière synthétique la position et l’étendue des observations. Les cinq nombres sont : la valeur minimale, le premier quartile, la médiane (ou deuxième quartile), le troisième quartile et la valeur maximale. Voici comment on les calcules facilement dans R : fivenum(x) # [1] 1 54 68 80 120 La boite de dispersion est une représentation graphique codifiée de ces cinq nombres. La représentation de x sous forme de nuage de points n’est ni très esthétique, ni très lisible, surtout si nous avons affaire à des milliers ou des millions d’observations qui se chevauchent sur le graphique14. Figure 4.12: Nuage de points univarié. La boite de dispersion va remplacer cette représentation peu lisible par un objet géométrique qui représente les cinq nombres. Figure 4.13: A) Nuage de points annoté avec les cinq nombres représentés par des traits horizontaux. B) Boite de dispersion obtenue pour les même données que A. Vous observez à la Fig. 4.13 que certaines valeurs minimales et maximales ne sont pas reliées à la boite de dispersion, il s’agit de valeurs extrêmes. Règle pour déterminer s’il y a des valeurs extrêmes avec une boite de dispersion : une valeur est considérée comme extrême si son écart par rapport à la boite est supérieur à une fois et demi la hauteur de la boite (encore appelée espace inter-quartile IQR correspondant à Q3 - Q1). Les tiges (ou “moustaches”) qui prolongent la boite de dispersion s’arrêtent donc aux dernières valeurs les plus petites et plus grandes, mais qui rentrent encore dans une fois et demi l’IQR. Les valeurs extrêmes sont ensuite représentées individuellement par un point au dessus et en dessous. La boite de dispersion finale ainsi que sa description sont représentées à la Fig. 4.14 ci-dessous. Figure 4.14: A) Boite de dispersion pour x et B) description des différents éléments constitutifs. Les instructions dans R pour produire un graphique en boites de dispersion parallèles (comparaison de la distribution d’une variable numérique pour différents niveaux d’une autre variable facteur) sont : chart(data = copepoda, size ~ class) + geom_boxplot() Figure 4.15: Distribution des tailles par groupes taxonomiques pour le zooplancton. La formule à employer est YNUM (size) ~ XFACTOR (class). Ensuite, pour réaliser une boite de dispersion vous devez ajouter la fonction geom_boxplot(). 4.3.1 Taille de l’échantillon Lors de la réalisation de boites de dispersion, vous devez être vigilant au nombre d’observations qui se cachent sous chacune d’elles. En effet, réaliser une boite de dispersion à partir d’échantillons ne comportant que cinq valeurs ou moins n’a aucun sens ! Figure 4.16: Piège des boites de dispersion : trop peu d’observations disponibles pour a. La boite de dispersion A est calculée à partir de seulement quatre observations. C’est trop peu. Comme les points représentant les observations ne sont habituellement pas superposés à la boite, cela peut passer inaperçu et tromper le lecteur ! Une bonne pratique consiste à ajouter n, le nombre d’observations au-dessus de chaque boite. Cela peut se faire facilement avec les fonctions give_n() et stat_summary() ci-dessous. give_n &lt;- function(x) c(y = max(x) * 1.1, label = length(x)) chart(data = copepoda, size ~ class) + geom_boxplot() + stat_summary(fun.data = give_n, geom = &quot;text&quot;, hjust = 0.5) Figure 4.17: Taille de copépodes pour différents groupes taxonomiques (le nombre d’observations est indiqué au dessus de chaque boite). 4.3.2 En fonction de 2 facteurs La Fig. 4.18 présente un graphique en boites de dispersion parallèles qui combine l’usage de deux variables facteurs différentes. # Importation du jeu de données ToothGrowth (tooth_growth &lt;- read(&quot;ToothGrowth&quot;, package = &quot;datasets&quot;)) # # A tibble: 60 x 3 # len supp dose # &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; # 1 4.2 VC 0.5 # 2 11.5 VC 0.5 # 3 7.3 VC 0.5 # 4 5.8 VC 0.5 # 5 6.4 VC 0.5 # 6 10 VC 0.5 # 7 11.2 VC 0.5 # 8 11.2 VC 0.5 # 9 5.2 VC 0.5 # 10 7 VC 0.5 # # … with 50 more rows # Remaniement et labelisation du jeu de données tooth_growth$dose &lt;- as.ordered(tooth_growth$dose) tooth_growth &lt;- labelise(tooth_growth, self = FALSE, label = list( len = &quot;Longueur des dents&quot;, supp = &quot;Supplémentation&quot;, dose = &quot;Dose&quot; ), units = list( len = &quot;mm&quot;, supp = NA, dose = &quot;mg/J&quot; ) ) # Réalisation graphique chart(data = tooth_growth, len ~ supp %fill=% dose) + geom_boxplot() + stat_summary(fun.data = give_n, geom = &quot;text&quot;, hjust = 0.5, position = position_dodge(0.75)) Figure 4.18: Croissance de dents de cochons d’Inde en fonction de la supplémentation (OJ = jus d’orange, VC = vitamine C) et de la dose administrée (n indiqué au dessus de chaque boite). Pour en savoir plus Un tutoriel boites de dispersion à l’aide de ggplot() présentant encore bien d’autres variantes possibles. Box plots in ggplot2. Autre explication en anglais avec sortie utilisant plotly. Grouped box plots. Explication plus détaillée sur les cinq nombres, en anglais. Notez que, lorsque la coupure tombe entre deux observations, une valeur intermédiaire est utilisée. Ici par exemple, le premier quartile est entre 53 et 55, donc, il vaut 54. Le troisième quartile se situe entre 78 et 82. Il vaut donc 80.↩ Il est possible de modifier la transparence des points et/ou de les déplacer légèrement vers la gauche ou vers la droite de manière aléatoire pour résoudre le problème de chevauchement des points sur un graphique en nuage de points univarié.↩ "],
["figures-composees.html", "4.4 Figures composées", " 4.4 Figures composées Il arrive fréquemment de vouloir combiner plusieurs graphiques dans une même figure. Plusieurs fonctions sont à votre disposition pour cela. Il faut tout d’abord distinguer deux types de figures multi-graphiques : Soit il s’agit d’un seul graphique que vous souhaitez subdiviser par rapport à une ou des variables facteurs. Soit il s’agit de graphiques indépendants que vous souhaitez assembler dans une même figure parce que les données ont un lien entre elles, ou parce que ces graphiques sont complémentaires pour comprendre les données. Dans le premier cas, les fonctions facet_XXX() comme facet_grid() peuvent être employées. Dans le second cas, la fonction combine_charts() est l’une des alternatives possibles. 4.4.1 Facettes L’une des règles les plus importantes que vous devez impérativement garder à l’esprit lors de la réalisation de vos graphiques est la simplicité. Au plus votre graphique contiendra d’information au plus il sera compliqué à décoder par vos lecteurs. # Importation de données relative à la croissance de poulets (chick_weight &lt;- read(&quot;ChickWeight&quot;, package = &quot;datasets&quot;)) # # A tibble: 578 x 4 # weight Time Chick Diet # &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt; &lt;fct&gt; # 1 42 0 1 1 # 2 51 2 1 1 # 3 59 4 1 1 # 4 64 6 1 1 # 5 76 8 1 1 # 6 93 10 1 1 # 7 106 12 1 1 # 8 125 14 1 1 # 9 149 16 1 1 # 10 171 18 1 1 # # … with 568 more rows # Réalisation du graphique (points semi-transparents) chart(data = chick_weight, weight ~ Time %col=% Diet) + geom_point(alpha = 0.5) + labs(x = &quot;Age [j]&quot;, y = &quot;Masse [g]&quot;) Figure 4.19: Croissance de poulets en utilisant quatre aliments différents. Le graphique à la Fig. 4.19 est mal adapté pour montrer les différences entre les quatre aliments : tous les points sont entremêlés. Il peut typiquement être simplifié en utilisant des facettes pour représenter les résultats relatifs aux différents régimes alimentaires sur des graphiques séparés. L’information est la même mais la lecture est beaucoup plus aisée. chart(data = chick_weight, weight ~ Time | Diet) + geom_point(alpha = 0.5) + labs(x = &quot;Age [j]&quot;, y = &quot;Masse [g]&quot;) Figure 4.20: Croissance de poulets en utilisant quatre aliments différents (1-4). Vous observez que les échelles en abscisse et en ordonnée sont similaires sur tous les graphiques. Cela permet une meilleure comparaison. Notez toutefois que, plus le nombre de facettes augmente, plus chaque graphique individuel devient petit. Faites attention à ne pas finir avec des graphiques individuels tellement petits qu’ils en deviennent illisibles ! 4.4.2 Graphiques assemblés La fonction combine_charts() permet de combiner plusieurs graphiques dans une figure unique. Nous l’avons déjà utilisée à plusieurs reprises. Cette fonction attend une liste de graphiques de type chart() à assembler. # Importation des données urchin &lt;- read(&quot;urchin_bio&quot;, package = &quot;data.io&quot;, lang = &quot;FR&quot;) # Réalisation des graphiques a &lt;- chart(data = urchin, weight ~ height %col=% origin) + geom_point() b &lt;- chart(data = urchin, weight ~ solid_parts %col=% origin) + geom_point() # Combinaison des graphiques dans une même figure combine_charts(list(a, b), common.legend = TRUE) Figure 4.21: A) Masse d’oursins en fonction de leur taille et de leur origine. B) Masse totale en fonction de la masse des parties solides de ces mêmes oursins. Il existe d’autres fonctions permettant de combiner plusieurs graphiques comme plot_grid() du package cowplot, mais avec combine_charts() vous pourrez déjà faire beaucoup. De plus, un libellé sous forme d’une lettre majuscule est automatiquement associé à chaque sous-région de la figure composée. Cela permet d’y faire plus facilement référence dans le texte et/ou dans la légende. Pour en savoir plus Partitionnement des graphiques en facettes. Différentes options sont présentées ici. Figures composées à l’aide de grid.arrange(). Une autre option, mais moins flexible et moins riche que combine_charts(). Figures composées à l’aide de plot_grid() avec les différentes options, aussi disponibles avec combine_charts(). Troisième possibilité pour des figures composées à l’aide de ggarrange(). combine_charts() fait la même chose, mais avec des valeurs par défaut légèrement différentes (labels = &quot;auto&quot; par défaut pour ce dernier, mais labels = NULL pour ggarrange()). "],
["differents-moteurs-graphiques.html", "4.5 Différents moteurs graphiques", " 4.5 Différents moteurs graphiques Prolifération des standards d’après xkcd. Depuis le début, l’ensemble des graphiques que nous vous avons proposés utilise la fonction chart() du package chart. Cependant, il ne s’agit pas de la seule fonction permettant de réaliser des graphiques dans R, loin de là. En fait, chart est tout récent et a été développé pour homogénéiser autant que possible les graphiques issus de trois moteurs graphiques différents : ggplot2, lattice et les graphiques base. La fonction chart() a d’autres avantages également : Un thème par défaut qui est le plus proche possible d’un rendu typique d’une publication scientifique. La possibilité d’utiliser l’interface formule avec ggplot2. La cohérence des objets graphiques obtenus qui peuvent tous êtres combinés en une figure composée, même si ils sont produits avec des moteurs graphiques différents. Un libellé automatique des axes et autres éléments du graphique en fonction des attributs label et units des variables (pour l’instant, seulement les graphiques de type ggplot2). # Importation des données (urchin &lt;- read(&quot;urchin_bio&quot;, package = &quot;data.io&quot;, lang = &quot;FR&quot;)) # # A tibble: 421 x 19 # origin diameter1 diameter2 height buoyant_weight weight solid_parts # &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 Pêche… 9.9 10.2 5 NA 0.522 0.478 # 2 Pêche… 10.5 10.6 5.7 NA 0.642 0.589 # 3 Pêche… 10.8 10.8 5.2 NA 0.734 0.677 # 4 Pêche… 9.6 9.3 4.6 NA 0.370 0.344 # 5 Pêche… 10.4 10.7 4.8 NA 0.610 0.559 # 6 Pêche… 10.5 11.1 5 NA 0.610 0.551 # 7 Pêche… 11 11 5.2 NA 0.672 0.605 # 8 Pêche… 11.1 11.2 5.7 NA 0.703 0.628 # 9 Pêche… 9.4 9.2 4.6 NA 0.413 0.375 # 10 Pêche… 10.1 9.5 4.7 NA 0.449 0.398 # # … with 411 more rows, and 12 more variables: integuments &lt;dbl&gt;, # # dry_integuments &lt;dbl&gt;, digestive_tract &lt;dbl&gt;, # # dry_digestive_tract &lt;dbl&gt;, gonads &lt;dbl&gt;, dry_gonads &lt;dbl&gt;, # # skeleton &lt;dbl&gt;, lantern &lt;dbl&gt;, test &lt;dbl&gt;, spines &lt;dbl&gt;, # # maturity &lt;int&gt;, sex &lt;fct&gt; # Réalisation du graphique chart(data = urchin, height ~ weight %col=% origin) + geom_point() Figure 4.22: Graphique typique obtenu avec chart() : rendu par défaut publiable tel quel, et libellé automatique des axes avec les unités. 4.5.1 ggplot2 Le moteur graphique ggplot2 est écrit pas Hadley Wickham, un personnage emblématique de la “révolution tidyverse” qui propose une surcouche moderne au dessus de R. ggplot2 implémente une “grammaire graphique” particulièrement puissante et flexible, proposée et popularisée par le statisticien Leland Wilkinson. Par défaut, chart() crée en réalité un graphique ggplot2 adapté. Voici la version ggplot2 standard du même graphique représenté à la Fig. 4.22 : ggplot(data = urchin, mapping = aes(x = weight, y = height, col = origin)) + geom_point() Figure 4.23: Graphique typique obtenu avec ggplot() (moteur graphique ggplot2). En comparant les Figs. 4.22 et 4.23 (en faisant abstraction des instructions R utilisées pour l’instant), plusieurs points sautent immédiatement aux yeux: Le thème par défaut de ggplot2 est très reconnaissable avec un quadrillage blanc sur fond gris clair. On aime ou on n’aime pas, mais il est clair que (1) ce n’est pas une présentation “standard” d’un graphique scientifique, et (2) le thème tord un peu le cou à une règle importante pour réaliser un graphique de qualité : minimiser la quantité d’“encre” nécessaire pour représenter un graphique, autrement dit, plus le graphique est simple et sobre, mieux c’est. Le thème par défaut de chart() respecte mieux tout ceci15. La taille des caractères est légèrement plus grande dans la Fig. 4.22 réalisée avec chart(). Le manque de lisibilité des parties textuelles dans un graphique est un défaut fréquent, dépendant de la résolution et de la taille de reproduction du graphique dans le document final. Le choix de chart() recule un peu ce risque. chart() est capable d’aller lire les métadonnées (libellés en français et unités des variables) et les utilisent automatiquement pour proposer des libellés corrects et complets des axes par défaut. ggplot() ne peut pas le faire, et il faut utiliser la fonction labs() pour l’indiquer manuellement. De manière générale, par rapport à ggplot(), chart() a été conçu pour produire le graphique le plus proche d’un rendu final impeccable avec tous les paramètres par défaut. Quelques règles simples vous permettent de passer des instructions ggplot() à chart() et vice versa16 : On peut toujours remplacer ggplot() par chart() dans les instructions R (à condition que le package chart soit chargé bien sûr, par exemple via SciViews::R). Dans ce cas, le thème par défaut diffère, et le libellé automatique des axes (non disponible avec ggplot()) est activé. Avec chart() on peut utiliser aes() pour spécifier les “esthétiques” (éléments à visualiser sur le graphique) comme pour ggplot(), mais on peut aussi utiliser une interface formule plus compacte. Cette interface formule rapproche la version chart() des graphiques ggplot2 d’un autre moteur de graphique dans R : lattice. Outre les esthétiques classiques x et y, l’interface formule de chart() permet d’en inclure d’autres directement dans la formule à l’aide d’opérateurs spécifiques %&lt;esth&gt;%=. Par exemple, aes(x = weight, y = height, col = origin) dans la Fig. 4.23 se traduit en la formule plus concise height ~ weight %col=% origin avec chart() (notez la position inversée de x et y dans la formule puisqu’on a y ~ x). Tous les esthétiques de ggplot2 sont supportés de cette manière. Partout où aes() est utilisé pour les instructions ggplot2, on peut utiliser à la place f_aes() et y spécifier plutôt une formule de type chart(). Avec ggplot() les facettes doivent être spécifiées à l’aide de facet_XXX(). A condition d’utiliser chart(), il est possible d’inclure les spécifications des facettes les plus utilisées directement dans la formule en utilisant l’opérateur |. Cette façon de procéder est, encore une fois, identique à ce qui se fait dans lattice (voir plus loin). Le point (5) mérite une petite démonstration pour comparaison : a &lt;- chart(data = urchin, height ~ weight | origin) + geom_point() b &lt;- ggplot(data = urchin, mapping = aes(x = weight, y = height)) + geom_point() + facet_grid( ~ origin) combine_charts(list(a, b)) Figure 4.24: Graphique à facettes. A. version chart(), B. version ggplot(). 4.5.2 lattice Autant ggplot2 est complètement modulable en ajoutant littéralement à l’aide de l’opérateur + des couches successives sur le graphique, autant lattice vise à réaliser les graphiques en une seule instruction. lattice utilise également abondamment l’interface formule pour spécifier les variables à utiliser dans le graphique. La version lattice du graphique d’exemple est présentée à la Fig. 4.25. xyplot(height ~ weight, data = urchin, groups = origin, auto.key = TRUE) Figure 4.25: Graphique exemple réalisé avec lattice. Et voici la version chart() utilisant le moteur lattice. Notez la façon d’appeler la fonction xyplot() de lattice via chart$xyplot() : theme_sciviews_lattice(n = 2) a &lt;- chart$xyplot(height ~ weight, data = urchin, groups = origin, auto.key = list(space = &quot;right&quot;, title = &quot;Origine&quot;, cex.title = 1, columns = 1), ylab = &quot;Hauteur du test [mm]&quot;, xlab = &quot;Masse totale [g]&quot;, par.settings = list(superpose.symbol = list(col = scales::hue_pal()(2)))) b &lt;- chart(data = urchin, height ~ weight %col=% origin) + geom_point() combine_charts(list(a, b)) Figure 4.26: Graphique exemple réalisé avec chart() A. avec le moteur lattice, B. avec le moteur ggplot2. La quantité d’instructions nécessaires pour rendre la version lattice proche de la version ggplot2 devrait disparaître dans les prochaines versions de chart(). Un autre objectif est aussi de gommer le plus possible les différences entre les rendus des différents moteurs de graphiques R, et en particuliers entre ggplot2 et lattice. Comparez la Fig. 4.26A avec la Fig. 4.25 pour apprécier le gain déjà obtenu en matière d’homogénéisation. Par rapport à ggplot2, les graphiques lattice sont moins flexibles du fait qu’ils doivent être spécifiés en une seule instruction. Cependant, ils sont beaucoup plus rapides à générer (appréciable quand il y a beaucoup de points à tracer) ! lattice offre également quelques types de graphiques non supportés par ggplot2 comme les graphiques en 3D à facettes, par exemple. Voici un graphique à facettes réalisé avec chart() et le moteur lattice. Notez que la formule utilisée est identique à cette employée pour la version ggplot2 avec chart(). chart$xyplot(data = urchin, height ~ weight | origin, scales = list(alternating = 1), xlab = &quot;Masse totale [g]&quot;, ylab = &quot;Hauteur du test [mm]&quot;) Figure 4.27: Graphique à facettes, avec chart() version lattice. Mise à part les instructions additionnelles encore nécessaires dans cette version de chart(), l’appel et le rendu sont très similaires par rapport à la version ggplot2 du même graphique avec chart() : chart(data = urchin, height ~ weight | origin) + geom_point() Figure 4.28: Graphique à facettes, avec chart() version ggplot2. 4.5.3 Graphiques de base Comme son nom le suggère, le moteur graphique de base est celui qui est implémenté de manière natif dans R. Il est donc utilisé un peu partout. Il est vieillissant et est plus difficile à manipuler que ggplot2 certainement, et même que lattice. Néanmoins, il est très flexible et rapide, … mais son rendu par défaut n’est plus vraiment au goût du jour. Voici notre graphique d’exemple rendu avec le moteur graphique R de base : plot(urchin$weight, urchin$height, col = c(&quot;red&quot;, &quot;darkgreen&quot;)[urchin$origin], pch = 1) legend(x = 80, y = 10, legend = c(&quot;Culture&quot;, &quot;Pêcherie&quot;), col = c(&quot;red&quot;, &quot;darkgreen&quot;), pch = 1) Figure 4.29: Graphique exemple réalisé avec le moteur graphique R de base. Vous rencontrerez très fréquemment la fonction plot(). C’est une fonction dite générique dont le comportement change en fonction de l’objet fourni en premier argument. Ainsi, elle réalise le graphique le plus pertinent à chaque fois en fonction du contexte. Notez tout de suite les instructions un peu confuses nécessaires pour spécifier la couleur souhaitée en fonction de l’origine des oursins. Le moteur graphique de base ne gère pas automatiquement des aspects plus complexes du graphique, telle que le positionnement d’une légende. Donc, à moins d’avoir prévu la place suffisante avant de tracer le graphique, nous ne pouvons que l’inclure à l’intérieur du cadre du graphique dans un second temps à l’aide de la fonction legend(). Comme cette dernière ne comprend rien à ce qui a été réalisé jusqu’ici, il faut lui respécifier les couleurs, formes et tailles de points utilisés ! C’est un des aspects pénibles du moteur graphique R de base. Voici maintenant une version chart() de ce graphique de base : chart$base({ par(mar = c(5.1, 4.1, 4.1, 6.1)) plot(urchin$weight, urchin$height, col = scales::hue_pal()(2)[urchin$origin], pch = 19, cex = 0.8, xlab = &quot;Masse totale [g]&quot;, ylab = &quot;Hauteur du test [mm]&quot;) legend(x = 105, y = 20, legend = c(&quot;Culture&quot;, &quot;Pêcherie&quot;), title = &quot;Origine&quot;, col = scales::hue_pal()(2), pch = 19, bty = &quot;n&quot;, cex = 0.8, y.intersp = 2) }) Figure 4.30: Graphique exemple réalisé avec le moteur graphique de base et la fonction chart(). Notez que le graphique est généré deux fois : une première fois dans un format propre aux graphiques R de base, et ensuite, il est traduit en une forme compatible avec les autres graphiques ggplot2 et lattice (et au passage, il gagne la grille en traits grisés). Dans le chunck, nous devons spécifier fig.keep = 2 pour éviter d’imprimer la première version dans le rapport lorsqu’on utilise chart$base(). Pour l’instant, le seul avantage de chart() avec les graphiques de base est qu’il les convertit en une forme combinable avec les autres graphiques dans une figure composite (sinon, ce n’est pas possible). A part cela, il faut fournir à chart$base() tout le code nécessaire pour tracer et personnaliser le graphique. Comme on peut le voir sur cet exemple, cela demande une quantité considérable de code. C’est aussi un autre aspect pénible de ce moteur graphique : il est très flexible, mais l’interface n’est pas optimale. Pour finir, les graphiques de base ont plus de mal avec les facettes, mais il peuvent quand même générer les versions les plus simples, par exemple à l’aide de la fonction coplot() qui accepte une formule très similaire à ce qui s’utilise avec lattice : coplot(data = urchin, height ~ weight | origin) Figure 4.31: Graphique à facettes avec le moteur graphique de base. A l’issue de cette comparaison, vous pourrez décider du moteur graphique que vous préférerez utiliser. Dans le cadre de ce cours, nous n’utiliserons en tous cas que quasi-exclusivement des graphiques ggplot2 créés à l’aide la fonction chart(). A vous de jouer Proposez cinq graphiques inédits (qui n’ont pas été vu jusqu’ici) dans vos différents projets. Employez par exemple les liens suivants pour vous inspirer : https://www.r-graph-gallery.com http://r-statistics.co/Top50-Ggplot2-Visualizations-MasterList-R-Code.html Terminez ce module en vérifiant que vous avez acquis l’ensemble des notions abordées. Ouvrez RStudio dans votre SciViews Box, puis exécutez l’instruction suivante dans la fenêtre console : BioDataScience::run(&quot;04a_test&quot;) Pour en savoir plus Chapitre Data visualisation de R for Data Science qui utilise ggplot(). Site rassemblant des extensions pour ggplot2 Introduction rapide à lattice Variantes de graphiques avec lattice Comparaison de lattice et ggplot2. Cette page fait aussi référence à un ensemble de graphiques différents générés en lattice et en ggplot2 pour comparaison (en anglais). Divers exemples de graphiques réalisés avec le moteur de base Autres exemples de graphiques R de base ggplot2 comparé aux graphiques R de base. Un point de vue différent d’un utilisateur habitué aux graphiques R de base (en anglais). Notez que plusieurs thèmes existent dans ggplot2. Il est facile d’en changer et des les personnaliser… mais c’est toujours appréciable d’avoir un rendu impeccable dès le premier essai.↩ Etant donné l’abondante littérature écrite sur ggplot2, il est utile de pouvoir convertir des exemples ggplot2 en graphiques chart(), si vous êtes convaincu par cette nouvelle interface.↩ "],
["import.html", "Module 5 Traitement des données I", " Module 5 Traitement des données I Objectifs Savoir importer des données depuis différents formats et différentes sources via la fonction read(). Appréhender les types de variables et l’importance de les encoder convenablement. Etre capable de convertir des variables d’un type à l’autre, y compris par l’utilisation du découpage en classes pour passer de variable quantitative à qualitative. Savoir remanier des variables, filtrer un tableau et le résumer afin d’en extraire l’information importante. Prérequis Le contenu du module 1 doit être parfaitement maîtrisé. Il est également souhaitable, mais pas indispensable, de comprendre comment réaliser des graphiques dans R pour pouvoir comprendre le contenu de ce module. "],
["importation-des-donnees.html", "5.1 Importation des données", " 5.1 Importation des données Il est possible d’encoder des très petits jeux de données dans R. La fonction tribble() permet de le faire facilement. Notez que les noms des colonnes du tableau sont à rentrer sous forme de formules (~var), que chaque entrée est séparée par une virgule, et que les chaines de caractères sont entourées de guillemets. Les espaces sont optionnels et peuvent être utilisés pour aligner les données afin que le tout soit plus lisible. Des commentaires peuvent être utilisés éventuellement en fin de ligne (un dièse # suivi du commentaire). small_dataset &lt;- tribble( ~treatment, ~dose, ~response, &quot;control&quot;, 0.5, 18.35, &quot;control&quot;, 1.0, 26.43, # This value needs to be double-checked &quot;control&quot;, 2.0, 51.08, &quot;test&quot; , 0.5, 10.29, &quot;test&quot; , 1.0, 19.92, &quot;test&quot; , 2.0, 41.06) # Print the table small_dataset # # A tibble: 6 x 3 # treatment dose response # &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 control 0.5 18.4 # 2 control 1 26.4 # 3 control 2 51.1 # 4 test 0.5 10.3 # 5 test 1 19.9 # 6 test 2 41.1 Dans la plupart des cas, vous utiliserez ou collecterez des données stockées dans des formats divers : feuilles Excel, fichiers CSV (“comma-separated-values”, un format standard d’encodage d’un tableau de données sous forme textuelle), formats spécifiques à divers logiciels statistiques comme SAS, Stata ou Systat, … Ces données peuvent être sur un disque local ou disponibles depuis un lien URL sur le net17. De nombreuses fonctions existent dans R pour importer toutes ces données. La fonction read() du package data.io est l’une des plus simples et conviviales d’entre-elles. Vous l’avez déjà utilisée, mais reprenons un exemple pour en discuter les détails. (biometry &lt;- read(&quot;biometry&quot;, package = &quot;BioDataScience&quot;, lang = &quot;fr&quot;)) # # A tibble: 395 x 7 # gender day_birth weight height wrist year_measure age # &lt;fct&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 M 1995-03-11 69 182 15 2013 18 # 2 M 1998-04-03 74 190 16 2013 15 # 3 M 1967-04-04 83 185 17.5 2013 46 # 4 M 1994-02-10 60 175 15 2013 19 # 5 W 1990-12-02 48 167 14 2013 23 # 6 W 1994-07-15 52 179 14 2013 19 # 7 W 1971-03-03 72 167 15.5 2013 42 # 8 W 1997-06-24 74 180 16 2013 16 # 9 M 1972-10-26 110 189 19 2013 41 # 10 M 1945-03-15 82 160 18 2013 68 # # … with 385 more rows Le jeu de données biometry est disponible dans le package R BioDataScience. Dans ce cas, il ne faut pas spécifier de chemin d’accès au fichier : R sait où le trouver tout seul. Il est également spécifié ici que la langue souhaitée est le français avec l’argument lang = &quot;fr&quot;. Le résultat de l’importation est assigné à la variable biometry(mais elle pourrait tout aussi bien porter un autre nom). Pour finir, le tout est entouré, de manière optionnelle, de parenthèses afin de forcer l’impression du résultat. Visualisez toujours votre tableau de données juste après l’importation. Vérifiez que les différentes colonnes ont été importées au bon format. En particulier, Les données numériques sont-elle bien comprises par R comme des nombres (&lt;dbl&gt; ou &lt;int&gt;) ? Les variables qualitatives ou semi-quantitatives sont importées comme chaines de caractères (&lt;chr&gt;) et doivent éventuellement être converties en variables de type facteur à l’aide de as.factor() ou facteur ordonné avec as.ordered(), voir plus loin. L’impression du tableau de données est une façon de voir cela, mais il y en a bien d’autres : essayez View(biometry), str(biometry), ou cliquez sur la petite icône bleue avec une flèche devant biometry dans l’onglet Environnement. Avant d’importer vos données dans R, vous devez vous poser les deux questions suivantes : Où ces données sont stockées ? Vous venez d’importer des données depuis un package R. Vous pouvez également les lire depuis un fichier sur le disque ou via une URL depuis le Web. Tous ces cas sont gérés par read() qui unifie donc de manière simple vos accès aux données. Quels est le format de vos données ? Souvent ce format est renseigné par l’extension du fichier. Par exemple .xlsx pour un Microsoft Excel ou .csv pour du “comma-separated-value”. Attention ! L’extension du fichier est cachée sous Windows, et parfois sous MacOS. Visualisez vos fichiers dans l’onglet Files dans RStudio pour voir leurs noms complets, avec les extensions. Pour l’instant, read() supporte 32 formats de fichiers différents, mais cette liste est amenée à s’agrandir à l’avenir. Pour découvrir les formats supportés, et les fonctions d’importation spécifiques appelées à chaque fois, utilisez : getOption(&quot;read_write&quot;) # # A tibble: 32 x 5 # type read_fun read_header write_fun comment # &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; # 1 csv readr::read_… data.io::hrea… readr::write_… comma separated values # 2 csv2 readr::read_… data.io::hrea… &lt;NA&gt; semicolon separated v… # 3 xlcsv readr::read_… data.io::hrea… readr::write_… write a CSV file more… # 4 tsv readr::read_… data.io::hrea… readr::write_… tab separated values # 5 fwf readr::read_… data.io::hrea… &lt;NA&gt; fixed width file # 6 log readr::read_… &lt;NA&gt; &lt;NA&gt; standard log file # 7 rds readr::read_… &lt;NA&gt; readr::write_… R data file (no compr… # 8 txt readr::read_… &lt;NA&gt; readr::write_… text file (as length … # 9 raw readr::read_… &lt;NA&gt; &lt;NA&gt; binary file (read as … # 10 ssv readr::read_… data.io::hrea… &lt;NA&gt; space separated value… # # … with 22 more rows Par la suite, vous allez apprendre à importer vos données depuis différentes sources. 5.1.1 Données sur le disque Lorsque l’extension du fichier reflète le format des données, il vous suffit juste d’indiquer le chemin d’accès au fichier à read(). La plupart du temps, cela suffira pour importer correctement les données. N’oubliez pas que le chemin d’accès à votre fichier peut s’écrire de manière absolue ou bien de manière relative. Vous devez autant que possible employer des chemins relatifs pour que votre projet soit portable. Si vous avez du mal à déterminer le chemin relatif par rapport à vos données, le snippet filerelchoose vous sera très utile : Assurez-vous que le chemin actif dans la fenêtre Console est le même que le répertoire contenant le fichier édité. Pour cela, utilisez l’entrée de menu RStudio Session -&gt; Set Working Directory -&gt; To Source File Location. Utilisez le snippet filerelchoose que vous activez dans une zone de code R (dans un script R, ou à l’intérieur d’un chunk dans un document R Markdown/R Notebook). Entrez file, attendez que le menu contextuel de complétion apparaisse, sélectionnez filerelchoose dans la liste et tapez Entrée. Une boite de dialogue de sélection de fichier apparaît. Sélectionnez le fichier qui vous intéresse et … file est remplacé par le chemin relatif vers votre fichier dans l’éditeur. Les explications détaillées concernant l’organisation de vos projets dans RStudio pour qu’ils soient portables, la gestion des chemins d’accès aux fichiers et les chemins relatifs sont détaillés dans l’annexe B, à la section B.1.1. C’est le moment de vérifier que vous avez bien compris et assimilé son contenu. Pièges et astuces Si l’extension est incorrecte, vous pouvez forcer un format de fichier particulier à l’importation en l’indiquant dans l’appel à read() comme read$&lt;ext&gt;(). Par exemple, pour forcer l’importation d’un fichier de type “comma-separated-values” pour un fichier qui se nommerait my_data.txt, vous écrirez read$csv(my_data.txt). Si les données ne sont pas importées correctement, cela signifie que les arguments d’importation par défaut ne sont pas adaptés. Les arguments à spécifier sont différents d’un format à l’autre. Voyez d’abord la fonction appelée en interne par read()dans le tableau obtenu via getOption(&quot;read_write&quot;). Par exemple, pour un fichier xlsx, il s’agit de la fonction readxl::read_excel() qui est utilisée. Ensuite, voyez l’aide de cette dernière fonction pour en découvrir les différents arguments (?readxl::read_excel). Là, vous pourrez découvrir les arguments sheet =qui indiquent la feuille à importer depuis le fichier (première feuille par défaut), ou range = qui indique la plage de données dans le feuille à utiliser (par défaut, depuis la cellule A1 en haut à gauche jusqu’à la fin du tableau). Donc, si votre fichier my_data.xlsx contient les feuilles sheet1, sheet2 et sheet3, et que les données qui vous intéressent sont dans la plage C5:E34 de sheet2, vous pourrez écrire: read(&quot;my_data.xlsx&quot;, sheet = &quot;sheet2&quot;, range = &quot;C5:E34&quot;). 5.1.2 Données depuis Internet Il existe différents logiciels qui permettent d’éditer des tableaux de données en ligne et de les partager sur le Net. Google Sheets est l’un d’entre eux, tout comme Excel Online. Des stockages spécifiques pour les données scientifiques existent aussi comme Figshare ou Zenodo. Ces sites permettent de partager facilement des jeux de données sur le Net. La science est de plus en plus ouverte, et les pratiques d’Open Data de plus en plus fréquentes et même imposées par des programmes de recherche comme les programmes européens ou le FNRS en Belgique. Vous serez donc certainement amenés à accéder à des données depuis des dépôts spécialisés sur Internet. Concentrez-vous sur les outils spécifiques à la gestion de ce type de données. il s’agit, en effet, d’une compétence clé qu’un bon scientifique des données se doit de maîtriser parfaitement. En recherchant à chaque fois la meilleure façon d’accéder à des données sur le Net, vous développerez cette compétence progressivement par la pratique… et vous pourrez faire valoir un atout encore rare mais apprécié lors d’un entretien d’embaûche plus tard. Voici un exemple de feuille de données Google Sheets : https://docs.google.com/spreadsheets/d/1iEuGrMk4IcCkq7gMNzy04DkSaPeWH35Psb0E56KEQMw. Il est possible d’importer ce genre de données directement depuis R, mais il faut d’abord déterminer l’URL à utiliser pour obtenir les données dans un format reconnu. Dans le cas de Google Sheets, il suffit d’indiquer à la fin de cette URL que l’on souhaite exporter les données au format CSV en rajoutant /export?format=csv à la fin de l’URL. Cette URL est très longue. Elle est peu pratique et par ailleurs, elle a toujours la même structure : &quot;https://docs.google.com/spreadsheets/d/{id}/export?format=csv&quot; avec {id} qui est l’identifiant unique de la feuille Google Sheets (ici 1iEuGrMk4IcCkq7gMNzy04DkSaPeWH35Psb0E56KEQMw). Vous pouvez indiquer explicitement ceci dans votre code et profiter des capacités de remplacement de texte dans des chaînes de caractères de la fonction glue::glue() pour effectuer un travail impeccable. googlesheets_as_csv &lt;- &quot;https://docs.google.com/spreadsheets/d/{id}/export?format=csv&quot; coral_id &lt;- &quot;1iEuGrMk4IcCkq7gMNzy04DkSaPeWH35Psb0E56KEQMw&quot; (coral_url &lt;- glue::glue(googlesheets_as_csv, id = coral_id)) # https://docs.google.com/spreadsheets/d/1iEuGrMk4IcCkq7gMNzy04DkSaPeWH35Psb0E56KEQMw/export?format=csv Vous n’aurez alors plus qu’à lire les données depuis cette URL. N’oubliez pas non plus de spécifier à read() que les données sont à lire au format CSV en utilisant read$csv() : (coral &lt;- read$csv(coral_url)) # Parsed with column specification: # cols( # localisation = col_character(), # species = col_character(), # id = col_double(), # salinity = col_double(), # temperature = col_double(), # date = col_datetime(format = &quot;&quot;), # time = col_double(), # gain = col_double(), # gain_std = col_double() # ) # # A tibble: 98 x 9 # localisation species id salinity temperature date # &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt; # 1 A0 s.hyst… 1 34.7 24.5 2018-04-24 09:10:00 # 2 A0 s.hyst… 2 34.7 24.5 2018-04-24 09:10:00 # 3 A0 s.hyst… 3 34.7 24.5 2018-04-24 09:10:00 # 4 A0 s.hyst… 4 34.7 24.5 2018-04-24 09:10:00 # 5 A0 s.hyst… 5 34.7 24.5 2018-04-24 09:10:00 # 6 A0 s.hyst… 6 34.7 24.5 2018-04-24 09:10:00 # 7 A0 s.hyst… 7 34.7 24.5 2018-04-24 09:10:00 # 8 A0 s.hyst… 8 34.7 24.5 2018-04-24 09:10:00 # 9 A0 s.hyst… 9 34.7 24.5 2018-04-24 09:10:00 # 10 A0 s.hyst… 10 34.7 24.5 2018-04-24 09:10:00 # # … with 88 more rows, and 3 more variables: time &lt;dbl&gt;, gain &lt;dbl&gt;, # # gain_std &lt;dbl&gt; Lorsque vous travaillez sur des données issues d’une source externe, et donc susceptibles d’être modifiées ou même pire, de disparaître. Il est préférable d’enregistrer une copie locale de ces données dans votre projet (dans le sous-dossier data de préférence). Si vous travaillez exclusivement avec R, l’un des meilleurs formats est RDS, un format natif qui conservera toutes les caractéristiques de votre objet, y compris sa classe, et d’éventuels attributs18. Par défaut, les données seront stockées non compressées, mais vous pourrez aussi décider de compresser avec les algorithmes &quot;gz&quot; (plus rapide et répandu), &quot;bz2&quot; (intermédiaire), ou &quot;xz&quot; (le plus efficace en taux de compression mais aussi le plus lent et gourmand en ressources CPU). Par exemple, pour enregistrer les données avec compression &quot;gz&quot;, vous écrirez : write$rds(coral, file = &quot;../data/coral.rds&quot;, compress = &quot;gz&quot;) Ensuite, vous pourrez simplement charger ces données plus loin depuis la version locale dans votre R Markdown comme ceci : coral &lt;- read(&quot;../data/coral.rds&quot;) Attention, ne supprimez jamais l’instruction permettant de retrouver vos données sur Internet sous prétexte que vous avez maintenant une copie locale à disposition. C’est le lien, le fil conducteur vers les données originales. Vous pouvez soit mettre l’instruction en commentaire en ajoutant un dièse devant, soit soustraire le chunk de l’évaluation en indiquant eval=FALSE dans son entête. Faites-en de même avec l’instruction write(). Ainsi, le traitement de vos données commencera à l’instruction read() et vous partirez de la copie locale. Si jamais vous voulez effectuer une mise à jour depuis la source initiale, il sera toujours possible de dé-commenter les instructions, ou de passer le chunk à eval=TRUE temporairement (ou encore plus simplement, forcez l’exécution du chunk dans l’éditeur en cliquant sur la petite flèche verte en haut à gauche du chunk). Pièges et astuces Comme il s’agit seulement d’une copie des données originelles, vous pouvez choisir de ne pas inclure le fichier .rds dans le système de gestion de version de Git. C’est très simple : il suffit d’ajouter une entrée .rds dans le fichier .gitignore à la racine de votre dépôt, et tous les fichiers avec cette extension seront ignorés. Notez toutefois que, si vous partagez votre projet sur GitHub, les données locales n’y apparaitront pas non plus. D’une part, cela décharge le système de gestion de version, et d’autre part, les gros fichiers de données n’ont pas vraiment leur place sur GitHub. Cependant, soyez conscient que quelqu’un qui réalise un clone ou un fork de votre dépôt devra d’abord réimporter lui aussi localement les données avant de pouvoir travailler, ce qui implique de bien comprendre le mécanisme que vous avez mis en place. Documentez-le correctement, avec une note explicite dans le fichier README.md, par exemple. Les données originales ne sont peut-être pas présentées de la façon qui vous convient. Cela peut nécessiter un travail important de préparation du tableau de données. Au fur et à mesure que le ou les chunks d’importation/préparation des données augmentent en taille, ils deviennent de plus en plus gênants dans un document consacré à l’analyse de ces données. Si c’est le cas, vous avez deux options possibles : Séparer votre R Markdown en deux. Un premier document dédié à l’importation/préparation des données et un second qui se concentre sur l’analyse. Une bonne pratique consiste à numéroter les fichiers en tête pour qu’ils apparaissent par ordre logique lorsqu’ils sont listés par ordre alphabétique (01_import.Rmd, 02_analysis.Rmd). Effectuer le travail d’importation/préparation du tableau de données dans un script R. Dans le R Markdown, vous pouvez ajouter l’instruction (commentée ou placée dans un chunk eval=FALSE) pour “sourcer” ce script R afin de réimporter/retraiter vos données : #source(&quot;../R/data-import.R&quot;) Si le travail de préparation des données est lourd (et donc, prend beaucoup de temps) il peut être avantageux d’enregistrer localement la version nettoyée de vos données plutôt que la version originale. Mais alors indiquez-le explicitement. Faites toujours la distinction entre données brutes et données nettoyées. Ne les mélangez jamais et documentez toujours de manière reproductible le processus qui mène des unes aux autres ! C’est tout aussi important que de garder un lien vers la source originale des données dans votre code et d’utiliser toujours des chemins relatifs vers vos fichiers pour une analyse portable et reproductible. 5.1.3 Données depuis un package Les packages R comme data.io, chart ou encore flow, fournissent une série de fonctions supplémentaires. Certains d’entre eux proposent également des jeux de données. Ici aussi, read() permet de les récupérer, même si c’est la fonction data() qui est souvent utilisée à cet effet dans R. Comparons read() et data() dans le cas des données issues de packages R. Avec data(), vous n’assignez pas le jeu de données à un nom. Ce nom vous est imposé comme le nom initial du jeu de données : data(&quot;urchin_bio&quot;, package = &quot;data.io&quot;) # package = optionnel si déjà chargé Le jeu de données urchin_bio n’est pas véritablement chargé dans l’environnement utilisateur avec data(). Seulement une “promesse” de chargement (Promise) est enregistrée. Voyez dans l’onglet Environnement ce qui apparaît. Ce n’est qu’à la première utilisation du jeu de données que le tableau est véritablement chargé. Par exemple : head(urchin_bio) # origin diameter1 diameter2 height buoyant_weight weight solid_parts # 1 Fishery 9.9 10.2 5.0 NA 0.5215 0.4777 # 2 Fishery 10.5 10.6 5.7 NA 0.6418 0.5891 # 3 Fishery 10.8 10.8 5.2 NA 0.7336 0.6770 # 4 Fishery 9.6 9.3 4.6 NA 0.3697 0.3438 # 5 Fishery 10.4 10.7 4.8 NA 0.6097 0.5587 # 6 Fishery 10.5 11.1 5.0 NA 0.6096 0.5509 # integuments dry_integuments digestive_tract dry_digestive_tract gonads # 1 0.3658 NA 0.0525 0.0079 0 # 2 0.4447 NA 0.0482 0.0090 0 # 3 0.5326 NA 0.0758 0.0134 0 # 4 0.2661 NA 0.0442 0.0064 0 # 5 0.4058 NA 0.0743 0.0117 0 # 6 0.4269 NA 0.0492 0.0097 0 # dry_gonads skeleton lantern test spines maturity sex # 1 0 0.1793 0.0211 0.0587 0.0995 0 &lt;NA&gt; # 2 0 0.1880 0.0205 0.0622 0.1053 0 &lt;NA&gt; # 3 0 0.2354 0.0254 0.0836 0.1263 0 &lt;NA&gt; # 4 0 0.0630 0.0167 0.0180 0.0283 0 &lt;NA&gt; # 5 0 NA NA NA NA 0 &lt;NA&gt; # 6 0 NA NA NA NA 0 &lt;NA&gt; Regardez à nouveau dans l’onglet Environnement. Ce coup-ci urchin_bio apparaît bien dans la section Data et l’icône en forme de petit tableau à la droite qui permet de le visualiser est enfin accessible. La fonction read() permet de choisir librement le nom que nous souhaitons donner à notre jeu de données. Si nous voulons l’appeler urchin au lieu de urchin_bio, pas de problèmes. De plus, il est directement chargé et accessible dans l’onglet Environnement (en effet, si on utilise une instruction qui charge un jeu de données, c’est très vraissemblablement parce que l’on souhaite ensuite le manipuler depuis R, non ?). urchin &lt;- read(&quot;urchin_bio&quot;, package = &quot;data.io&quot;) Nous avons déjà vu que read() donne accès également dans certains cas à des métadonnées (par exemple le label et les unités des jeux de données) dans différentes langues, ce que ne permet pas data(). Enfin, la syntaxe et la fonction utilisée sont pratiquement identiques pour charger des données depuis un fichier, depuis Internet ou depuis un package avec read(). C’est logique et facile à retenir. data() ne permet que de récupérer des données liées à un package R, et c’est tout ! Pour toutes ces raisons, nous préférons utiliser ici read() à data(). 5.1.3.1 Langue du jeu de données La fonction read() est également capable de lire un fichier annexe permettant de rajouter des métadonnées (données complémentaires) à notre tableau, comme les labels et les unités des variables en différentes langues. Lorsque l’on importe le jeu de données avec la fonction data(), ces métadonnées ne sont pas employées. data(&quot;urchin_bio&quot;, package = &quot;data.io&quot;) # Visualisation des données chart(urchin_bio, height ~ weight %col=% origin) + geom_point() Comparez ceci avec le même graphique, mais obtenu à partir de différentes versions du jeu de données urchin_bio importé à l’aide de read() avec des valeurs différentes pour l’argument lang =. urchin &lt;- read(&quot;urchin_bio&quot;, package = &quot;data.io&quot;) urchin_en &lt;- read(&quot;urchin_bio&quot;, package = &quot;data.io&quot;, lang = &quot;en&quot;) urchin_fr &lt;- read(&quot;urchin_bio&quot;, package = &quot;data.io&quot;, lang = &quot;fr&quot;) urchin_FR &lt;- read(&quot;urchin_bio&quot;, package = &quot;data.io&quot;, lang = &quot;FR&quot;) Les différences dans les labels sont observables sur le graphique ci-dessous. a &lt;- chart(urchin, height ~ weight %col=% origin) + geom_point() b &lt;- chart(urchin_en, height ~ weight %col=% origin) + geom_point() c &lt;- chart(urchin_fr, height ~ weight %col=% origin) + geom_point() d &lt;- chart(urchin_FR, height ~ weight %col=% origin) + geom_point() combine_charts(list(a, b, c, d)) A &amp; B : l’argument lang = par défaut est lang = &quot;en&quot;. Il utilise les labels et unités en anglais avec les unités dans le système international. C : l’argument lang = &quot;fr&quot; utilise les labels et unités en français. Il laisse cependant les niveaux des variables facteurs en anglais (Farm et Fishery) afin d’éviter de devoir changer les instructions de manipulation des données qui feraient référence à ces niveaux. D : l’argument lang = &quot;FR&quot; ajoute les labels et unités en français. De plus, il traduit également les niveaux des variables facteurs (Culture et Pêcherie). Il vous est conseillé d’employer l’argument lang = &quot;fr&quot; lors de vos différents travaux. La langue internationale en science est l’anglais et vous serez très certainement amené dans votre carrière scientifique à produire des documents en français et en anglais. L’utilisation de lang = &quot;fr&quot;rend le même code réutilisable sur la version française ou anglaise, contrairement à lang = &quot;FR&quot;. Observez les exemples ci-dessous. urchin_en %&gt;.% filter(., origin == &quot;Farm&quot;) %&gt;.% head(.) # # A tibble: 6 x 19 # origin diameter1 diameter2 height buoyant_weight weight solid_parts # &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 Farm 53.1 54.5 26.3 9.57 60.2 41.7 # 2 Farm 52.7 52.7 25.9 10.8 63.2 46.6 # 3 Farm 54 54.2 24.5 10.7 64.4 44.3 # 4 Farm 51.1 51.3 28.8 11.2 62.4 45.0 # 5 Farm 52.1 53.6 31.2 11.1 63.7 44.0 # 6 Farm 52.3 51.4 28.6 12.4 68.6 53.9 # # … with 12 more variables: integuments &lt;dbl&gt;, dry_integuments &lt;dbl&gt;, # # digestive_tract &lt;dbl&gt;, dry_digestive_tract &lt;dbl&gt;, gonads &lt;dbl&gt;, # # dry_gonads &lt;dbl&gt;, skeleton &lt;dbl&gt;, lantern &lt;dbl&gt;, test &lt;dbl&gt;, # # spines &lt;dbl&gt;, maturity &lt;int&gt;, sex &lt;fct&gt; urchin_fr %&gt;.% filter(., origin == &quot;Farm&quot;) %&gt;.% head(.) # # A tibble: 6 x 19 # origin diameter1 diameter2 height buoyant_weight weight solid_parts # &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 Farm 53.1 54.5 26.3 9.57 60.2 41.7 # 2 Farm 52.7 52.7 25.9 10.8 63.2 46.6 # 3 Farm 54 54.2 24.5 10.7 64.4 44.3 # 4 Farm 51.1 51.3 28.8 11.2 62.4 45.0 # 5 Farm 52.1 53.6 31.2 11.1 63.7 44.0 # 6 Farm 52.3 51.4 28.6 12.4 68.6 53.9 # # … with 12 more variables: integuments &lt;dbl&gt;, dry_integuments &lt;dbl&gt;, # # digestive_tract &lt;dbl&gt;, dry_digestive_tract &lt;dbl&gt;, gonads &lt;dbl&gt;, # # dry_gonads &lt;dbl&gt;, skeleton &lt;dbl&gt;, lantern &lt;dbl&gt;, test &lt;dbl&gt;, # # spines &lt;dbl&gt;, maturity &lt;int&gt;, sex &lt;fct&gt; Pas d’adaptation nécessaire du code pour passer de urchin_en à urchin_fr. urchin_FR %&gt;.% filter(., origin == &quot;Pêcherie&quot;) %&gt;.% head(.) # # A tibble: 6 x 19 # origin diameter1 diameter2 height buoyant_weight weight solid_parts # &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 Pêche… 9.9 10.2 5 NA 0.522 0.478 # 2 Pêche… 10.5 10.6 5.7 NA 0.642 0.589 # 3 Pêche… 10.8 10.8 5.2 NA 0.734 0.677 # 4 Pêche… 9.6 9.3 4.6 NA 0.370 0.344 # 5 Pêche… 10.4 10.7 4.8 NA 0.610 0.559 # 6 Pêche… 10.5 11.1 5 NA 0.610 0.551 # # … with 12 more variables: integuments &lt;dbl&gt;, dry_integuments &lt;dbl&gt;, # # digestive_tract &lt;dbl&gt;, dry_digestive_tract &lt;dbl&gt;, gonads &lt;dbl&gt;, # # dry_gonads &lt;dbl&gt;, skeleton &lt;dbl&gt;, lantern &lt;dbl&gt;, test &lt;dbl&gt;, # # spines &lt;dbl&gt;, maturity &lt;int&gt;, sex &lt;fct&gt; Le code a dû être modifier dans l’instruction filter() lors du passage à urchin_FR (Farm -&gt; Pêcherie). Bien évidemment, pour un rapport plus formel en français, tout doit être traduit en français et l’option lang = &quot;FR&quot; accompagnée d’une vérification et une adaptation éventuelle du code est à préférer dans ce cas précis. R permet également d’interroger des bases de données spécialisées, mais nous n’aborderons ce sujet spécifique qu’au cours de Science des Données Biologique 5 en Master 2.↩ Si vous devez aussi accéder à vos données à partir d’autres langages comme Python, Java ou C++, utilisez un format commun reconnu par les différents logiciels. Le CSV fonctionne généralement bien, mais des formats binaires plus performants sont également disponibles. Parmi ces formats “inter-langages”, gardez un œil sur Apache Arrow très prometteur et avec une version pour R qui sera disponible prochainement.↩ "],
["types-de-variables.html", "5.2 Types de variables", " 5.2 Types de variables Lors de la réalisation de graphiques dans les modules précédents vous avez compris que toutes les variables ne sont pas équivalentes. Certains graphiques sont plutôt destinés à des variables qualitatives (par exemple, graphique en barres), alors que d’autres représentent des données quantitatives comme le nuage de points. (biometry &lt;- read(&quot;biometry&quot;, package = &quot;BioDataScience&quot;, lang = &quot;fr&quot;)) # # A tibble: 395 x 7 # gender day_birth weight height wrist year_measure age # &lt;fct&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 M 1995-03-11 69 182 15 2013 18 # 2 M 1998-04-03 74 190 16 2013 15 # 3 M 1967-04-04 83 185 17.5 2013 46 # 4 M 1994-02-10 60 175 15 2013 19 # 5 W 1990-12-02 48 167 14 2013 23 # 6 W 1994-07-15 52 179 14 2013 19 # 7 W 1971-03-03 72 167 15.5 2013 42 # 8 W 1997-06-24 74 180 16 2013 16 # 9 M 1972-10-26 110 189 19 2013 41 # 10 M 1945-03-15 82 160 18 2013 68 # # … with 385 more rows La Figure 5.1 montre deux boites de dispersion parallèles différentes. Laquelle de ces deux représentations est incorrecte et pourquoi ? a &lt;- chart(biometry, height ~ gender %fill=% gender) + geom_boxplot() b &lt;- chart(biometry, height ~ weight %fill=% gender) + geom_boxplot() combine_charts(list(a, b), common.legend = TRUE) Figure 5.1: Boites de dispersion parallèles de la taille (height) en fonction de A. une variable qualitative (gender) et B. une variable quantitative (weight) et couleur en fonction de gender` C’est la figure 5.1B qui tente de représenter une variable quantitative numérique heightsous forme de boites de dispersion parallèles (correct), mais en fonction d’une variable de découpage en sous-ensemble (weight) qui est elle-même une variable quantitative, … alors qu’une variable qualitative telle que gender aurait dû être utilisée (comme dans la Fig. 5.1A). Dans le cas présent, R a bien voulu réaliser le graphique (avec juste un petit message d’avertissement), mais comment l’interpréter ? Dans d’autres situations, il vous renverra purement et simplement un message d’erreur. Les jeux de données, lorsqu’ils sont bien encodés (tableaux “cas par variables”, en anglais on parlera de tidy data), sont en fait un ensemble de variables en colonnes mesurées sur un ensemble d’individus en lignes. Vous avez à votre disposition plusieurs types de variables pour personnaliser le jeu de données. Deux catégories principales de variables existent, chacune avec deux sous-catégories : Les variables quantitatives sont issues de mesures quantitatives ou de dénombrements Les variables quantitatives continues sont représentées par des valeurs réelles (double dans R) Les variables quantitatives discrètes sont typiquement représentées par des entiers (integer dans R) Les variables qualitatives sont constituées d’un petit nombre de valeurs possibles (on parle des niveaux de la variables ou de leurs modalités) Les variables qualitatives ordonnées ont des niveaux qui peuvent être classés dans un ordre du plus petit au plus grand. elles sont typiquement représentées dans R par des objets ordered. Les variables qualitatives non ordonnées ont des niveaux qui ne peuvent être rangés et sont typiquement représentées par des objets factor en R Il existe naturellement encore d’autres types de variables. Les dates sont représentées, par exemple, par des objets Date, les nombres complexes par complex, les données binaires par raw, etc. La fonction skim() du package skimr permet de visualiser la classe de la variable et bien plus encore. Elle fournit un résumé différent en fonction du type de la variable et propose, par exemple, un histogramme stylisé pour les variables numériques comme le montre le tableau ci-dessous. skimr::skim(biometry) # Skim summary statistics # n obs: 395 # n variables: 7 # # ── Variable type:Date ─────────────────────────────────────────────────────────────────────────────────────────────────────── # variable missing complete n min max median n_unique # day_birth 0 395 395 1927-08-29 2000-08-11 1988-10-05 210 # # ── Variable type:factor ───────────────────────────────────────────────────────────────────────────────────────────────────── # variable missing complete n n_unique top_counts ordered # gender 0 395 395 2 M: 198, W: 197, NA: 0 FALSE # # ── Variable type:numeric ──────────────────────────────────────────────────────────────────────────────────────────────────── # variable missing complete n mean sd p0 p25 p50 p75 # age 0 395 395 35.34 17.32 15 19 27 50 # height 0 395 395 170.71 9.07 146 164 171 177 # weight 0 395 395 71.2 15.45 41.5 59 69.3 80 # wrist 2 393 395 16.65 1.67 10 15.5 16.5 18 # year_measure 0 395 395 2015.32 1.61 2013 2014 2016 2017 # p100 hist # 89 ▇▂▁▅▂▁▁▁ # 193 ▁▂▆▆▇▅▃▁ # 131 ▂▇▇▆▂▁▁▁ # 23 ▁▁▂▇▇▂▁▁ # 2017 ▅▅▁▁▁▅▁▇ Avec une seule instruction, on obtient une quantité d’information sur notre jeu de données comme le nombre d’observations, le nombre de variables et un traitement spécifique pour chaque type de variable. Cette instruction permet de visualiser et d’appréhender le jeu de données mais ne doit généralement pas figurer tel quel dans un rapport d’analyse. "],
["conversion-de-variables.html", "5.3 Conversion de variables", " 5.3 Conversion de variables Il est possible de convertir les variables seulement dans un sens : du plus détaillé au moins détaillé, c’est-à-dire, quantitatif continu -&gt; quantitatif discret -&gt; qualitatif ordonné -&gt; qualitatif non ordonné. 5.3.1 Quantitatif continu à discret R essaye de gommer autant que possible la distinction entre nombres integer et double tous deux rassemblés en numeric. Si besoin, la conversion se fait automatiquement. En pratique, concentrez-vous essentiellement sur les objets numeric pour tout ce qui est quantitatif. Un nombre tel que 1 est considéré par R comme un double par défaut. Si vous vouliez expressément spécifier que c’est un entier, vous pouvez le faire en ajoutant un L majuscule derrière le nombre. Ainsi, 1L est compris par R comme l’entier 1. Encore une fois, cette distinction explicite est rarement nécessaire dans R. Si vous voulez arrondir des nombres, vous pouvez utiliser la fonction round() avec son argument digits = qui indique le chiffre derrière la virgule qui doit être arrondi (0 par défaut). Pour arrondir vers l’entier le plus proche vers le haut, utilisez floor() et pour le plus proche vers le bas, employez ceiling(). (x &lt;- seq(-1, 1, by = 0.1) + 0.01) # [1] -0.99 -0.89 -0.79 -0.69 -0.59 -0.49 -0.39 -0.29 -0.19 -0.09 0.01 # [12] 0.11 0.21 0.31 0.41 0.51 0.61 0.71 0.81 0.91 1.01 round(x) # [1] -1 -1 -1 -1 -1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 round(x, digits = 1) # [1] -1.0 -0.9 -0.8 -0.7 -0.6 -0.5 -0.4 -0.3 -0.2 -0.1 0.0 0.1 0.2 0.3 # [15] 0.4 0.5 0.6 0.7 0.8 0.9 1.0 ceiling(x) # [1] 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 2 floor(x) # [1] -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 0 0 0 0 0 0 0 0 0 0 1 5.3.2 Quantitatif à qualitatif Le traitement diffère selon le nombre de valeurs différentes rencontrées dans le jeu de données. Si une variable numérique contient en réalité un petit nombre de valeurs différentes, il suffit de convertir la classe de l’objet de numeric vers factor ou ordered pour que R comprenne que la variable doit être traitée comme une variable qualitative. Un exemple concret l’illustre ci-dessous. Si, par contre, le nombre de valeurs différentes est important (dizaines ou plus) alors il va falloir créer des regroupements. C’est le découpage en classes abordé plus loin. Voici un jeu de données qui étudie l’allongement des dents chez le cobaye en fonction de la supplémentation alimentaire en acide ascorbique. tooth &lt;- read(&quot;ToothGrowth&quot;, package = &quot;datasets&quot;, lang = &quot;fr&quot;) Le jeu de données comprend 60 observations effectuées sur des cochons d’Inde. Ces derniers reçoivent deux types de suppléments alimentaires : soit du jus d’orange (OJ), soit de la vitamine C (VC). Des lots différents reçoivent des doses différentes d’acide ascorbique via ces suppléments, soit 0.5, 1, ou 2 mg/j. Vous pouvez inspecter ces données rapidement avec la fonction skim(). skimr::skim(tooth) # Skim summary statistics # n obs: 60 # n variables: 3 # # ── Variable type:factor ───────────────────────────────────────────────────────────────────────────────────────────────────── # variable missing complete n n_unique top_counts ordered # supp 0 60 60 2 OJ: 30, VC: 30, NA: 0 FALSE # # ── Variable type:numeric ──────────────────────────────────────────────────────────────────────────────────────────────────── # variable missing complete n mean sd p0 p25 p50 p75 p100 # dose 0 60 60 1.17 0.63 0.5 0.5 1 2 2 # len 0 60 60 18.81 7.65 4.2 13.07 19.25 25.27 33.9 # hist # ▇▁▇▁▁▁▁▇ # ▃▅▃▅▃▇▂▂ La variable dose est encodée sous forme numérique alors que cette dernière ne contient que trois niveaux différents et devra être le plus souvent traitée comme une variable qualitative ordonnée à trois niveaux . Vous devrez donc probablement recoder cette variable en variable facteur. Ce n’est pas le caractère quantitatif ou qualitatif du mécanisme sous-jacent mesuré qui détermine si la variable est quantitative ou qualitative, mais d’autres critères comme la précision avec laquelle la mesure a été effectuée. Par exemple, un anémomètre mesure la vitesse du vent sous forme de variable quantitative alors qu’une échelle approximative de type vent nul, vent faible, vent moyen, vent fort ou tempête basée sur l’observation des rides ou des vagues à la surface de la mer pourrait éventuellement convenir pour mesurer le même phénomène si une grande précision n’est pas nécessaire. Mais dans ce cas, la variable devra être traitée comme une variable qualitative. De même, un plan expérimental qui réduit volontairement les valeurs fixées dans une expérience, comme ici les doses journalières d’acide ascorbique, fera aussi basculer la variable en qualitative, et ce, quelle que soit la précision avec laquelle les valeurs sont mesurées par ailleurs. Un découpage en classes aura aussi le même effet de transformer une variable quantitative en variable qualitative ordonnée. Indiquons à présent explicitement à R que la variable dose doit être considérée comme qualitative : tooth$dose &lt;- as.factor(tooth$dose) # Visualisation des données skimr::skim(tooth) # Skim summary statistics # n obs: 60 # n variables: 3 # # ── Variable type:factor ───────────────────────────────────────────────────────────────────────────────────────────────────── # variable missing complete n n_unique # dose 0 60 60 3 # supp 0 60 60 2 # top_counts ordered # 0.5: 20, 1: 20, 2: 20, NA: 0 FALSE # OJ: 30, VC: 30, NA: 0 FALSE # # ── Variable type:numeric ──────────────────────────────────────────────────────────────────────────────────────────────────── # variable missing complete n mean sd p0 p25 p50 p75 p100 # len 0 60 60 18.81 7.65 4.2 13.07 19.25 25.27 33.9 # hist # ▃▅▃▅▃▇▂▂ Vous pouvez (et devez !) cependant aller encore plus loin car la variable est en réalité qualitative ordonnée, et doit être représentée par un objet “facteur ordonné” (ordered) plutôt que factor. Il y a en effet, une progression dans les doses administrées. Lors de la conversion, R considère les différents niveaux par ordre alphabéthique par défaut. Ici cela convient, mais ce n’est pas toujours le cas. Il vaut donc mieux spécifier explicitement l’ordre des niveaux dans l’argument optionnel levels =. Cela donne : tooth$dose &lt;- ordered(tooth$dose, levels = c(0.5, 1, 2)) # Visualisation des données skimr::skim(tooth) # Skim summary statistics # n obs: 60 # n variables: 3 # # ── Variable type:factor ───────────────────────────────────────────────────────────────────────────────────────────────────── # variable missing complete n n_unique # dose 0 60 60 3 # supp 0 60 60 2 # top_counts ordered # 0.5: 20, 1: 20, 2: 20, NA: 0 TRUE # OJ: 30, VC: 30, NA: 0 FALSE # # ── Variable type:numeric ──────────────────────────────────────────────────────────────────────────────────────────────────── # variable missing complete n mean sd p0 p25 p50 p75 p100 # len 0 60 60 18.81 7.65 4.2 13.07 19.25 25.27 33.9 # hist # ▃▅▃▅▃▇▂▂ Les fonctions as.factor() ou factor() et as.ordered() ou ordered() effectuent cette conversion de character ou numeric vers des objets factor ou ordered. Une variable facteur ordonnée sera alors reconnue comme telle par un ensemble de fonction dans R. Elle ne sera, de ce fait, pas traitée de la même manière qu’une variable facteur non ordonnée, ni même qu’une variable numérique. Soyez bien attentif à l’encodage correct des données dans R avant d’effectuer vos graphiques et vos analyses. 5.3.3 Découpage en classes La conversion d’une variable quantitative à qualitative doit souvent passer par une réduction des niveaux en rassemblant les valeurs proches dans des classes. Vous avez déjà utilisé de manière implicite le découpage en classes lorsque vous avez réalisé des histogrammes. Si les histogrammes sont bi- ou multimodaux, un découpage se justifie. Par exemple, le jeu de données portant sur la biométrie humaine est typique d’un cas de distribution bimodale. En fait, ce sont des étudiants (ayant tous une vingtaine d’années) qui ont réalisé ces mesures. La plupart ont choisi de s’inclure dans l’échantillon, d’où un premier mode vers une vingtaine d’années. Ensuite, ils ont pu mesurer d’autres personnes, éventuellement dans leur entourage. Beaucoup ont demandé à leurs parents, ce qui résulte en un second mode vers la cinquantaine19. Donc, la distribution bimodale résulte plus de l’échantillonnage en lui-même que d’une réalité démographique ! Cela ne change cependant rien pour l’exercice. biometry &lt;- read(&quot;biometry&quot;, package = &quot;BioDataScience&quot;, lang = &quot;fr&quot;) chart(data = biometry, ~ age) + geom_histogram(bins = 20) + ylab(&quot;Effectifs&quot;) Les addins de RStudio vont vous permettre de réaliser facilement un découpage du jeu de données en fonction de classes d’âges (bouton Addins -&gt; QUESTIONR -&gt; Numeric range dividing). Vous spécifiez le découpage voulu dans une boite de dialogue sur base de l’histogramme et lorsque vous cliquez sur le bouton Done, le code R qui effectue ce découpage est inséré dans l’éditeur RStudio à l’endroit du curseur. La nouvelle variable facteur age_rec basée sur le découpage en classes sera ensuite utile pour faire ressortir de l’information supplémentaire en contrastant les individus plus jeunes et ceux plus âgés. # Instructions obtenues à partir de l&#39;addins biometry$age_rec &lt;- cut(biometry$age, include.lowest = FALSE, right = TRUE, breaks = c(14, 27, 90)) # Visualisation de la variable facteur obtenue chart(biometry, formula = ~ age %fill=% age_rec) + geom_histogram(bins = 20) + ylab(&quot;Effectifs&quot;) 5.3.4 Qualitatif ordonné ou non Les données qualitatives sont souvent représentées par du texte (nom d’une couleur par exemple) et importées sous forme de chaines de caractère (character) par défaut dans R à partir de la fonction read(). Vous devez les convertir de manière explicite à l’aide de as.factor(), factor(), as.ordered() ou ordered() par la suite. Voici un exemple : df &lt;- tibble( color = c(&quot;blue&quot;, &quot;green&quot;, &quot;blue&quot;, &quot;red&quot;, &quot;green&quot;), intensity = c(&quot;low&quot;, &quot;low&quot;, &quot;high&quot;, &quot;mid&quot;, &quot;high&quot;)) df # # A tibble: 5 x 2 # color intensity # &lt;chr&gt; &lt;chr&gt; # 1 blue low # 2 green low # 3 blue high # 4 red mid # 5 green high # Conversion en factor (color) et ordered (intensity) df$color &lt;- factor(df$color, levels = c(&quot;red&quot;, &quot;green&quot;, &quot;blue&quot;)) df$intensity &lt;- ordered(df$intensity, levels = c(&quot;low&quot;, &quot;mid&quot;, &quot;high&quot;)) df # # A tibble: 5 x 2 # color intensity # &lt;fct&gt; &lt;ord&gt; # 1 blue low # 2 green low # 3 blue high # 4 red mid # 5 green high # Information plus détaillée str(df) # Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 5 obs. of 2 variables: # $ color : Factor w/ 3 levels &quot;red&quot;,&quot;green&quot;,..: 3 2 3 1 2 # $ intensity: Ord.factor w/ 3 levels &quot;low&quot;&lt;&quot;mid&quot;&lt;&quot;high&quot;: 1 1 3 2 3 skimr::skim(df) # Skim summary statistics # n obs: 5 # n variables: 2 # # ── Variable type:factor ───────────────────────────────────────────────────────────────────────────────────────────────────── # variable missing complete n n_unique top_counts # color 0 5 5 3 gre: 2, blu: 2, red: 1, NA: 0 # intensity 0 5 5 3 low: 2, hig: 2, mid: 1, NA: 0 # ordered # FALSE # TRUE Les différents niveaux des variables factor ou ordered sont et doivent rester entièrement de votre responsabilité. Certains aspects anciens de R essayent de gérer cela pour vous, mais ces fonctions ou options (StringsAsFactor = par exemple) tendent heureusement à être remplacées par des versions moins assertives. De même, les niveaux ne sont pas réduits lorsque vous filtrez un tableau pour ne retenir que certains niveaux. Vous devez indiquer explicitement ensuite que vous voulez éliminer les niveaux vides du tableau avec la fonction droplevels(). Le jeu de données iris contient des données relatives à trois espèces différentes (table() permet de compter le nombre d’observations pour chaque niveau d’une variable qualitative factor ou ordered) : iris &lt;- read(&quot;iris&quot;, package = &quot;datasets&quot;, lang = &quot;fr&quot;) table(iris$species) # # setosa versicolor virginica # 50 50 50 Si nous restreignons le tableau aux 20 premiers individus, cela donne : iris20 &lt;- iris[1:20, ] table(iris20$species) # # setosa versicolor virginica # 20 0 0 Nous voyons que le tableau réduit iris20 ne contient des données que d’une seule espèce. Pourtant table() continue de lister les autres niveaux de la variable. Les niveaux connus sont aussi imprimés avec levels() : levels(iris20$species) # [1] &quot;setosa&quot; &quot;versicolor&quot; &quot;virginica&quot; Dans le cas ici, nous souhaitons uniquement nous focaliser sur l’espèce I. setosa. Dès lors, l’utilisation de la fonction droplevels() permet de faire disparaître les autres niveaux de la variable species. iris20$species &lt;- droplevels(iris20$species) levels(iris20$species) # [1] &quot;setosa&quot; table(iris20$species) # # setosa # 20 Notez que ceci ne constitue pas un échantillonnage correct par rapport à la population générale du Hainaut pour plusieurs raisons. (1) toutes les tranches d’âges ne sont échantillonnées de manière équivalente pour les raisons évoquées, (2) des liens génétiques existent au sein des familles, ce qui résulte en une non indépendance des observations entre elles, et (3) seule une sous-population constituée de personnes fréquentant l’université et de leur entourage a été échantillonnée. Cependant, dans le cadre de l’exercice, nous accepterons ces biais, tout en étant conscients qu’ils existent.↩ "],
["remaniement-des-donnees.html", "5.4 Remaniement des données", " 5.4 Remaniement des données Dans le module 4, vous avez réalisé vos premiers remaniements de données dans le cadre des graphiques en barres. Nous ne nous sommes pas étendu sur les fonctions utilisées à cette occasion. Le remaniement des données est une étape cruciale en analyse des données et il faut en maîtriser au moins les principaux outils. Heureusement, il est déjà possible d’aller loin en combinant une petite dizaine d’outils simples. Les cinq principaux (les plus utilisés) dans l’approche Tidyverse utilisée ici sont : sélectionner des colonnes au sein d’un jeu de données avec select() filtrer des lignes dans un jeu de données avec filter() calculer de nouvelles variables dans un jeu de données avec mutate() regrouper les données au sein d’un tableau avec group_by() résumer les variables d’un jeu de données avec summarise() Ces outils provenant du package dplyr sont décrits en détails dans le chapitre 5 de “R for Data Science”. Nous allons nous familiariser avec eux via une approche pratique sur base d’exemples concrets. urchin &lt;- read(&quot;urchin_bio&quot;, package = &quot;data.io&quot;, lang = &quot;fr&quot;) rmarkdown::paged_table(urchin) 5.4.1 select() Lors de l’utilisation de vos jeux de données, vous serez amené à réduire vos données en sous-tableau ne reprenant qu’un sous-ensemble des variables initiales. select() effectue cette opération20 : urchin1 &lt;- select(urchin, origin, solid_parts, test) rmarkdown::paged_table(urchin1) urchin2 &lt;- select(urchin, c(1, 4, 14)) rmarkdown::paged_table(urchin2) urchin3 &lt;- select(urchin, origin, contains(&quot;weight&quot;)) rmarkdown::paged_table(urchin3) urchin4 &lt;- select(urchin, ends_with(&quot;ht&quot;)) rmarkdown::paged_table(urchin4) 5.4.2 filter() De même que toutes les colonnes d’un tableau ne sont pas forcément utiles, il est souvent nécessaire de sélectionner les lignes en fonction de critères particuliers pour restreindre l’analyse à une sous-population données, ou pour éliminer les cas qui ne correspondent pas à ce que vous voulez. La fonction filter() effectue ce travail. Repartons du jeu de données urchin_bio simplifié à trois variables (urchin2). rmarkdown::paged_table(urchin2) Si vous voulez sélectionner uniquement un niveau lvl d’une variable facteur fact, vous pouvez utiliser un test de condition “égal à” (==) : fact == &quot;lvl&quot;. Notez bien le double signe égal ici, et n’oubliez pas d’indiquer le niveau entre guillemets. De même, vous pouvez sélectionner tout sauf ce niveau avec l’opérateur “différent de” (!=). Les opérateur “plus petit que” (&lt;) ou “plus grand que” (&gt;) fonctionnent sur les chaines de caractère selon une logique d’ordre alphabétique, donc, &quot;a&quot; &lt; &quot;b&quot;21. Comparaison Opérateur Exemple Égal à == fact == &quot;lvl&quot; Différent de != fact != &quot;lvl&quot; Plus grand que &gt; fact &gt; &quot;lvl&quot; Plus grand ou égal à &gt;= fact &gt;= &quot;lvl&quot; Plus petit que &lt; fact &lt; &quot;lvl&quot; Plus petit ou égale à &lt;= fact &lt;= &quot;lvl&quot; # Tous les oursins sauf ceux issus de la pêche urchin_sub1 &lt;- filter(urchin2, origin != &quot;Fishery&quot;) rmarkdown::paged_table(urchin_sub1) Vous pouvez aussi utiliser une variable numérique pour filtrer les données. Les comparaisons précédentes sont toujours applicables, sauf que cette fois vous faites porter la comparaison par rapport à une constante (ou par rapport à une autre variable numérique). # Oursins plus hauts que 20mm urchin_sub2 &lt;- filter(urchin2, height &gt; 20) rmarkdown::paged_table(urchin_sub2) Vous pouvez combiner différentes comparaisons avec les opérateurs “et” (&amp;) et “ou” (|) : # Oursins plus hauts que 20 mm ET issus d&#39;élevage (&quot;Farm&quot;) urchin_sub3 &lt;- filter(urchin2, height &gt; 20 &amp; origin == &quot;Farm&quot;) rmarkdown::paged_table(urchin_sub3) Avec des variables facteurs composées de nombreux niveaux comme on peut en retrouver dans le jeu de données zooplankton du package BioDataScience, vous pouvez être amené à sélectionner plusieurs niveaux au sein de cette variable. L’opérateur %in% permet d’indiquer que nous souhaitons garder tous les niveaux qui sont dans une liste. Il n’existe pas d’opérateur %not_in%, mais il suffit d’inverser le résultat en précédent l’instruction de ! pour obtenir cet effet. Par exemple, !letters %in% c(&quot;a&quot;, &quot;d&quot;, &quot;f&quot;) conserve toutes les lettres sauf a, d et f. L’opérateur ! est d’ailleurs utilisable avec toutes les comparaisons pour en inverser les effets. Ainsi, !x == 1 est équivalent à x != 1. zooplankton &lt;- read(&quot;zooplankton&quot;, package = &quot;data.io&quot;, lang = &quot;FR&quot;) # Garde uniquement les copépodes (correspondant à 4 groupes distincts) copepoda &lt;- filter(zooplankton, class %in% c(&quot;Calanoïde&quot;, &quot;Cyclopoïde&quot;, &quot;Harpacticoïde&quot;, &quot;Poecilostomatoïde&quot;)) rmarkdown::paged_table(select(copepoda, ecd:perimeter, class)) Enfin, la détection et l’élimination de lignes contenant des valeurs manquantes (encodées comme NA) est spéciale. En effet, vous ne pouvez pas écrire quelque chose comme x == NA car ceci se lit comme “x est égale à … je ne sais pas quoi”, ce qui renvoie à son tour NA pour toutes les comparaisons quelles qu’elles soient. Vous pouvez utiliser la fonction spécialement prévue pour ce test is.na(). Ainsi, is.na(x) effectue en réalité ce que vous voulez et peut être utilisée à l’intérieur de filter(). Cependant, il existe une fonction spécialement prévue pour débarrasser les tableaux des lignes contenant des valeurs manquantes : drop_na() du package tidyr. Si vous spécifier des noms de colonnes (facultatifs), la fonction ira rechercher les valeurs manquantes uniquement dans ces colonnes-là, sinon, elle scrutera tout le tableau. urchin_sub4 &lt;- drop_na(urchin) rmarkdown::paged_table(urchin_sub4) 5.4.3 mutate() La fonction mutate() permet de calculer de nouvelles variables (si le nom fourni n’existe pas encore dans le jeu de donnée) ou écrase les variables existantes de même nom. Repartons du jeu de données urchin. Pour calculer de nouvelles variables, vous pouvez employer : les opérateurs arithmétiques : addition : + soustraction : - multiplication : * division : / exposant : ^ modulo (reste lors d’une division entière) : %% division entière : %/% urchin &lt;- mutate(urchin, sum_skel = lantern + spines + test, ratio = sum_skel / skeleton, skeleton2 = skeleton^2) rmarkdown::paged_table(select(urchin, skeleton:spines, sum_skel:skeleton2)) les fonctions mathématiques : ln() ou log() (logarithme népérien), lg() ou log10() (logarithme en base 10) ln1p() ou log1p() (logarithme népérien de x + 1), ou lg1p() (logarithme en base 10 de x + 1) exp() (exponentielle, ex) et expm1() (ex - 1) sqrt() (racine carrée) sin(), cos(), tan() … urchin &lt;- mutate(urchin, skeleton_log = log(skeleton), skeleton_sqrt = sqrt(skeleton)) rmarkdown::paged_table(select(urchin, skeleton, skeleton_log, skeleton_sqrt)) La fonction transmute() effectue la même opération, mais en plus, elle laisse tomber les variables d’origine pour ne garder que les nouvelles variables calculées. 5.4.4 group_by() La fonction group_by() ne change rien dans le tableau lui-même, mais ajoute une annotation qui indique que les calculs ultérieurs devront être effectués sur des sous-ensembles du tableau en parallèle. Ceci est surtout utile avec summarise() (voir ci-dessous). Pour annuler le regroupement, il suffit d’utiliser ungroup(). urchin_by_orig &lt;- group_by(urchin, origin) head(urchin_by_orig) # # A tibble: 6 x 24 # # Groups: origin [1] # origin diameter1 diameter2 height buoyant_weight weight solid_parts # &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 Fishe… 9.9 10.2 5 NA 0.522 0.478 # 2 Fishe… 10.5 10.6 5.7 NA 0.642 0.589 # 3 Fishe… 10.8 10.8 5.2 NA 0.734 0.677 # 4 Fishe… 9.6 9.3 4.6 NA 0.370 0.344 # 5 Fishe… 10.4 10.7 4.8 NA 0.610 0.559 # 6 Fishe… 10.5 11.1 5 NA 0.610 0.551 # # … with 17 more variables: integuments &lt;dbl&gt;, dry_integuments &lt;dbl&gt;, # # digestive_tract &lt;dbl&gt;, dry_digestive_tract &lt;dbl&gt;, gonads &lt;dbl&gt;, # # dry_gonads &lt;dbl&gt;, skeleton &lt;dbl&gt;, lantern &lt;dbl&gt;, test &lt;dbl&gt;, # # spines &lt;dbl&gt;, maturity &lt;int&gt;, sex &lt;fct&gt;, sum_skel &lt;dbl&gt;, ratio &lt;dbl&gt;, # # skeleton2 &lt;dbl&gt;, skeleton_log &lt;dbl&gt;, skeleton_sqrt &lt;dbl&gt; identical(ungroup(urchin_by_orig), urchin) # [1] FALSE 5.4.5 summarise() Si vous voulez résumer vos données (calcul de la moyenne, médiane, etc.), vous pouvez réaliser ceci sur une variable en particulier avec les fonctions dédiées. Par exemple mean(urchin$skeleton) renvoie la masse moyenne de squelette pour tous les oursins (ce calcul donne NA dès qu’il y a des valeurs manquantes, mais l’argument na.rm = TRUE permet d’obtenir un résultat en ne tenant pas compte de ces données manquantes : mean(urchin$skeleton, na.rm = TRUE)). Cela devient vite laborieux s’il faut réitérer ce genre de calcul sur plusieurs variables du jeu de données, et assembler ensuite les résultats dans un petit tableau synthétique. D’autant plus, s’il faut séparer d’abord le jeu de données en sous-groupes pour faire ces calculs. La fonction summarise() reporte automatiquement ces calculs, en tenant compte automatiquement des regroupements proposés via group_by(). tooth &lt;- read(&quot;ToothGrowth&quot;, package = &quot;datasets&quot;, lang = &quot;fr&quot;) tooth_summary &lt;- summarise(tooth, &quot;moyenne&quot; = mean(len), &quot;minimum&quot; = min(len), &quot;médiane&quot; = median(len), &quot;maximum&quot; = max(len)) knitr::kable(tooth_summary, digits = 2, caption = &quot;Allongement des dents chez des cochons d&#39;Inde recevant de l&#39;acide ascorbique.&quot;) Tableau 5.1: Allongement des dents chez des cochons d’Inde recevant de l’acide ascorbique. moyenne minimum médiane maximum 18.81 4.2 19.25 33.9 Voici les mêmes calculs, mais effectués séparément pour les deux types de supplémentations alimentaires : tooth_by_supp &lt;- group_by(tooth, supp) tooth_summary2 &lt;- summarise(tooth_by_supp, &quot;moyenne&quot; = mean(len), &quot;minimum&quot; = min(len), &quot;médiane&quot; = median(len), &quot;maximum&quot; = max(len)) knitr::kable(tooth_summary2, digits = 2, caption = &quot;Allongement des dents chez des cochons d&#39;Inde en fonction du supplément jus d&#39;orange (OJ) ou vitamine C (VC).&quot;) Tableau 5.2: Allongement des dents chez des cochons d’Inde en fonction du supplément jus d’orange (OJ) ou vitamine C (VC). supp moyenne minimum médiane maximum OJ 20.66 8.2 22.7 30.9 VC 16.96 4.2 16.5 33.9 Pièges et astuces Tout comme lors de réalisation d’une boite de dispersion, vous devez être particulièrement vigilant au nombre d’observation par sous-groupe. Pensez toujours à ajoutez à chaque tableau de résumé des données, le nombre d’observation par sous-groupe grâce à la fonction n(). tooth_summary2 &lt;- summarise(tooth_by_supp, &quot;moyenne&quot; = mean(len), &quot;minimum&quot; = min(len), &quot;médiane&quot; = median(len), &quot;maximum&quot; = max(len), &quot;n&quot; = n()) knitr::kable(tooth_summary2, digits = 2, caption = &quot;Allongement des dents chez des cochons d&#39;Inde en fonction du supplément jus d&#39;orange (OJ) ou vitamine C (VC).&quot;) Tableau 5.3: Allongement des dents chez des cochons d’Inde en fonction du supplément jus d’orange (OJ) ou vitamine C (VC). supp moyenne minimum médiane maximum n OJ 20.66 8.2 22.7 30.9 30 VC 16.96 4.2 16.5 33.9 30 Voyez ?select_helpers pour une panoplie de fonctions supplémentaires qui permettent une sélection “intelligente” des variables.↩ L’ordre alphabétique qui fait également intervenir les caractères accentués diffère en fonction de la configuration du système (langue). L’état du système tel que vu par R pour le tri alphabétique est obtenu par Sys.getlocale(&quot;LC_COLLATE&quot;). Dans la SciViews Box, ceci est toujours &quot;en_US.UTF-8&quot;, ceci afin de rendre le traitement reproductible d’un PC à l’autre, qu’il soit en anglais, français, espagnol, chinois, ou n’importe quelle autre langue.↩ "],
["chainage-des-instructions.html", "5.5 Chaînage des instructions", " 5.5 Chaînage des instructions Le chaînage (ou “pipe” en anglais) permet de combiner une suite d’instructions R. Il permet une représentation facilement lisible et compréhensible d’un traitement décomposé en plusieurs étapes simples de remaniement des données. Différents opérateurs de chaînage existent dans R. Le Tidyverse et RStudio sont en faveur de l’adoption d’un opérateur de chaînage %&gt;% issu du package magrittr. Si nous sommes sensibles au clin d’œil fait ici à un artiste belge bien connu (“ceci n’est pas un pipe”), nous n’adhérons pas à ce choix pour des raisons multiples et plutôt techniques qui n’ont pas leur place dans ce document22. Nous vous présentons ici l’un des opérateurs de chaînage du package flow : %&gt;.%. Le jeu de données sur la biométrie humaine est employé pour cette démonstration qui va comparer le remaniement d’un tableau de données avec et sans l’utilisation du chaînage. biometry &lt;- read(&quot;biometry&quot;, package = &quot;BioDataScience&quot;, lang = &quot;fr&quot;) Vous vous intéressez à l’indice de masse corporelle ou IMC (BMI en anglais) des individus de moins de 25 ans. Vous souhaitez représenter la moyenne, la médiane et le nombre d’observations de manière séparée pour les hommes et les femmes. Pour obtenir ces résultats vous devez : calculer le BMI, filtrer le tableau pour ne retenir que les individus de moins de 25 ans, résumer les données afin d’obtenir la moyenne et la médiane par genre, afficher un tableau de données avec ces résultats. Il est très clair ici que le traitement peut être décomposé en étapes plus simples. Cela apparaît naturellement rien que dans la description de ce qui doit être fait. Sans l’utilisation de l’opérateur de chaînage, deux approches sont possibles : Imbriquer les instructions les unes dans les autres (très difficile à lire et à déboguer) : knitr::kable( summarise( group_by( filter( mutate(biometry, bmi = weight / (height/100)^2), age &lt;= 25), gender), mean = mean(bmi), median = median(bmi), number = n()), rows = NULL, digits = 1, col = c(&quot;Genre&quot;, &quot;Moyenne&quot;, &quot;Médiane&quot;, &quot;Observations&quot;), caption = &quot;IMC d&#39;hommes (M) et femmes (W) de 25 ans maximum.&quot; ) Tableau 5.4: IMC d’hommes (M) et femmes (W) de 25 ans maximum. Genre Moyenne Médiane Observations M 22.3 22.1 97 W 21.8 21.0 94 Passer par des variables intermédiaires (biometry_25 et biometry_tab). Les instructions sont plus lisibles, mais les variables intermédiaires “polluent” inutilement l’environnement de travail (en tout cas, si elles ne servent plus par après) : biometry &lt;- mutate(biometry, bmi = weight / (height/100)^2) biometry_25 &lt;- filter(biometry, age &lt;= 25) biometry_25 &lt;- group_by(biometry_25, gender) biometry_tab &lt;- summarise(biometry_25, mean = mean(bmi), median = median(bmi), number = n()) knitr::kable(biometry_tab, rows = NULL, digits = 1, col = c(&quot;Genre&quot;, &quot;Moyenne&quot;, &quot;Médiane&quot;, &quot;Observations&quot;), caption = &quot;IMC d&#39;hommes (M) et femmes (W) de 25 ans maximum.&quot;) Tableau 5.5: IMC d’hommes (M) et femmes (W) de 25 ans maximum. Genre Moyenne Médiane Observations M 22.3 22.1 97 W 21.8 21.0 94 Des trois approches, la version ci-dessous avec chaînage des opérations est la plus lisible et la plus pratique23. biometry %&gt;.% mutate(., bmi = weight / (height/100)^2) %&gt;.% filter(., age &lt;= 25) %&gt;.% group_by(., gender) %&gt;.% summarise(., mean = mean(bmi), median = median(bmi), number = n()) %&gt;.% knitr::kable(., rows = NULL, digits = 1, col = c(&quot;Genre&quot;, &quot;Moyenne&quot;, &quot;Médiane&quot;, &quot;Observations&quot;), caption = &quot;IMC d&#39;hommes (M) et femmes (W) de 25 ans maximum.&quot;) Tableau 5.6: IMC d’hommes (M) et femmes (W) de 25 ans maximum. Genre Moyenne Médiane Observations M 22.3 22.1 97 W 21.8 21.0 94 Le pipe %&gt;.% injecte le résultat précédent dans l’instruction suivante à travers l’objet . Ainsi, en seconde ligne mutate(.), . se réfère à biometry. A la ligne suivante, filter(.), le . se réfère au résultat issu de l’opération mutate(), et ainsi de suite. La logique d’enchaînement des opérations sur le résultat, à chaque fois, du calcul précédent est donc le fondement de cet opérateur “pipe”. Le pipe permet d’éviter de répéter le nom des objets (version avec variables intermédiaires), ce qui alourdit inutilement le code et le rend moins agréable à la lecture. L’imbrication des fonctions dans la première version est catastrophique pour la compréhension du code car les arguments des fonctions de plus haut niveau sont repoussés loin. Par exemple, l’argument de l’appel à group_by() (gender) se retrouve quatre lignes plus loin. Et encore, nous avons pris soin d’indenter le code pour repérer sur un plan vertical qui appartient à qui, mais imaginez ce que cela donne si l’instruction est mise à plat sur une seule ligne ! Le code le plus clair à la lecture est définitivement celui avec chaînage des opérations. Or, un code plus lisible est plus compréhensible… et donc, moins bogué. A vous de jouer Maintenant que vous venez d’apprendre à importer correctement vos données, à les remanier avec quelques-uns des opérateurs les plus fréquents, et que vous savez chaîner vos instructions, il est temps de vous exercer sur un cas concret. Pour cette activité, vous allez travailler seul sur les données d’un projet étudiant la croissance des coraux. Créez un rapport et effectuez les différents exercices en suivant les instructions qui sont dans le fichier README.mddu dépôt accessible depuis : Pour l’année académique 2019-2020, les URLs à utiliser pour accéder à votre tâche sont les suivants : Cours de Bioinformatique et Sciences des données à Charleroi : https://classroom.github.com/a/XkmTR88D Cours de Sciences des données I à Mons : https://classroom.github.com/a/4eP5yL1L Pour les autres utilisateurs de ce livre, veuillez faire un “fork” du dépôt sdd1_coral_growth. Si vous souhaitez accéder à une version précédente de l’exercice, sélectionner la branche correspondante à l’année que vous recherchez Afin d’appliquer vos nouvelles conniassances, des challenges vous sont proposés afin d’améliorer vos compétences en remaniement de données. Réalisez dans le projet sur la biométrie humaine les exercies suivants : https://github.com/BioDataScience-Course/sdd_lesson/blob/2019-2020/sdd1_05/exercises/sdd1_05_biometry.md Consignez vos résultats dans une document R Markdown avec une explication de vos différents remaniements. Réalisez dans le projet sur le zooplankton les exercies suivants : https://github.com/BioDataScience-Course/sdd_lesson/blob/2019-2020/sdd1_05/exercises/sdd1_05_zooplankton.md Consignez vos résultats dans une document R Markdown avec une explication de vos différents remaniements. Terminez ce module en vérifiant que vous en avez acquis les notions principales. Ouvrez RStudio dans votre SciViews Box, puis exécutez l’instruction suivante dans la fenêtre console : BioDataScience::run(&quot;05a_test&quot;) Pour en savoir plus Présentation en détail du “dot-pipe” assez proche fonctionnellement de %&gt;.% du package flow. Section sur le pipe dans “R for Data Science” expliquant l’utilisation du pipe de magrittr (et aussi quand ne pas l’utiliser !) Le lecteur intéressé pourra lire les différents articles suivants : more pipes in R, y compris les liens qui s’y trouvent, permet de se faire une idée de la diversité des opérateurs de chaînage dans R et de leur historique. Dot pipe présente l’opérateur %.&gt;% du package wrapr très proche du nôtre et in praise of syntactic sugar explique ses avantages. Nous partageons l’idée que le “pipe de base” ne devrait pas modifier l’instruction de droite contrairement à ce que fait %&gt;% de magrittr, et notre opérateur %&gt;.% va en outre plus loin encore que %.&gt;% dans la facilité de débogage du code chaîne. ↩ Le chaînage n’est cependant pas forcément plus facile à déboguer que la version avec variables intermédiaires. Le package flow propose la fonction debug_flow() à appeler directement après un plantage pour inspecter la dernière instruction qui a causé l’erreur, voir ?debug_flow.↩ "],
["qualit.html", "Module 6 Traitement des données II", " Module 6 Traitement des données II Objectifs Comprendre les principaux tableaux de données utilisés en science des données Savoir réaliser des tableaux de contingences Acquérir des données et les encoder correctement et de manière à ce que les analyses soient reproductibles Être capable de remanier des tableaux de données et de fusionner plusieurs tableaux Prérequis Ce module est la continuation du module 5 dont le contenu doit être bien compris et maîtrisé avant de poursuivre ici. "],
["tableaux-de-donnees.html", "6.1 Tableaux de données", " 6.1 Tableaux de données Les tableaux de données sont principalement représentés sous deux formes : les tableaux cas par variables et les tableaux de contingence. 6.1.1 Tableaux cas par variables Chaque individu est représenté en ligne et chaque variable en colonne par convention. En anglais, on parlera de tidy data. Nous nous efforcerons de toujours créer un tableau de ce type pour les données brutes. La question à se poser est la suivante : est-ce que j’ai un seul et même individu représenté sur chaque ligne du tableau ? Si la réponse est non, le tableau de données n’est pas correctement encodé. Par exemple, considérez les données suivantes concernant la taille adulte mesurée en cm pour deux espèces de petits rongeurs : hamster cobaye 15,4 21,7 18,2 22,0 17,6 24,3 14,2 23,9 16,8 Nous notons immédiatement que ce n’est pas un tableau correctement présenté en cas par variable. En effet, un même individu ne peut appartenir simultanément aux hamsters et aux cobayes. Il s’agit forcément d’individus différents mesurés. Nous n’avons donc pas respecté ici la règle de “une ligne = un individu”. D’ailleurs, nous avons plus de hamsters mesurés ici que de cobayes, ce qui se marque par un déséquilibre dans le remplissage du tableau… qui doit aussi être un signal d’alarme (à moins qu’il n’y ait des valeurs manquantes). Une présentation correcte de ces données consiste à utiliser une colonne “taille” et une seconde “espèce” pour indiquer toutes les caractéristiques pertinentes dans le cadre de l’étude tout en respectant les conventions d’un tableau cas par variables. Cela donne : espèce taille hamster 15,4 hamster 18,2 hamster 17,6 hamster 14,2 hamster 16,8 cobaye 21,7 cobaye 22,0 cobaye 24,3 cobaye 23,9 Ce dernier tableau est moins compact et plus “verbeux” que le présenter (il faut répéter “hamster” ou “cobaye” plusieurs fois dans la première colonne). Pour cette raison, un novice ou un encodeur ignorant les règles élémentaires des sciences des données est souvent amené, de bonne fois, à adopter la présentation qui semble la plus compacte, mais ce n’est pas toujours la plus pertinente ! Heureusement, dons R le outils existent pour passer d’une forme à l’autre facilement. Considérez maintenant ce second exemple fictif avec le résultat d’un test enregistré avant et après un traitement sensé améliorer le résultat du test : individu résultat individu 1 avant 10,3 individu 1 après 12,5 individu 2 avant 11,6 individu 2 après 12,4 individu 3 avant 10,8 individu 3 après 10,6 Ici, les labels utilisés dans la colonne “individu” suggère que l’expérience à été menée avant et après à chaque fois sur le même individu. Ainsi, le tableau a 6 lignes, mais ne représente que 3 individus. Ici encore, le tableau est mal encodé. Il aurait fallu considérer une et une seule ligne par individu et reporter les résultats “avant” et “après” le traitement dans des colonnes différentes. Voici ces mêmes données présentées correctement selon un tableau cas par variables : avant après 10,3 12,5 11,6 12,4 10,8 10,6 Avant toute analyse, vérifier le type de tableau de données que vous avez à disposition. Le point de départ le plus courant et le plus sûr est le tableau cas par variables. La question à se poser pour vérifier si le tableau est bien présenté est : “a-t-on une et une seule ligne dans le tableau pour chaque individu ?” Si ce n’est pas le cas, il faut remanier le tableau avant de commencer son analyse. Les tableaux de données que vous avez traités jusqu’à présent étaient tous des tableaux cas par variables. Chaque ligne représentait un individu sur qui une ou plusieurs variables (en colonnes) étaient mesurées. biometry &lt;- read(&quot;biometry&quot;, package = &quot;BioDataScience&quot;, lang = &quot;fr&quot;) head(biometry) # # A tibble: 6 x 7 # gender day_birth weight height wrist year_measure age # &lt;fct&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 M 1995-03-11 69 182 15 2013 18 # 2 M 1998-04-03 74 190 16 2013 15 # 3 M 1967-04-04 83 185 17.5 2013 46 # 4 M 1994-02-10 60 175 15 2013 19 # 5 W 1990-12-02 48 167 14 2013 23 # 6 W 1994-07-15 52 179 14 2013 19 L’encodage d’un petit tableau cas par variables directement dans R est facile. Cela peut se faire de plusieurs façons différentes. En voici deux utilisant les fonctions tibble() (spécification colonne par colonne, utilisez le snippet .dmtibble pour vous aider) et tribble() (spécification ligne par ligne, utilisez le snippet .dmtribble) : # Spécification colonne par colonne avec tibble() (df &lt;- as_dataframe(tibble( x = c(1, 2), y = c(3, 4) ))) # # A tibble: 2 x 2 # x y # &lt;dbl&gt; &lt;dbl&gt; # 1 1 3 # 2 2 4 # Spécification ligne par ligne avec tribble() (df1 &lt;- as_dataframe(tribble( ~x, ~y, 1, 3, 2, 4 ))) # # A tibble: 2 x 2 # x y # &lt;dbl&gt; &lt;dbl&gt; # 1 1 3 # 2 2 4 La seconde approche est plus naturelle, mais la première permet d’utiliser diverses fonctions de R pour faciliter l’encodage, par exemple : Séquence d’entiers successifs : 1:10 # [1] 1 2 3 4 5 6 7 8 9 10 Répétition d’un vecteur 5 fois : rep(c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), 5) # [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;a&quot; &quot;b&quot; &quot;c&quot; Répétition de chaque item d’un vecteur 5 fois : rep(c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), each = 5) # [1] &quot;a&quot; &quot;a&quot; &quot;a&quot; &quot;a&quot; &quot;a&quot; &quot;b&quot; &quot;b&quot; &quot;b&quot; &quot;b&quot; &quot;b&quot; &quot;c&quot; &quot;c&quot; &quot;c&quot; &quot;c&quot; &quot;c&quot; Pour de plus gros tableaux, il vaut mieux utiliser un tableur tel que Excel ou LibreOffice Calc pour l’encodage. Les tableurs en ligne comme Google Sheets ou Excel Online conviennent très bien également et facilitent un travail collaboratif ainsi que la mise à disposition sut le Net, comme nous avons vu au module 5. 6.1.2 Tableaux de contingence Le tableau cas par variables n’est toutefois pas la seule représentation (correcte) possible des données. Un tableau de contingence représente de manière bien plus compacte le dénombrement de l’occurrence de chaque niveau d’une (tableau à une entrée) ou de deux variables qualitatives (tableau à double entrée). La fonction table() crée ces deux types de tableaux de contingence à partir de données encodées en tableau cas par variables : biometry$age_rec &lt;- cut(biometry$age, include.lowest = FALSE, right = TRUE, breaks = c(14, 27, 90)) (bio_tab &lt;- table(biometry$gender, biometry$age_rec)) # # (14,27] (27,90] # M 106 92 # W 97 100 Le tableau de contingence peut toujours être calculé à partir d’un tableau cas par variable, mais il peut également être encodé directement si nécessaire. Voici un petit tableau de contingence à simple entrée encodé directement comme tel (vecteur nommé transformé en objet table à l’aide de la fonction as.table()) : anthirrhinum &lt;- as.table(c( &quot;fleur rouge&quot; = 54, &quot;fleur rose&quot; = 122, &quot;fleur blanche&quot; = 58) ) anthirrhinum # fleur rouge fleur rose fleur blanche # 54 122 58 Une troisième possibilité est d’utiliser un tableau indiquant les fréquences d’occurence dans une colonne (freq ci-dessus). Ce n’est pas un tableau cas par variables, mais une forme bien plus concise et pratique pour pré-encoder les données qui devront être ensuite transformées en tableau de contingence à l’aide de la fonction xtabs(). Voici un exemple pour un tableau de contingence à double entrée. Notez que le tableau cas par variable correspondant devrait contenir 44 + 116 + 19 + 128 = 307 lignes et serait plus fastidieux à construire et à manipuler (même en utilisant la fonction rep()). timolol &lt;- tibble( traitement = c(&quot;timolol&quot;, &quot;timolol&quot;, &quot;placebo&quot;, &quot;placebo&quot;), patient = c(&quot;sain&quot;, &quot;malade&quot;, &quot;sain&quot;, &quot;malade&quot;), freq = c(44, 116, 19, 128) ) # Creation du tableau de contingence timolol_table &lt;- xtabs(data = timolol, freq ~ patient + traitement) timolol_table # traitement # patient placebo timolol # malade 128 116 # sain 19 44 La sortie par défaut d’un tableau de contingence n’est pas très esthétique, mais plusieurs options existent pour le formater d’une façon agréable. En voici deux exemples : pander::pander(timolol_table, caption = &quot;Exemple de table de contingence à double entrée.&quot;) Exemple de table de contingence à double entrée.   placebo timolol malade 128 116 sain 19 44 knitr::kable(timolol_table, caption = &quot;Exemple de table de contingence à double entrée.&quot;) Tableau 6.1: Exemple de table de contingence à double entrée. placebo timolol malade 128 116 sain 19 44 Il est même possible de représenter graphiquement un tableau de contingence pour l’inclure dans une figure composée, éventuellement en le mélangeant avec des graphiques24. tab1 &lt;- ggpubr::ggtexttable(head(biometry), rows = NULL) tab2 &lt;- ggpubr::ggtexttable(table(biometry$gender, biometry$age_rec)) combine_charts(list(tab1, tab2), nrow = 2) Différentes fonctions dans R existent également pour convertir un tableau de contingence en tableau cas par variables (ou en tous cas, en un tableau similaire). Par exemple, as_dataframe() renvoie un tableau indiquant les fréquences d’occurrences : (timolol2 &lt;- as_dataframe(timolol_table, n = &quot;freq&quot;)) # # A tibble: 4 x 3 # patient traitement freq # &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; # 1 malade placebo 128 # 2 sain placebo 19 # 3 malade timolol 116 # 4 sain timolol 44 Si vous insistez, vous pouvez aussi obtenir un tableau cas par variables (mais celui-ci est très long et peu pratique à manipuler) à l’aide de la fonction uncount()25 : uncount(timolol2, freq) # # A tibble: 307 x 2 # patient traitement # &lt;chr&gt; &lt;chr&gt; # 1 malade placebo # 2 malade placebo # 3 malade placebo # 4 malade placebo # 5 malade placebo # 6 malade placebo # 7 malade placebo # 8 malade placebo # 9 malade placebo # 10 malade placebo # # … with 297 more rows 6.1.3 Métadonnées Les données dans un tableau de données doivent impérativement être associées à un ensemble de métadonnées. Les métadonnées (“metadata” en anglais) apportent des informations complémentaires nécessaires pour une interprétation correcte des données. Elles permettent donc de replacer les données dans leur contexte et de spécifier des caractéristiques liées aux mesures réalisées comme les unités de mesure par exemple. \\[Donn\\acute{e}es \\ de \\ qualit\\acute{e} \\ = \\ tableau \\ de \\ donn\\acute{e}es + \\ m\\acute{e}tadonn\\acute{e}es\\] Les données correctement qualifiées et documentée sont les seules qui peuvent être utilisées par un collaborateur externe. C’est à dire qu’une personne externe à l’expérience ne peut interpréter le tableau de données que si les métadonnées sont complètes et explicites. Exemple de métadonnées : Unités de mesure (exemple : 3,5 mL, 21,2 °C) Précision de la mesure (21,2 +/- 0,2 dans le cas d’un thermomètre gradué tous les 0,2 °C) Méthode de mesure utilisée (thermomètre à mercure, ou électronique, ou …) Type d’instrument employé (marque et modèle du thermomètre par exemple) Date de la mesure Nom du projet lié à la prise de mesure Nom de l’opérateur en charge de la mesure … Vous avez pu vous apercevoir que la fonction read() permet d’ajouter certaines métadonnées comme les unités aux variables d’un jeu de données. Cependant, il n’est pas toujours possible de rajouter les métadonnées dans un tableau sous forme électronique, mais il faut toujours les consigner dans un cahier de laboratoire, et ensuite les retranscrire dans le rapport. La fonction labelise() vous permet de rajouter le label et les unités de mesure pour vos différentes variables directement dans le tableau. Par exemple, voici l’encodage direct d’un petit jeu de données qui mesure la distance du saut (jump) en cm de grenouilles taureaux en fonction de leur masse (weight) en g pour 5 individus différents (ind). Vous pouvez annoter ce data frame de la façon suivante : frog &lt;- tribble( ~ind, ~jump, ~weight, 1, 71, 204, 2, 70, 240, 3, 100, 296, 4, 120, 303, 5, 103, 422 ) # Ajout des labels et des unités frog &lt;- labelise(frog, self = FALSE, label = list( ind = &quot;Individu&quot;, jump = &quot;Distance du saut&quot;, weight = &quot;Masse&quot;), units = list( jump = &quot;cm&quot;, weight = &quot;g&quot;) ) # Affichage synthétique des données et métadonnées associées str(frog) # Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 5 obs. of 3 variables: # $ ind : num 1 2 3 4 5 # ..- attr(*, &quot;label&quot;)= chr &quot;Individu&quot; # $ jump : num 71 70 100 120 103 # ..- attr(*, &quot;label&quot;)= chr &quot;Distance du saut&quot; # ..- attr(*, &quot;units&quot;)= chr &quot;cm&quot; # $ weight: num 204 240 296 303 422 # ..- attr(*, &quot;label&quot;)= chr &quot;Masse&quot; # ..- attr(*, &quot;units&quot;)= chr &quot;g&quot; # Affichage des labels label(frog) # ind jump weight # &quot;Individu&quot; &quot;Distance du saut&quot; &quot;Masse&quot; Les métadonnées sont enregistrées dans des attributs en R (attr). De même, comment() permet d’associer ou de récupérer un attribut commentaire : # Ajout d&#39;un commentaire concernant le jeu de données lui-même comment(frog) &lt;- &quot;Saut de grenouilles taureaux&quot; # Ajout d&#39;un commentaire sur une variable comment(frog$jump) &lt;- &quot;Premier saut mesuré après stimulation de l&#39;animal&quot; # Affichage synthétique str(frog) # Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 5 obs. of 3 variables: # $ ind : num 1 2 3 4 5 # ..- attr(*, &quot;label&quot;)= chr &quot;Individu&quot; # $ jump : num 71 70 100 120 103 # ..- attr(*, &quot;label&quot;)= chr &quot;Distance du saut&quot; # ..- attr(*, &quot;units&quot;)= chr &quot;cm&quot; # ..- attr(*, &quot;comment&quot;)= chr &quot;Premier saut mesuré après stimulation de l&#39;animal&quot; # $ weight: num 204 240 296 303 422 # ..- attr(*, &quot;label&quot;)= chr &quot;Masse&quot; # ..- attr(*, &quot;units&quot;)= chr &quot;g&quot; # - attr(*, &quot;comment&quot;)= chr &quot;Saut de grenouilles taureaux&quot; # Récupération des commentaires comment(frog) # [1] &quot;Saut de grenouilles taureaux&quot; comment(frog$jump) # [1] &quot;Premier saut mesuré après stimulation de l&#39;animal&quot; comment(frog$weight) # Rien! # NULL 6.1.4 Dictionnaire des données Le dictionnaire des données est un élément important de la constitution d’une base de données. Il s’agit d’un tableau annexe qui reprend le nom de chaque variable, son label (nom plus long et explicite), son type (numérique, facteur, facteur ordonné, date, …), la taille (de moindre importance pour nous), et un commentaire éventuel. Dans notre contexte, il est également utile de renseigner les unités de mesure, et la façon dont les données manquantes sont encodées. Cela donne donc un tableau du genre : Variable Label Unités Type Val. manquantes Commentaire date Date - Date NA Date de mesure age Âge années numeric -1 diameter Diamètre du test mm numeric NA Moyenne de deux diamètres perpendiculaires origin Origine - factor unknown “Fishery” = oursins sauvages, “Farm” = oursins d’élevage Ce tableau peut-être encodé sous forme textuelle et placé dans le même dossier que le jeu de données lui-même. Il peut aussi être encodé comme feuille supplémentaire dans une fichier Excel. Le dictionnaire des données est un outil important pour comprendre ce que contient le tableau de données, et donc, son interprétation. Ne le négligez pas ! Utilisez cette option avec parcimonie : il vaut toujours mieux représenter un tableau comme … un tableau plutôt que comme une figure !↩ Notez également que passer d’un tableau cas par variables à un tableau des fréquences d’occurrences se fait à l’aide de count().↩ "],
["population-et-echantillonnage.html", "6.2 Population et échantillonnage", " 6.2 Population et échantillonnage Nos analyses statistiques seraient bien évidemment beaucoup plus simple si nous pouvions toujours mesurer tous les individus concernés par nos études. En fait, c’est presque toujours impossible car les populations concernées sont souvent très larges, voire infinies. Nous devons donc mesurer un petit sous-ensemble de la population, ce que nous appelons un échantillon (“sample” en anglais) de taille finie déterminée (on parle de la taille de l’échantillon ou “sample size” en anglais). Le processus qui mène à la sélection des individus dans l’échantillon s’appelle l’échantillonnage (“sampling” en anglais). De cet échantillon, nous souhaitons malgré tout retirer un maximum d’information concernant la population d’origine. Cela s’appelle faire de l’inférence sur la population à partir de l’échantillon (“to infer” en anglais). Population et échantillon. Faites attention à la signification des termes selon les disciplines. Par exemple, le terme population ne signifie pas la même chose pour un biologiste (ensemble des individus d’une même espèce pouvant se reproduire entre eux) et pour un statisticien (ensemble des valeurs que peut prendre une variable). Ainsi, la définition statstique du terme se réfère à un bien plus grand sous-ensemble en général. Par exemple, si nous étudions la souris Mus musculus, nous considèrerons bien évidemment une population (ou une souche donnée pour les souris de laboratoire) en qualité de biologistes. Les statisticiens considèreront l’ensemble des souris qui existent, ont existé et existeront à l’avenir comme la population de souris. De même, un individu statistique est une entité sur laquelle nous effectuons nos mesures (ce qui donne lieu à une observation pour chaque variable). Ainsi, un individu statistique peut correspondre ou non à un individu biologique selon ce que l’on considère. Par exemple, une section dans un organe peut constituer un individu statistique lors de l’étude de l’hétérogénéité à l’intérieur de cet organe. Nous aurons donc autant d’individus statistiques que de sites de prélèvement sur l’organe, … même si ce dernier provient au final du même individu biologique (même animal ou végétal) ! La variabilité tant au niveau de la population que de l’échantillon provient essentiellement de deux facteurs : La variabilité individuelle inhérente (nous n’avons pas tous la même taille ne la même masse, par exemple), Les erreurs de mesure. Ces deux facteurs se cumulent pour contribuer à disperser les valeurs d’un individu à l’autre. Nous n’avons que peu de prise sur la variabilité individuelle, mais nous pouvons parfois réduire les erreurs de mesure si cela s’avère souhaitable en utilisant un appareil de mesure plus précis, par exemple. Quoi qu’il en soit, plus la variabilité est importante, plus la taille de l’échantillon devra également être grande pour concerver une bonne “représentativité” de la population. Du point de vue de la notation mathématique, nous utiliserons une lettre latine majuscule en italique pour représenter un variable, par exemple, X. La taille de l’échantillon est souvent notée n. Les observations individuelles de la variable x pour les n individus de l’échantillon seront alors notés avec une lettre minuscule en italique assortie d’un indice compris entre 1 et n, donc, xi de manière générale avec l’indice i variant de 1 à n. Ainsi, x5 représente la cinquième observation pour la variable X dans notre échantillon, et xn est la dernière observation. 6.2.1 Échantillonnage aléatoire La démarche du statisticien et du scientifique des données est de retirer un maximum d’information d’un échantillon afin de faire des inférences fiables sur la population tout entière (c’est-à-dire, tenter de tirer des conclusions les plus générales possibles à partir de l’étude d’un petit échantillon seulement). Ceci n’est possible que si l’échantillon “est conforme” à la population. En langage statistique on dit que l’échantillon est représentatif. Un échantillon représentatif n’est pas facile à obtenir et il faut respecter scrupuleusement certaines règles pour l’obtenir. A contrario un échantillon non représentatif s’obtient sans précautions particulières, mais ce dernier est quasi-toujours totalement inutile (on ne peut tirer de conclusions que sur cet échantillon particulier). Imaginez un soudage qui ne représente pas la population sondée… quel est son intérêt ? Il est nul ! La meilleure façon d’obtenir un échantillon représentatif est de réaliser un échantillonnage aléatoire. Dans ce cas, chaque individu de la population doit avoir la même chance d’être sélectionné dans l’échantillon. Nous parlons aussi d’échantillonnage au hasard. Vous devez bien réfléchir au processus qui va mener à la sélection des individus dans l’échantillon. Celui-ci doit comporter une étape qui fait intervenir le hasard. “Choisir au hasard” ses individus en les prélevant au petit bonheur la chance en fonction de son humeur n’est pas une bonne approche. En effet, notre main qui saisira les individus à inclure dans l’échantillon aura tendance à prélever ceux qui sont plus facile à attraper ou plus proches, par exemple. Il peut s’agir d’individus moins vigoureux, moins réactifs, ou au contraire, moins peureux… Du coup, nous n’étudierions qu’une fraction de la population qui correspond à une caractéristique particulière (ceux qui sont faibles et peu réactifs, par exemple) Une bonne sélection aléatoire doit faire intervenir le hasard (tirage au sort dans une urne, pile ou face, jet de dés, ou génération de nombres dits “pseudo-aléatoires” à l’aide d’un ordinateur). Par exemple, si vous avez un élevage de souris dans votre laboratoire, et que vous considérez cet élevage comme votre population, vous pouvez réaliser deux groupes (témoin et traitement) de cinq souris chacuns de plusieurs façons : Prendre dix souris dans les cages “au hasard”, et les répartir toujours “au hasard” entre les deux groupes. Ce type d’échantillonnage est incorrect. En effet, votre choix sera (inconsciemment) conditionné. Donner un identifiant numérique unique à chacune de vos souris. Ensuite tirer deux fois cinq identifiants à partir d’une urne, ou effectuer ce même traitement virtuellement à l’aide d’un ordinateur. Dans ce cas, l’échantillonnage sera réellement aléatoire et correctement réalisé ! En pratique, nous pourrons utiliser la fonction sample() dans R. Elle permet de simulation facilement et rapidement le processus de tirage au hasard depuis une urne. Dans le cas de nos deux groupes de cinq souris à partir d’un élevage qui contient 213 animaux numérotés de 1 à 213 (identifiants uniques), nous ferons : # Echantillon de 5 souris témoins sample(1:213, size = 5) # [1] 15 12 102 136 42 # Echantillon de cinq souris pour le traitement sample(1:213, size = 5) # [1] 187 108 211 68 49 Ici, la sélection aléatoire (en réalité, on parle de “pseudo-aléatoire” pour ces nombres générés par un algorithme, mais qui ont des propriétés très similaires au hasard pur) nous indique que nous devrons aller chercher les souris n°15, 12, 102, 136 er 42 dans notre élevage pour notre groupe témoin, et les souris n°187, 108, 211, 68 et 49 pour le groupe traitement. Dans le cas où un échantillonnage doit se faire avec replacement (l’équivalent de replacer une boule dans l’urne à chaque tirage au sort), nous pouvons indiquer l’argument replace = TRUE dans sample(). Donc, quelque chose comme sample(0:9, size = 50, replace = TRUE) pour échantillon les chiffres 0 à 9 au hasard cinquante fois avec remise (à chaque tirage chaque chiffre a même chance d’être tiré). L’utilisation de sample() est pratique, mais cela rend le code non reproductible. En effet, à chaque fois, la fonction génère une série différente au hasard (c’est son rôle !) Toutefois, comme la série est pseudo-aléatoire, il est possible de regénérer la même série une seconde fois si on part du même point dans l’algorithme qui la calcule. Cela peut être réalisé à l’aide de set.seed(). Vous devez indiquer comme argument à cette dernière fonction un nombre aléatoire. Ce nombre représentera une position dans la séquence générée par l’algorithme de sorte que la série suivante obtenue à l’aide de sample() sera toujours la même. Voici comment cela fonctionne : set.seed(2563) # Utiliser un nombre différent à chaque appel de set.seed() ! sample(1:10, size = 6) # [1] 10 3 4 5 7 8 6.2.2 Échantillonnage stratifié L’échantillonnage aléatoire n’est pas la seule stratégie correcte. L’échantillonnage stratifié consiste à diviser la population en sous-populations identifiables facilement (par exemple, séparer la population en fonction du sexe, ou de l’âge, voire des deux simultanément). Ensuite, un échantillonnage aléatoire est réalisé à l’intérieur de chaque sous-population pour un nombre déterminé d’individus, souvent le même. Cette approche plus complexe est utile si les sous-population sont très mal balancées (une sous-population possède bien plus d’individus qu’une autre). Par exemple, vous voulez comparer des prélèvements sanguins faits sur une population d’européens hospitalisés dans le but de déterminer les critères qui permettent de diagnostiquer une infection nosocomiale26 rare. Les individus sont suivis en hopital par définition et nous admettrons que l’on sait a priori s’ils sont atteints de la maladie ou non grâce à d’autres tests plus lourds et coûteux. La maladie est rare heureusement. Sa prévalence n’est que de 1 cas sur 10.000. Dans ce cas, si vous effectuez un échantillonnage aléatoire de 100 patients ou même de 1.000 patients hospitalisés, vous avez toutes les chances de ne pas include même un seul patient atteint de l’infection ! Ici l’échantillonnage stratifié est utile. Il consiste à séparer les patients en deux sous-populations : ceux qui sont atteints et les autres. Ensuite, vous décidez, par exemple, d’analyser 50 prélèvements sanguins dans chacune des deux sous-populations. Vous échantillonnez alors 50 patients aléatoirement comme nous l’avons fait plus haut pour nos souris, mais à l’intérieur de chaque sous-population. Au final, votre échantillon contiendra alors 50 patients infectés et 50 autres qui ne le sont pas. Cet échantillon est également représentatif (sauf, bien sûr, de la prévalence de l’infection). Une infection nosocomiale est une infection contractée dans un hopital.↩ "],
["acquisition-de-donnees.html", "6.3 Acquisition de données", " 6.3 Acquisition de données Dans le module 5, vous avez pris connaissance des types de variable et venez d’apprendre comment encoder différents types de tableaux de données et de leurs associer les indispensables métadonnées. Cependant, la première étape avant d’acquérir des données est de planifier correctement son expérience. La Science des Données est intimement liée à la démarche scientifique et intervient dans toutes les étapes depuis la caractérisation de la question et le planning de l’expérience jusqu’à la diffusion des résultats. Plus en détails, cela correspond à : Définir une question (objectif) Réaliser une recherche bibliographique sur la thématique Définir le protocole de l’expérience à partir de l’objectif Définir la population étudiée et l’échantillonnage Définir les variables à mesurer Définir les unité des mesures Définir la précision des mesures Définir les instruments de mesure nécessaires Définir les conventions d’encodage Codifier l’identification des individus Définir les niveaux des variables facteurs et leurs labels Acquérir et encoder les données Traiter les données Importer des données Remanier des données Visualiser et décrire des données Analyser les données (traitements statistiques, modélisation,…). Produire des supports de présentation répondant à la question de départ et diffuser l’information dans la communauté scientifique Nous traitons ici des premières étapes qui visent à acquérir les données. 6.3.1 Précision et exactitude Les erreurs de mesures sont inévitables lors de l’acquisition de nos données. Cependant, il est possible de les minimiser en choisissant un instrument plus précis (“precise” en anglais) et plus exact (“accurate” en anglais). La figure ci-dessous illustre de manière visuelle la différence qu’il y a entre précision et exactitude. 6.3.2 Codification des données Afin d’éviter que divers collaborateurs encodent différemment la même information, vous allez devoir préciser très clairement comment encoder les différentes variables de votre jeu de données. Par exemple pour une variable genre, est-ce que vous indiquez homme ou femme, ou h / f, ou encore H / F ? De même, vous allez devoir attribuer un code unique à chaque individu mesuré. Enfin, vous devez vous assurer que toutes les mesures sont réalisées de la même manière et avec des instruments qui, s’ils sont différents, seront cependant intercalibrés. Comment faire ? Réfléchissez à cette question sur base d’une mesure de la masse des individus à l’aide de pèse-personnes différents ! 6.3.2.1 Respect de la vie privée Lors d’expérience sur des personnes, le respect de la vie privée doit être pris en compte27. Le nom et le prénom, ou toute autre information permettant de retrouver les individus étudiés (adresse mail, numéro de sécurité sociale, etc.) ne peuvent pas apparaître dans la base de données consolidée. En outre, il vous faudra un accord explicite des personnes que vous voulez mesurer, et il faudra leur expliquer ce que vous faites, et comment les données seront ensuite utilisées. Une question se pose : comment pouvoir revenir vers les enregistrements liés à un individu en particulier (en cas d’erreur d’encodage, par exemple) si les informations relatives directement à ces individus ne sont pas consignées dans le tableau final ? Réfléchissez à la façon dont vous vous y prendriez avant de lire la suite… Voici un petit tableau qui correspond à ce que vous ne pourrez pas faire (nom et prénom explicitement mentionnés dans le tableau) : (biometry_marvel &lt;- as_dataframe(tribble( ~id, ~sex ,~weight, ~height, &quot;Banner Bruce&quot;, &quot;M&quot;, 95, 1.91, &quot;Stark Tonny&quot;, &quot;M&quot;, 80, 1.79, &quot;Fury Nicholas&quot;, &quot;M&quot;, 82, 1.93, &quot;Romanoff Natasha&quot;, &quot;F&quot;, 53, 1.70 ))) # # A tibble: 4 x 4 # id sex weight height # &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 Banner Bruce M 95 1.91 # 2 Stark Tonny M 80 1.79 # 3 Fury Nicholas M 82 1.93 # 4 Romanoff Natasha F 53 1.7 Vous devez fournir un code permettant de garder l’anonymat des sondés à l’ensemble des personnes étudiées vis à vis des analystes qui vont utiliser ces données. Cependant, le code doit permettre au chercheur ayant pris ces mesures de les retrouver dans son cahier de laboratoire, si besoin. Une façon de procéder consiste à attribuer un numéro au hasard par tirage dans une urne à chacune des personnes chargées des mesures. Ensuite, chaque expérimentateur attribue lui-même un second numéro aux différentes personnes qu’il mesure. Prenons par exemple le scientifique n°24 (seul lui sait qu’il porte ce numéro). Il attribue un code de 1 à n à chaque personne étudiée. En combinant le code secret de l’expérimentateur et le code individu, cela donne un identifiant unique de la forme 24_1, 24_2, etc. Il pourra alors encoder sa partie comme suit : (biometry_marvel1 &lt;- as_dataframe(tribble( ~id, ~sex , ~weight, ~height, &quot;24_1&quot;, &quot;M&quot;, 95, 1.91, &quot;24_2&quot;, &quot;M&quot;, 80, 1.79, &quot;24_3&quot;, &quot;M&quot;, 82, 1.93, &quot;24_4&quot;, &quot;F&quot;, 53, 1.70 ))) # # A tibble: 4 x 4 # id sex weight height # &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 24_1 M 95 1.91 # 2 24_2 M 80 1.79 # 3 24_3 M 82 1.93 # 4 24_4 F 53 1.7 Il garde néanmoins les correspondances dans son carnet de laboratoire, au cas où il faudrait faire des vérifications ou revenir à la donnée originale. (biometrie_correspondance &lt;- data_frame( name = biometry_marvel$id, id = biometry_marvel1$id )) # Warning: `data_frame()` is deprecated, use `tibble()`. # This warning is displayed once per session. # # A tibble: 4 x 2 # name id # &lt;chr&gt; &lt;chr&gt; # 1 Banner Bruce 24_1 # 2 Stark Tonny 24_2 # 3 Fury Nicholas 24_3 # 4 Romanoff Natasha 24_4 A partir des données du tableau général consolidé, personne à part lui ne peut revenir sur ces données d’origine et mettre un nom sur les individus mesurés. Et lui-même n’a pas la possibilité de déterminer qui se cache derrière les autres identifiants tels 3_1, 12_4, 21_2, etc. A vous de jouer Ouvrez RStudio dans votre SciViews Box, puis exécutez l’instruction suivante dans la fenêtre console : BioDataScience::run(&quot;06a_test&quot;) Votre objectif est d’acquérir des données pour étudier la prévalence de l’obésité dans la population. En classe, vous allez réfléchir par équipes aux données qu’il vous faudra mesurer : quoi ? pourquoi ? comment ? Les résultats de votre réflexion seront ensuite consolidées pour arriver à un consensus général. Ensuite, le fruit de cette réflexion ainsi que l’analyse que vous réaliserez seront à ajouter dans le projet sdd1_biometry. Une feuille Google Sheets sera mise à disposition pour encoder vos données de manière collaborative sur base des spécifications que vous aurez formulées. Pour les étudiants du cours de bioinformatique et sciences des données des données à Charleroi : Le tableau de données que vous devez completer est disponible via le lien suivant : https://docs.google.com/spreadsheets/d/1XopTEpRjM0TdyVfJHva80JWaAyB7TmBbm1_k99ewzWM/edit?usp=sharing Le dictionnaire des données est disponible via le lien suivant : https://docs.google.com/document/d/1eBnbmzanzZXBk-UMVl8d3uQeUWzePfK0sYjW7SlzIDk/edit?usp=sharing Le tableau de données est téléchargeable via le lien suivant : https://docs.google.com/spreadsheets/d/e/2PACX-1vS9UthgeinmFnbJjziwOR6t0qXKgIAQdY7vkd7PZIax4XHxlvr632rVn4-NT3djJFKfgEshftEvuiJa/pub?gid=0&amp;single=true&amp;output=csv Pour les étudiants du cours de sciences des données des données à Mons : Le tableau de données que vous devez completer est disponible via le lien suivant : https://docs.google.com/spreadsheets/d/1GJAGWjwNBtEGqQXqFcNxkrwcw-5n-gqwmiGAzxUZrxg/edit?usp=sharing Le dictionnaire des données est disponible via le lien suivant : https://docs.google.com/spreadsheets/d/1j55bB9YEAVbS4eRE-i6L-NEYhHXua-dxs-aQr_qko7k/edit?usp=sharing Le tableau de données est téléchargeable via le lien suivant : https://docs.google.com/spreadsheets/d/e/2PACX-1vSfY7b0ICF64uv9vIYi8Jg38Rw3pKvLHC5TW0XOZYVQ4ce2dTmXGM5Cm8J922MsYm_fk75DKOK2wC4b/pub?output=csv Attention, veuillez à respectez les conventions que vous aurez édifiées ensemble lors de l’encodage… et n’oubliez pas de préciser également les métadonnées ! En Europe, les données numériques concernant les personnes sont soumises à des règles strictes édictées dans le Règlement Général pour la Protection des Données ou RGPD en abrégé, en vigueur depuis le 25 mai 2018. Vous devez vous assurer de respecter ce règlement lors de la collecte et de l’utilisation de données relatives à des personnes. Pour les autres type de données, le droit d’auteur ou des copyrights peuvent aussi limiter votre champ d’action. Renseignez-vous !↩ "],
["recombinaison-de-tableaux.html", "6.4 Recombinaison de tableaux", " 6.4 Recombinaison de tableaux 6.4.1 Formats long et large gather() versus spread() par gadenbuie. Le format long d’un tableau de données correspond à un encodage en un minimum de colonnes, les données étant réparties sur un plus grand nombre de lignes en comparaison du format large qui regroupe les données dans plusieurs colonnes successives. Voici un exemple fictif d’un jeu de données au fomat long : # # A tibble: 6 x 3 # sex traitment value # &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; # 1 m control 1.2 # 2 f control 3.4 # 3 m test1 4.8 # 4 f test1 3.1 # 5 m test2 0.9 # 6 f test2 1.2 Voici maintenant le même jeu de données présenté dans le format large : # # A tibble: 2 x 4 # sex control test1 test2 # &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 m 1.2 4.8 0.9 # 2 f 3.4 3.1 1.2 Dans le format large, les différents niveaux de la variable facteur treatment deviennent autant de colonnes (donc de variables) séparées, et la variable d’origine n’existe plus de manière explicite. Ces deux tableaux contiennent la même information. Bien évidemment, un seul de ces formats est un tableau cas par variables correct. Le format long sera le bon si toutes les mesures sont réalisées sur des individus différents. Par contre, le format large sera correct, si les différentes mesures ont été faites à chaque fois sur les mêmes individus (dans le cas présent, un seul mâle et une seule femelle auraient alors été mesurés dans les trois situations). C’est la règle qui veut qu’une ligne corresponde à un et un seul individu dans un tableau cas par variables qui permet de décider si le format long ou le format large est celui qui est correctement encodé. Encoder correctement un tableau de données n’est pas une chose simple. Il peut y avoir plusieurs manières de le représenter. De plus, beaucoup de scientifiques ignorent ou oublient l’importance de bien encoder un tableau sous forme cas par variables. Lorsque vous souhaitez effectuer une représentation graphique, un format peut convenir mieux qu’un autre également, en fonction de ce que vous souhaitez visualiser sur le graphique. Il est donc important de connaitre les fonctions permettant de recombiner simplement un tableau de données d’une forme vers l’autre : gather() et spread(). L’aide-mémoire Data Import est un outil pratique pour vous aider à retrouver les fonctions. Les explications relatives à cette partie s’y trouvent dans la section Reshape Data. L’utilisation des fonction gather() et spread() provenant du package tidyr est également décrite en détails dans R for Data Science. Prenons l’exemple d’un jeu de données provenant de l’article scientifique suivant : Paleomicrobiology to investigate copper resistance in bacteria : isolation and description of Cupriavidus necator B9 in the soil of a medieval foundry. L’article est basé sur l’analyse métagénomique de type “shotgun” pour quatre communautés microbiennes (notées c1, c4, c7et c10, respectivement)28. Il en résulte une longue liste de séquences que l’on peut attribuer à des règnes. shotgun_wide &lt;- tibble( kingdom = c(&quot;Archaea&quot;, &quot;Bacteria&quot;, &quot;Eukaryota&quot;, &quot;Viruses&quot;, &quot;other sequences&quot;, &quot;unassigned&quot;, &quot;unclassified sequences&quot;), c1 = c( 98379, 6665903, 81593, 1245, 757, 1320419, 15508), c4 = c( 217985, 9739134, 101834, 4867, 1406, 2311326, 21572), c7 = c( 143314, 7103244, 71111, 5181, 907, 1600886, 14423), c10 = c(272541, 15966053, 150918, 15303, 2688, 3268646, 35024)) rmarkdown::paged_table(shotgun_wide) Ce tableau est clair et lisible… seulement, est-il correctement encodé en cas par variables d’après vous ? Quelle que soit la réponse à cette question, il est toujours possible de passer de ce format large à un format long dans R de la façon suivante : shotgun_long &lt;- gather(shotgun_wide, c1, c4, c7, c10, key = &quot;batch&quot;, value = &quot;sequences&quot;) rmarkdown::paged_table(shotgun_long) Voici la logique derrière gather(), présentée sous forme d’une animation : gather() par apreshill. Vous conviendrez que le tableau nommé shotgun_long est moins compact et moins aisé à lire comparé à shotgun_wide. C’est une raison qui fait que beaucoup de scientifiques sont tentés d’utiliser le format large alors qu’ici il ne correspond pas à un tableau cas par variables correct, puisqu’il est impossible que les mêmes individus soient présents dans les différents lots (il s’agit de communautés microbiennes indépendantes les unes des autres). De plus, seul le format shotgun_long permet de produire des graphiques pertinents29. chart(data = shotgun_long, sequences ~ batch %fill=% kingdom) + geom_col(position = &quot;fill&quot;) Essayez de réaliser ce type de graphique en partant de shotgun_wide… Bonne chance ! Très souvent, lorsqu’il est impossible de réaliser un graphique avec chart() ou ggplot() parce que les données se présentent mal, c’est parce que le jeu de données est encodé de manière incorrecte ! Si les données sont, par contre, correctement encodées, demandez-vous alors si le graphique que vous voulez faire est pertinent. Pour passer du format long au format large (traitement inverse à gather()), il faut utiliser la fonction spread(). Ainsi pour retrouver le tableau d’origine (ou quelque chose de très semblable) à partir de shotgun_long nous utiliserons : shotgun_wide2 &lt;- spread(shotgun_long, key = batch, value = sequences) rmarkdown::paged_table(shotgun_wide2) La logique de spread() est illustrée via l’animation suivante : spread() par apreshill. 6.4.2 Recombinaison de variables Parfois, ce sont les variables qui sont encodées de manière inappropriée par rapport aux analyses que vous souhaitez faire. Les fonctions separate() et unite() permettent de séparer une colonne en plusieurs, ou inversément. L’aide-mémoire Data Import vous rappelle ces fonctions dans sa section Split Cells. Elles sont également décrites en détails dans R for Data Science. Partons, par exemple, du jeu de données sur la biométrie des crabes du package MASS : crabs &lt;- read(&quot;crabs&quot;, package = &quot;MASS&quot;, lang = &quot;fr&quot;) rmarkdown::paged_table(crabs) La fonction unite() permet de combiner facilement les colonnes sex et species comme montré dans l’exemple ci-dessous. N’hésitez pas à faire appel à la page d’aide de la fonction via ?unite pour vous guider. crabs &lt;- unite(crabs, col = &quot;sp_sex&quot;, sex, species, sep = &quot;_&quot;) rmarkdown::paged_table(crabs) La fonction complémentaire à unite() est separate(). Elle permet de séparer une variable en deux ou plusieurs colonnes séparées. Donc, pour retrouver un tableau similaire à celui d’origine, nous pourrons faire : crabs &lt;- separate(crabs, col = &quot;sp_sex&quot;, into = c(&quot;sex&quot;, &quot;species&quot;), sep = &quot;_&quot;) rmarkdown::paged_table(crabs) Les analyses métagénomiques coûtent très cher. Il est souvent impossible de faire des réplicats. Un seul échantillon d’ADN a donc été séquencé ici pour chaque communauté.↩ Notez malgré tout que, à condition de bien en comprendre les implications, le format complémentaire peut se justifier dans une publication pour y présenter un tableau le plus lisible possible, ce qui est le cas ici. Mais pour les analyses, c’est le format qui correspond à un tableau cas par variables qui doit être utilisé.↩ "],
["traitements-multi-tableaux.html", "6.5 Traitements multi-tableaux", " 6.5 Traitements multi-tableaux Durant vos analyses, vous serez confronté à devoir gérer plusieurs tableaux que vous allez vouloir rassembler en un seul. Selon le travail à réaliser, il s’agit de coller les tableaux l’un au dessus de l’autre, l’un à côté de l’autre, ou d’effectuer un travail de fusion plus complexe. Nous allons maintenant voir ces différents cas successivement. L’aide-mémoire Data Transformation vous rappelle les différentes fonctions à utiliser dans sa section Combine Tables. Leur utilisation est également décrite en détails dans R for Data Science. 6.5.1 Empilement vers le bas Pour empiler des tableaux l’un au dessus de l’autre, la fonction la plus simple est bind_rows(). Partons de données mesurée dans les mésoscosmes de notre laboratoire lors des travaux pratiques du cours d’océanographie générale. Les différentes variables mesurées sont les suivantes : les données physico-chimiques : la température, le pH, la salinité, l’oxygène dissous à l’aide, respectivement, d’un pHmètre, d’un conductimètre et d’un oxymètre la concentration en nutriments : orthophosphates (PO43-) et nitrates (NO3-) dissous dans l’eau par analyse colorimétrique Pour la première série de mesures, ils ont encodé deux fichiers qu’ils ont du par la suite rassembler. Le groupe 1 a encodé le tableau suivant : physico1 &lt;- as_dataframe(tibble( sample = c(&quot;A0&quot;, &quot;B0&quot;, &quot;A0&quot;, &quot;B0&quot;, &quot;A0&quot;, &quot;B0&quot;, &quot;A0&quot;, &quot;B0&quot;), student = c(&quot;st1&quot;, &quot;st1&quot;, &quot;st2&quot;, &quot;st2&quot;, &quot;st3&quot;, &quot;st3&quot;, &quot;st4&quot;, &quot;st4&quot;), ph = c(7.94, 7.94, 7.94, 7.99, 7.94, 7.99, 7.94, 7.99), salinity = c(34.0, 35.3, 33.9, 35.1, 34.0, 35.2, 33.9, 35.1), oxygen = c(7.98, 8.00, 7.98, 7.98, 7.99, 7.86, 7.89, 7.98), temperature = c(24.6, 24.4, 25.1, 24.7, 24.9, 24.7, 25.0, 24.6) )) rmarkdown::paged_table(physico1) Le groupe 2 a encodé ce tableau : physico2 &lt;- as_dataframe(tibble( sample = c(&quot;A0&quot;, &quot;B0&quot;, &quot;A0&quot;, &quot;B0&quot;), student = c( &quot;st5&quot;, &quot;st5&quot;, &quot;st6&quot;, &quot;st6&quot;), ph = c(7.94, 7.99, 7.93, 7.99), salinity = c(33.8, 35.0, 33.9, 35.1), oxygen = c(7.96, 8.01, 7.90, 8.00), temperature = c(25.0, 24.6, 24.0, 24.0) )) rmarkdown::paged_table(physico2) L’empilement des deux tableaux de données en un seul se fait via la fonction bind_rows() lorsque les tableaux contiennent les mêmes variables présentées exactement dans le même ordre comme ici : physico &lt;- bind_rows(physico1, physico2) rmarkdown::paged_table(physico) 6.5.2 Empilement à droite Pour combiner des tableaux de données par les colonnes, de gauche à droite, la fonction la plus simple à utiliser est bind_cols(). Les étudiants ont également réalisé des prélèvements d’eaux qui ont été dosés par colorimétrie avec un autoanalyseur. Les échantillons des deux groupes ont été analysés dans la même série par l’appareil, ce qui donne le tableau suivant pour les nutriments : nutrients &lt;- as_dataframe(tibble( sample = rep(c(&quot;A0&quot;, &quot;B0&quot;), times = 6), student = rep(c(&quot;st4&quot;, &quot;st6&quot;, &quot;st5&quot;, &quot;st2&quot;, &quot;st1&quot;, &quot;st3&quot;), each = 2), po4 = c(2.445, 0.374, 2.446, 0.394, 2.433, 0.361, 2.441, 0.372, 2.438, 0.388, 2.445, 0.390), no3 = c(1.145, 0.104, 0.447, 0.066, 0.439, 0.093, 0.477, 0.167, 0.443, 0.593, 0.450, 0.125) )) rmarkdown::paged_table(nutrients) Vous devez être très vigilant lors de l’utilisation de bind_cols() car cette dernière combine vos tableaux sans s’assurer que vos lignes soient alignées convenablement ! oceano &lt;- bind_cols(nutrients, physico) rmarkdown::paged_table(oceano) Qu’observez vous ? Effectivement nos deux tableaux de données n’ont pas les lignes dans le même ordre. Il faut être vigilant lors de ce genre de combinaison de tableaux. Il est préférable d’employer des fonctions de fusion de tableaux plus complexes comme full_joint() (ci-dessous). Pour utiliser correctement bind_cols(), il faut vous assurer que les lignes des deux tableaux correspondent exactement, par exemple, en utilisant arrange() : nutrients2 &lt;- arrange(nutrients, student, sample) rmarkdown::paged_table(nutrients2) Le tableau nutrients2 a maintenant les données présentées dans le même ordre (en lignes) que le tableau physico. Nous pouvons donc rassembler ces deux tableaux à l’aide de bind_cols() : oceano &lt;- bind_cols(nutrients2, physico) rmarkdown::paged_table(oceano) Après vérification de l’adéquation des lignes, nous n’aurons plus besoin des colonnes sample1 et student1. La vérification automatique à l’aide de code R et l’élimination de ces variables du tableau oceano vous sont laissées comme exercices… 6.5.3 Fusion de tableaux La fusion fera intervenir une ou plusieurs colonnes communes des deux tableaux pour déterminer quelles lignes du premier correspondent aux lignes du second. Ainsi, la fusion des tableaux est assurée d’être réalisée correctement quel que soit l’ordre des lignes dans les deux tableaux d’origine. Utilisons full_join() en joignant les lignes en fonction des valeurs de student et sample : oceano &lt;- full_join(nutrients, physico, by = c(&quot;student&quot;, &quot;sample&quot;)) rmarkdown::paged_table(oceano) Observez bien ce dernier tableau. L’ordre retenu est celui de nutrients (le premier), mais les données issues de physico ont été retriées avant d’être fusionnées pour que les données correspondent. Comparez au tableau physico d’origine réimprimé ci-dessous : rmarkdown::paged_table(physico) Il existe, en fait, plusieurs versions pour la fusion de tableaux, représentées par une série de fonctions xxx_join(). Lorsque les lignes entre les deux tableaux fusionnés correspondent parfaitement comme dans l’exemple traité ici, les différentes variantes ont le même effet. Mais lorsque des lignes diffèrent, les variantes ont leur importance : full_join() garde toutes les lignes, full_join() par gadenbuie. left_join() ne garde que les lignes uniques du tableau de gauche en plus des lignes communes, left_join() par gadenbuie. right_join() ne garde que les lignes uniques du tableau de droite en plus des lignes communes, right_join() par gadenbuie. inner_join() garde uniquement les lignes communes aux deux tableaux, inner_join() par gadenbuie. A vous de jouer Ouvrez RStudio dans votre SciViews Box, puis exécutez l’instruction suivante dans la fenêtre console : BioDataScience::run(&quot;06b_recombinaison&quot;) Pour les étudiants de Bioinformatique et Sciences des données à charleroi Pour cette activité, vous allez travailler en équipe sur les données d’une analyse métagénomique sur 4 communautés microbiennes obtenues par une équipe de recherche de l’Université de Mons. Créez un rapport et effectuez les différents exercices en suivant les instructions qui sont dans le fichier README.mddu dépôt accessible depuis : Pour l’année académique 2019-2020, les URLs à utiliser pour accéder à votre tâche sont les suivants : Cours de Bioinformatique et Sciences des données à Charleroi : https://classroom.github.com/g/6UoszJja Cours de Sciences des données I à Mons : Pour les autres utilisateurs de ce livre, veuillez faire un “fork” du dépôt sdd1_metagenomics. Si vous souhaitez accéder à une version précédente de l’exercice, sélectionner la branche correspondante à l’année que vous recherchez. Pour les étudiants de Sciences des données I à Mons : Pour cette activité, vous allez travailler en binome sur les données de la population belge. Créez un rapport et effectuez les différents exercices en suivant les instructions qui sont dans le fichier README.mddu dépôt accessible depuis : Pour l’année académique 2019-2020, l’URL à utiliser pour accéder à votre tâche sont les suivants : Cours de Sciences des données I à Mons : https://classroom.github.com/g/nohKNIkL Pour les autres utilisateurs de ce livre, veuillez faire un “fork” du dépôt belgium_inhabitants. Si vous souhaitez accéder à une version précédente de l’exercice, sélectionner la branche correspondante à l’année que vous recherchez. Pour en savoir plus Un aide-mémoire général en science des données avec lien vers les sites webs importants et les autres aide-mémoires. "],
["proba.html", "Module 7 Probabilités &amp; distributions", " Module 7 Probabilités &amp; distributions Objectifs Appréhender le calculs de probabilités Appréhender les principales lois de distributions et leurs utilisations pratiques Prérequis Vous devez être à l’aise avec l’utilisation de R, RStudio et R Markdown. Vous avez appris à maîtriser ces outils dans les modules 1 &amp; 2. "],
["probabilites.html", "7.1 Probabilités", " 7.1 Probabilités La vidéo suivante vous introduit la notion de probabilité et le calcul de probabilités d’une manière plaisante à partir d’un jeu de hasard proposé par un petit chat à ses amis… Sachant qu’un événement en statistique est un fait qui se produit, la probabilité que cet événement se produise effectivement peut être quantifiée sur base de l’observation des réalisations passées. Ainsi si l’événement en question s’est produit, disons, 9 fois sur un total de 12 réalisations, on dira que la probabilité que cet événement se produise est de 9/12, soit 0,75. Notez qu’une probabilité est un nombre compris entre zéro (lorsqu’il ne se produit jamais) et un (lorsqu’il se produit toujours). On écrira, pour la probabilité de l’événement E : \\[0 \\leq \\mathrm{P}(E) \\leq 1\\] 7.1.1 Dépistage Voyons tout de suite une application plus proche de la biologie : le dépistage d’une maladie qui touche 8% de la population. Le test de dépistage mis en place détecte 95% des malades. De plus, le test se trompe dans 10% des cas pour les personnes saines. Comment connaitre le risque d’être malade si on est diagnostiqué positif par ce test ? Pour résoudre ce problème, nous devons d’abord apprendre à combiner des probabilités. Ce n’est pas bien compliqué. Si on a affaire à des événements successifs indépendants (c’est-à-dire que l’occurrence de l’un ne dépend pas de l’occurrence de l’autre), la probabilité que les deux événements successifs indépendants se produisent tous les deux est la multiplication des deux probabiltés. On pourra écrire : \\[\\mathrm{P}(E_1 \\,\\mathrm{et}\\, E_2) = \\mathrm{P}(E_1) * \\mathrm{P}(E_2)\\] Vous pouvez dès lors calculer la probabilité que l’on teste un patient malade (probabilité = 0,08) et que le test soit positif (0,95) dans ce cas : # Personne malade et détectée positive (p_sick_positive &lt;- 0.08 * 0.95) # [1] 0.076 Ceci n’indique pas la probabilité que le test soit positif car il est également parfois (erronément) positif pour des patients sains. Mais au fait, quelle est la probabilité d’avoir un patient sain ? La probabilité que l’un parmi tous les événements possibles se produise vaut toujours un. Les seuls événements possibles ici sont que le patient soit sain ou malade. Donc, \\[\\mathrm{P}(sain) + \\mathrm{P}(malade) = 1 \\rightarrow \\mathrm{P}(sain) = 1 - \\mathrm{P}(malade) = 0.92\\] Nous pouvons maintenant déterminer la probabilité que le test soit positif dans le cas d’une personne saine : # Personne saine et détectée positive (p_healthy_positive &lt;- 0.92 * 0.10) # [1] 0.092 Bon, il nous reste à combiner les probabilités que le test soit positif si la personne est malade et si la personne est saine. Mais comment faire ? Ici, on n’a pas affaire à des événements successifs, mais à des évènements mutuellement exclusifs. On les appellent des événements disjoints. Pour déterminer si l’un parmi deux événements disjoints se produit, il suffit d’additionner leurs probabilités respectives. Nous pouvons maintenant déterminer la probabilité que le test soit positif quelle que soit la personne testée : # La probabilité que le test soit positif (p_positive &lt;- p_sick_positive + p_healthy_positive) # [1] 0.168 Nous nous trouvons ici face à un résultat pour le moins surprenant ! En effet, nous constatons que le test est positif dans 16,8% des cas, mais seulement 7,6% du temps, il sera correct (probabilité d’une personne malade détectée). Parmi tous les cas positifs au test, il y en a… p_sick_positive / p_positive # [1] 0.452381 … seulement 45,2% qui sont effectivement malades (on parle de vrais positifs) ! Ceci ne correspond pas du tout aux indications de départ sur les performances du test. Dans le cas de deux faits successifs qui ne peuvent chacun que résulter en deux événements, nous avons seulement quatre situations possibles. Si l’un des cas est qualifié de positif et l’autre de négatif, nous aurons : les vrais positifs (test positif alors que la personne est malade), ici 0.08 * 0.95 les faux positifs (test positif alors que la personne est saine), ici 0.92 * 0.10 les vrais négatifs (test négatif alors que la personne est saine), ici 0.92 * 0.90 les faux négatifs (test négatif alors que la personne est malade), ici 0.08 * 0.05 En fait, les performances finales du test de dépistage dépendent aussi de la prévalence de la maladie. Ainsi pour une maladie très commune qui affecterait 80% de la population, nous obtenons : # Faux positifs 0.20 * 0.10 # [1] 0.02 # Vrais positifs 0.80 * 0.95 # [1] 0.76 # Total des positifs 0.20 * 0.10 + 0.80 * 0.95 # [1] 0.78 # Fractions de tests positifs qui sont corrects (0.80 * 0.95) / (0.20 * 0.10 + 0.80 * 0.95) # [1] 0.974359 Ouf ! Dans ce cas-ci le test positif est correct dans 97,4% des cas. Mais qu’en serait-il si la maladie est très rare (probabilité de 0,008) ? # Faux positifs 0.992 * 0.10 # [1] 0.0992 # Vrais positifs 0.008 * 0.95 # [1] 0.0076 # Total des positifs 0.992 * 0.10 + 0.008 * 0.95 # [1] 0.1068 # Fractions de tests positifs qui sont corrects (0.008 * 0.95) / (0.992 * 0.10 + 0.008 * 0.95) # [1] 0.07116105 Dans ce cas, un test positif n’aura effectivement détecté un malade que dans … 7,1% des cas ! Les 92,9% autres cas positifs seront en fait des personnes saines. Comme nous pouvons le constater ici, le calcul des probabilités est relativement simple. Mais en même temps, les résultats obtenus peuvent être complètement contre-intuitifs. D’où l’intérêt de faire ce genre de calcul justement. 7.1.2 Arbre des probabilités Il se peut que tout cela vous paraisse très (trop) abstrait. Vous êtes peut-être quelqu’un de visuel qui comprend mieux les concepts en image. Dans ce cas, la méthode alternative de résolution des calculs de probabilités via les arbres de probabilités devrait vous éclairer. Le principe consiste à représenter un arbre constitué de nœuds (des faits qui se produisent). De ces nœuds, vous représentez autant de branches (des segments de droites) que d’événements possibles. La figure suivante est l’arbre des probabilités correspondant au cas du dépistage de la maladie qui touche 8% de la population. Arbre de probabilités permettant de déterminer la probabilité d’avoir un résultat positif au test. Du premier nœud (le fait qu’une personne est atteinte ou non de la maladie), nous avons deux branches menant aux deux événements “malade” et “sain”. Chacune de ces deux situations est un nouveau nœud d’où deux événements sont possibles à chaque fois (2 fois 2 nouvelles branches) : un test “positif”, ou un test “négatif”. Les nœuds terminaux (les “négatifs” et “positifs” ici) sont aussi appelés les feuilles de l’arbre. L’arbre reprend donc tous les cas possibles depuis le nœud de départ (sa racine), jusqu’aux feuilles. L’étape suivante consiste à aller indiquer le long des branches les probabilités associées à chaque événement : 0.08 pour “malade”, 0.95 pour un dépistage “positif” si la personne est malade, etc. A ce stade, une petite vérification peut être faite. La somme des probabilités aux feuilles doit toujours valoir un, et il en est de même de la somme de toutes les branches issues d’un même nœud. Le calcul se fait ensuite comme suit. On repère tous les cas qui nous intéressent. Ici, il s’agit de toutes les trajectoires qui mènent à un test “positif”. Le calcul des probabilités se fait en multipliant les probabilités lorsqu’on passe d’un noeud à l’autre et en additionnant les probabilités ainsi calculées le long des feuilles terminales de l’arbre considéré. Donc, le chemin “malade” -&gt; “positif” correspond à 0.08 * 0.95 = 0.076. Le chemin “sain” -&gt; “positif” correspond à 0.92 * 0.10 = 0.092. Enfin, nous sommons les probabilités ainsi calculées pour toutes les feuilles de l’arbre qui nous intéressent. Ici, ce sont toutes les feuilles qui correspondent à un test “positif”, soit 0.076 + 0.092 = 0.168. Et voilà ! Nous avons répondu au problème : la probabilité d’avoir un résultat positif avec le test de dépistage dans un population dont 8% est atteint de la maladie est de 16.8%. 7.1.3 Théorème de Bayes Nous devons introduire ici le concept de probabilité conditionnelle. Une probabilité conditionnelle est la probabilité qu’un événement E2 se produise si et seulement si un premier événement E1 s’est produit (E1 et E2 sont deux événements successifs). La probabilité conditionnelle s’écrit \\(\\mathrm{P}(E2|E1)\\). Vous noterez que l’arbre des probabilités représente, en réalité, des probabilités conditionnelles à partir du second niveau (l’arbre peut être évidemment plus complexe). Arbre de probabilités avec probabilités conditionnelles en rouge. On pourra considérer, donc, la probabilité conditionnelle d’avoir un résultat positif au test, si la personne est malade \\(\\mathrm{P}(positif|malade)\\). Vous l’avez indiquée plus haut dans l’arbre des probabilités, c’est 0.95. Maintenant, la probabilité qu’une personne soit malade si elle est positive au test \\(\\mathrm{P}(malade|positif)\\) est une information capitale ici. Le lien entre les deux n’est pas facile à faire. C’est grâce aux travaux du révérend Thomas Bayes au 18ème siècle que ce problème a été résolu. Les implications du théorème de Bayes sont énormes car cela permet de déterminer des probabilités dites a posteriori en fonction de connaissances a priori. Si nous réanalysons le raisonnement qui est fait dans l’arbre de probabilités, on peut remarquer que le premier calcul (“malade” -&gt; “positif”) correspond en fait à la probabilité que le test soit positif si le patient est malade \\(\\mathrm{P}(positif|malade)\\) multipliée par la probabilité que le patient soit malade \\(\\mathrm{P}(malade)\\), et ceci est aussi égal à \\(\\mathrm{P}(positif\\, et\\, malade)\\). Donc, \\[\\mathrm{P}(positif|malade) * \\mathrm{P}(malade) = \\mathrm{P}(positif\\, et\\, malade)\\] Par un raisonnement symétrique, on peut aussi dire que : \\[\\mathrm{P}(malade|positif) * \\mathrm{P}(positif) = \\mathrm{P}(positif\\, et\\, malade)\\] Donc, nous avons aussi : \\[\\mathrm{P}(malade|positif) * \\mathrm{P}(positif) = \\mathrm{P}(positif|malade) * \\mathrm{P}(malade)\\] … et en divisant les deux termes par \\(\\mathrm{P}(positif)\\), on obtient : \\[\\mathrm{P}(malade|positif) = \\frac{\\mathrm{P}(positif|malade) * \\mathrm{P}(malade)}{\\mathrm{P}(positif)}\\] De manière générale, le théorème de Bayes s’écrit : \\[\\mathrm{P}(A|B) = \\frac{\\mathrm{P}(B|A) * \\mathrm{P}(A)}{\\mathrm{P}(B)}\\] Nous avons maintenant une façon simple de déterminer \\(\\mathrm{P}(malade|positif)\\) à partir de \\(\\mathrm{P}(positif|malade)\\), \\(\\mathrm{P}(malade)\\), et \\(\\mathrm{P}(positif)\\), c’est-à-dire des probabilités auxquelles nous avons facilement accès expérimentalement en pratique. Calculez comme exercice la probabilité qu’un patient soit malade s’il est positif au test via le théorème de Bayes, et comparez le résultat de votre calcul à ce que nous avions obtenu plus haut (45.2%). A retenir Probabilité d’un événement : \\[\\mathrm{P}(E) = \\frac{\\mathrm{nbr\\ occurences\\ } E}{\\mathrm{nbr\\ total\\ essais}}\\] Probabilité de deux événements successifs (cas général) : \\[\\mathrm{P(A\\, \\mathrm{et}\\, B)} = \\mathrm{P}(B|A) * \\mathrm{P(A)}\\] Probabilité qu’un parmi deux événements se produise (cas général) : \\[\\mathrm{P(A\\, \\mathrm{ou}\\, B)} = \\mathrm{P}(A) + \\mathrm{P(B)} - \\mathrm{P}(A\\, \\mathrm{et}\\, B)\\] Application vraiment utile du théorème de Bayes par xkcd. Pour en savoir plus Une autre explication du théorème de Bayes (en anglais). 7.1.4 Probabilités et contingence Comme un tableau de contingence indique le nombre de fois que des événements ont pu être observés, il peut servir de base à des calculs de probabilités. Partons du dénombrement de fumeur en fonction du revenu dans une population. tabac &lt;- data.frame( revenu_faible = c(634,1846,2480), revenu_moyen = c(332, 1622,1954), revenu_eleve = c(247,1868,2115), total = c(1213, 5336, 6549)) rownames(tabac) &lt;- c(&quot;fume&quot;, &quot;ne fume pas&quot;, &quot;total&quot;) knitr::kable(tabac) revenu_faible revenu_moyen revenu_eleve total fume 634 332 247 1213 ne fume pas 1846 1622 1868 5336 total 2480 1954 2115 6549 Quelle est la probabilité d’être un fumeur \\(\\mathrm{P}(fumeur)\\) ? Rappelons-nous de la définition de probabilité : nombre de cas où l’événement se produit sur le nombre total de cas. Ici, on a 1213 fumeurs dans un effectif total de l’échantillon de 6549 personnes, soit : 1213 / 6549 # [1] 0.1852191 Quelle est la probabilité d’être fumeur si le revenu élevé \\(\\mathrm{P}(fumeur|revenu\\_eleve)\\) ? Le nombre de fumeurs à revenus élevés se monte à 247. Attention, ici l’échantillon de référence n’est plus la population totale, mais seulement ceux qui ont des revenus élevés, donc 2115 personnes : 247 / 2115 # [1] 0.1167849 Quelle est la probabilités d’avoir un revenu faible ou d’avoir un élevé ? Cette question peut s’écrire : \\(\\mathrm{P}(revenu\\-faible\\, ou\\, revenu\\_eleve)\\). 2480 / 6549 + 2115 / 6549 # [1] 0.7016338 Il s’agit d’une somme de probabilités disjointes. Quelle est la probabilités d’être fumeur ou d’avoir un revenu moyen ? Cette question peut s’écrire : \\(\\mathrm{P}(fumeur\\, ou\\, revenu\\_moyen)\\). 1213 / 6549 + 1954 / 6549 - 332 / 6549 # [1] 0.4328905 Il s’agit d’une somme de probabilités non disjointes30. 7.1.4.1 Populations de taille infinie Dans une population, voici les proportions de différents groupes sanguins : 44% O, 42% A, 10% B, 4% AB Quelles est la probabilité d’obtenir 1 individu du groupe B ? Cette question peut s’écrire : \\(\\mathrm{P}(B)\\). 0.10 # [1] 0.1 Quelle est la probabilité d’obtenir 3 individus du groupe B d’affilée ? Cette question peut s’écrire : \\(\\mathrm{P}(B\\, et\\, B\\, et\\, B)\\). 0.10 * 0.10 * 0.10 # [1] 0.001 Nous avons ici 3 événements successifs indépendants. Donc, on multiplie leurs probabilités respectives. 7.1.4.2 Populations de taille finie Dans une population de 100 personnes dont les proportions des différentes groupes sanguins sont identiques au cas précédent. Quelles est la probabilité d’obtenir un échantillon de trois individus du groupe B issus de cette population31 ? Cette question peut s’écrire : \\(\\mathrm{P}(B\\, et\\, B\\, et\\, B)\\). 10 / 100 * 9 / 99 * 8 / 98 # [1] 0.000742115 Il s’agit d’événements successifs non-indépendants. En effet, le retrait d’un individu de la population de taille finie modifie les proportions relatives des groupes sanguins dans le reste de la population, et donc, les probabilités aux tirages suivants. Ainsi pour le groupe B, nous n’avons plus que 9 individus de ce groupe dans une population de 99 individus après le premier tirage d’un individu du groupe B ! Autrement dit, \\(\\mathrm{P}(B|B) \\neq \\mathrm{P}(B|not\\ B)\\). On a donc, \\(\\mathrm{P}(B|B) = 9/99\\) et ensuite \\(\\mathrm{P}(B|B\\, \\mathrm{et}\\, B) = 8/98\\). A vous de jouer Ouvrez RStudio dans votre SciViews Box, puis exécutez l’instruction suivante dans la fenêtre console : BioDataScience::run(&quot;07a_proba&quot;) Si E1 et E2 sont deux événements non disjoints, la probabilité que l’un de ces deux événements se produise est : \\(\\mathrm{P}(E1\\, ou\\, E2) = \\mathrm{P}(E1) + \\mathrm{P}(E2) - \\mathrm{P}(E1\\, et\\, E2)\\).↩ En statistique, on appelle cela un tirage au sort sans remise. Le résultat est très différent si le premier individu tiré au hasard était remis dans la population et pouvait être éventuellement pris à nouveau au second ou troisième tirage (tirage au sort avec remise). Notez aussi que, pour une population de taille infinie ou quasi-infinie, les deux types de tirage au sort sont équivalents à celui avec remise car enlever un individu d’une population infinie ne change pas fondamentalement son effectif, donc les probabilités ultérieures.↩ "],
["lois-de-distributions.html", "7.2 Lois de distributions", " 7.2 Lois de distributions Étant donné que les sciences des données reposent sur un nombre (si possible important) de répétitions d’une mesure -des réplicats-, il est possible de déterminer à quelle fréquence un événement E se produit de manière expérimentale. La probabilité observée est quantifiable sur base d’un échantillon comme nous venons de le voir dans la section précédente. La probabilité théorique est connue si le mécanisme sous-jacent est parfaitement connu. Donc, en situation réelle, seule la probabilité observée est accessible, et ce n’est qu’une approximation de la vraie valeur, ou valeur théorique. Cependant, dans des situations particulières les statisticiens ont calculé les probabilités théoriques. Ce sont des lois de distribution. Elles associent une probabilité théorique à chaque événement possible. La comparaison des probabilités théoriques et observées constitue l’un des piliers des statistiques. Le raisonnement est le suivant : si les probabilités observées sont suffisamment proches des probabilités théoriques, alors, nous pouvons considérer que les événements sont générés selon un mécanisme identique ou proche de celui qui est à la base de la loi de distribution théorique correspondante. Même dans la vie de tous les jours, les calculs de probabilités peuvent être utiles, enfin… d’après xkcd. Avant d’explorer ces lois de distributions statistiques, nous devons d’abord introduire la distinction entre probabilité discrète et probabilité continue. Une probabilité discrète est associée à une variable qualitative ou à la rigueur, à une variable continue discrète qui peut prendre un nombre fini -et généralement relativement petit- de valeurs. A chaque valeur est associé un événement, et chaque événement a une certaine probabilité de se produire dans un contexte donné. Jusqu’à présent, nous n’avons traité que ce cas-là. Par contre, une variable quantitative continue peut prendre un nombre infini de valeurs matérialisées généralement par l’ensemble des nombres réels. Dans ce cas, l’association d’un événement à une valeur de la variable, et d’une probabilité à chaque événement reste vraie en théorie. Mais en pratique, ces probabilités dites continues ne sont pas calculables par les équations étudiées jusqu’ici. Par contre, les lois de distributions continues permettent des calculs, moyennant une petite astuce que nous étudierons plus loin dans ce chapitre. A vous de jouer Tout au long de cette section des questions sous la forme d’un learnr vous sont proposées. Complétez progressivement le learnr avec vos nouvelles connaissances Ouvrez RStudio dans votre SciViews Box, puis exécutez l’instruction suivante dans la fenêtre console : BioDataScience::run(“07b_distri”) "],
["distribution-uniforme.html", "7.3 Distribution uniforme", " 7.3 Distribution uniforme La loi de la distribution uniforme se rapporte à un mécanisme qui génère tous les événements de manière équiprobable. 7.3.1 Distribution discrète Dans le cas d’événements discrets, si \\(n_E\\) est le nombre total d’événements possibles, la probabilité d’un de ces événements vaut donc : \\[\\mathrm{P}(E) = \\frac{1}{n_E}\\] La distribution uniforme est d’application pour les jeux de hasard (dés, boules de loto, …). En biologie, elle est plus rare. Dans le cas d’un sexe ratio de 1:1 (autant de mâles que de femelles), la probabilité qu’un nouveau né soit un mâle ou une femelle suit une distribution uniforme et vaut 1/2. La distribution spatiale des individus dans une population biologique peut être uniforme lorsque les individus interagissent de telle manière que la distance entre eux soit identique (par exemple, dans un groupe de manchots Aptenodytes patagonicus sur la banquise). Imaginons un animal hypothétique pour lequel la portée peut être de 1 à 4 petits de manière équiprobable. Nous avons alors 1/4 des portées qui présentent respectivement, 1, 2, 3 ou 4 petits (Fig. 7.1). Figure 7.1: Probabilité du nombre de petits dans une portée qui suivrait un distribution strictement uniforme entre 1 et 4. 7.3.2 Distribution continue D’emblée, nous pouvons facilement démontrer quel est le problème avec les probabilités dans le cas de la distribution uniforme continue. Nous avons en effet, un nombre infini d’événements équiprobables possibles. Donc, la probabilité de chaque événement est (\\(n_E = \\infty\\)) : \\[\\mathrm{P}(E) = \\frac{1}{\\infty} = 0\\] … et ce calcul est correct ! Dans le cas de probabilités continues, la probabilité d’un événement en particulier est toujours nulle. Nous pouvons seulement calculer que l’un parmi plusieurs événements se produise (compris dans un intervalle). La représentation graphique d’une loi de distribution continue est un outil utile pour la comprendre et vérifier ses calculs. La forme la plus courante consiste à montrer la courbe de densité de probabilité pour une distribution continue. Sur l’axe X, nous avons les quantiles (les valeurs observables), et sur l’axe Y, la densité de probabilité32. Par exemple, si nous constatons qu’un insecte butineur arrive sur une fleur en moyenne toutes les 4 minutes, la probabilité qu’un butineur arrive dans un intervalle de temps compris entre 0 et 4 min depuis le moment initial \\(t_0\\) de nos observations suit une distribution uniforme continue (Fig. 7.2). Figure 7.2: Probabilité qu’un nouvel insecte butineur arrive dans un intervalle de 0 à 4 min si, en moyenne, un insecte arrive toutes les 4 min. Une autre représentation courante est la densité de probabilité cumulée qui représente la probabilité d’observer un quantile ou moins. Dans le cas présent, cela représente la probabilité qu’au moins un insecte butineur soit observé pour des durées d’observation croissantes (Fig. 7.3). Figure 7.3: Probabilité cumulée qu’un nouvel insecte butineur arrive dans un intervalle de 0 à 4 min si, en moyenne, un insecte arrive toutes les 4 min. Notation : nous noterons qu’une variable suit une loi de distribution comme ceci (le tilde ~ se lit “suit une distribution”, et U représente la distribution uniforme avec entre parenthèse, les paramètres de la distribution, ici, les bornes inférieure et supérieure) : \\[X \\sim U(0, 4)\\] Cela signifie : “la variable aléatoire X suit une distribution uniforme 0 à 4”. La distribution \\(U(0, 1)\\) est particulière et est appelée distribution uniforme standard. Elle a la propriété particulière que si \\(X \\sim U(0, 1)\\) alors \\((1-X) \\sim U(0, 1)\\). 7.3.3 Quantiles vers probabilités L’aire sous la courbe représente une probabilité associée à l’intervalle considéré pour les quantiles. Répondez aux questions suivantes pour notre variable \\(X \\sim U(0,4)\\) : Quelle est la probabilité que X = 1 (un insecte butineur arrive après exactement 1,00000… min d’observation) ? Quelle est la probabilité que X soit compris entre 1 et 1,5 ? La réponse à la question (1) est immédiate. Cette probabilité est nulle (voir plus haut)33 ! Pour la question (2), nous pouvons répondre en calculant l’aire sous la courbe entre les quantiles 1 et 1,5 (représentée par l’aire en rouge à la Fig. 7.4). Figure 7.4: Probabilité qu’un insecte butineur arrive entre 1 et 1,5 min après le début d’une observation (aire P en rouge). Ici, le calcul est assez simple à faire à la main. Mais nous verrons d’autres lois de distribution plus complexes. Dans tous les cas, R offre des fonctions qui calculent les aires à gauche ou à droite d’un quantile donné. Le nom de la fonction est toujours p&lt;distri&gt;(), avec pour la distribution uniforme punif(). L’aire à gauche du quantile nécessite de spécifier l’argument lower.tail = TRUE (“queue en bas de la distribution” en anglais). Pour l’aire à droite, on indiquera évidemment lower.tail = FALSE. Donc, pour calculer la probabilité qu’un insecte arrive en moins de 1,5 sec, nous écrirons : punif(1.5, min = 0, max = 4, lower.tail = TRUE) # [1] 0.375 Mais comme nous voulons déterminer la probabilité qu’un insecte arrive entre 1 et 1,5 sec, nous devons soustraire à cette valeur la probabilité qu’un insecte arrive en moins de 1 sec (la zone hachurée en rouge dans la Fig 7.4 est en effet l’aire à gauche depuis le quantile 1,5 moins l’aire à gauche depuis le quantile 1) : punif(1.5, min = 0, max = 4, lower.tail = TRUE) - punif(1.0, min = 0, max = 4, lower.tail = TRUE) # [1] 0.125 La réponse est 0,125, soit une fois sur huit. Le calcul de probabilités sur base de lois de distributions continues se fait via les aires à gauche ou à droite d’un quantile sur le graphique de densité de probabilité. Pour une aire centrale, nous soustrayons les aires à gauche des deux quantiles respectifs. 7.3.4 Probabilités vers quantiles Le calcul inverse est parfois nécessaire. Par exemple pour répondre à la question suivante : Combien de temps devons-nous patienter pour observer l’arrivée d’un insecte butineur sur la fleur une fois sur trois observations en moyenne ? Ici, nous partons d’une probabilité (1/3) et voulons déterminer le quantile qui définit une aire à gauche de 1/3 sur le graphique (Fig. 7.5). Figure 7.5: Temps d’observation nécessaire (quantile Q) pour voir arriver un butineur une fois sur trois (aire P en rouge de 1/3 à gauche de Q). Dans R, la fonction qui effectue ce calcul est q&lt;distri&gt;(). Donc ici, il s’agit de qunif(). Les arguments sont les mêmes que pour punif() sauf le premier qui est une ou plusieurs probabilités. Nous répondons à la question de la façon suivante : qunif(1/3, min = 0, max = 4, lower.tail = TRUE) # [1] 1.333333 Donc il faut observer pendant 1,33 min (1 min et 20 sec) pour avoir 1 chance sur 3 d’observer l’arrivée d’un insecte butineur. 7.3.5 Calcul avec les snippets La SciViews Box propose différents snippets pour nous aider à effectuer nos différents calculs et graphiques relatifs à la distribution uniforme continue. Ils se retrouvent dans le menu (d)istributions accédé depuis .... Donc ..i donne directement accès à ce menu, et puis (d)istributions: uniform accédé depuis .iu directement. Ensuite, il suffit de choisir le snippet dans le menu déroulant (voir ci-dessous). .iuproba : calcul de probabilités depuis des quantiles .iuquant : calcul de quantile depuis des probabilités .iurandom : génération de nombres pseudo-aléatoires .iudens : graphique de la densité de probabilité .iucumul : graphique de la densité de probabilité cumulée .iullabel : ajout d’un label sur le graphique à gauche .iurlabel: ajout d’un label sur le graphique à droite Le snippet .iurandom nécessite quelques explications supplémentaires. R est capable de simuler la génération de nombres aléatoires selon différentes lois de distribution (r&lt;distri&gt;()). runif() est la fonction qui le fait pour une distribution uniforme continue. Comme il ne s’agit pas réellement de nombres aléatoires, on parle de générateur de nombres pseudo-aléatoires. En fait, il s’agit d’une série de nombres qui a les mêmes propriétés que des nombres réellement aléatoires. R se positionne au hasard dans cette série. Donc, à chaque fois que vous appelez la fonction runif(), vous obtenez logiquement des valeurs différentes. A des fins de reproductibilité, il est possible de forcer R à partir en un point précis de la série avec la fonction set.seed() avec un nombre comme argument qui donne la position. Par exemple set.seed(281)34. La génération de nombres aléatoires dans les instructions qui suivent seront alors toujours les mêmes. Voici un exemple de 10 nombres aléatoires générés depuis une distribution uniforme standard (compris entre 0 et 1). Chaque fois que vous exécuterez ces deux instructions exactement l’une après l’autre, vous obtiendrez toujours la même suite. Si vous ré-exécutez la seconde instruction sans la première, vous obtiendrez une suite différente. set.seed(946) runif(10, min = 0, max = 1) # [1] 0.6378020 0.7524999 0.5593599 0.6688387 0.8989262 0.5300384 0.1520689 # [8] 0.9031163 0.2693327 0.6738862 Plus la densité de probabilité est élevée, plus les événements dans cette région du graphique sont probables.↩ La question n’est pas après environ une minute, mais à exactement 1 min 0 sec, 0 millisec, …, ce qui est alors hautement improbable.↩ Si vous utilisez set.seed() prenez soin de spécifier toujours une valeur différente prise au hasard comme argument !↩ "],
["distribution-binomiale.html", "7.4 Distribution binomiale", " 7.4 Distribution binomiale Partons d’un exemple pratique pour découvrir cette distribution. La mucoviscidose est, dans la population européenne, la plus fréquente des maladies génétiques héréditaires. Elle se caractérise par un mucus (voies respiratoires) anormalement épais qui est à l’origine de diverses complications. L’altération d’une protéine CFTR est à l’origine de cette maladie. Comme le gène qui code pour cette protéine est récessif, il faut que le deux allèles soient porteurs simultanément de la mutation pour que la maladie apparaisse. Parmi des familles de six enfants dont le père et la mère normaux sont tous deux porteurs hétérozygotes du gène altéré, quelle est la probabilité d’obtenir 0, 1, 2, …, 6 enfants atteints de mucoviscidose ? 7.4.1 Epreuve de Bernoulli La distribution binomiale est un loi de distribution discrète qui répond à ce genre de question. Ses conditions d’applications sont : résultats binaire (deux événements possibles uniquement ; l’un sera nommé “succès” et l’autre “échec” par convention), essais indépendants (les probabilités ne changement pas d’un essai à l’autre), n le nombre d’essais totaux est fixé à l’avance, probabilité du “succès” p constante (probabilité de l’“échec” = 1 - p). Les conditions particulières de cette situation sont appelées épreuve de Bernoulli. Mathématiquement, nous l’écrirons comme suit. Soit une variable aléatoire \\(Y\\) qui comptabilise le nombre de succès, la probabilité d’obtenir \\(j\\) succès parmi \\(n\\) essais est : \\[P(Y=j)= C^j_n \\times p^j \\times (1-p)^{n-j}\\] Le coefficient binomial \\(C^j_n\\) vaut35 : \\[C^j_n = \\frac{n!}{j!(n-j)!}\\] \\(C^j_n\\) représente le nombre de combinaisons possibles pour obtenir \\(j\\) succès parmi \\(n\\) essais réalisés dans un ordre quelconque. On pourra écrire aussi : \\[Y \\sim B(n,p)\\] Notre exemple rentre parfaitement dans le cadre de l’épreuve de Bernoulli avec n = 6 et p, la probabilité du succès, c’est-à-dire, d’avoir un enfant qui ne développe pas la maladie de 3/4 : \\(Y \\sim B(6, 0.75)\\). 7.4.2 Calculs et graphiques Les calculs sur base d’une distribution binomiale sont assez similaires à ceux de la distribution uniforme dans R, en remplaçant unif par binom dans le nom des fonction. Voici la liste des snippets à votre disposition dans la SciViews Box pour vous aider (menu (d)istributions: binomial à partir de .ib) : Puisqu’il s’agit d’une distribution discrète, un petit nombre d’événements possibles existent. Le snippet .ibtable retourne l’ensemble des valeurs possibles pour \\(j\\) allant de 1 à \\(n\\) en une seule étape. Les autres snippets devraient vous être familiers. (.table &lt;- data.frame(success = 0:6, probability = dbinom(0:6, size = 6, prob = 0.75))) # success probability # 1 0 0.0002441406 # 2 1 0.0043945312 # 3 2 0.0329589844 # 4 3 0.1318359375 # 5 4 0.2966308594 # 6 5 0.3559570312 # 7 6 0.1779785156 La représentation graphique donne la Fig. 7.6. Figure 7.6: Probabilité d’avoir j enfants sains parmi 6 dans des familles dont les deux parents sont porteurs hétérozygotes du gène de la mucoviscidose. La situation la plus probable est donc d’avoir 5 enfants sains sur 6. Nous pouvons aussi observer que, lorsque \\(p\\) s’éloigne de 0,5, les probabilités à l’extrême opposée tendent assez rapidement vers zéro (ici, la probabilité de n’avoir qu’un seul, ou aucun enfant sain). La distribution binomiale trouve de très nombreuses applications en biologie, en écologie, en génétique et dans d’autres disciplines. Elle permet même de représenter vos chances de réussite à l’examen de science des données biologiques ! Voici, pour finir, l’allure d’une distribution binomiale pour laquelle la probabilité du succès est égale à la probabilité d’échec (0,5). Cette distribution est symétrique. Figure 7.7: Probabilité d’avoir des garçons parmi une fratrie de 6 enfants (si le sexe ratio de 1:1). Le factoriel d’un nombre \\(n\\), noté \\(n!\\) est \\(1 \\times 2 \\times 3 \\times ... \\times n\\), avec \\(0! = 1\\).↩ "],
["distribution-de-poisson.html", "7.5 Distribution de poisson", " 7.5 Distribution de poisson Maintenant, nous pouvons poser la question autrement. Prenons un couple sain au hasard en Belgique, quelle est la probabilité que ce couple transmette la mucoviscidose à leur descendance ? Ne considérons pas ici les personnes elles-même atteintes de la maladie qui prendront certainement des précautions particulières. Sachant qu’une personne sur 20 est porteuse du gène défectueux (hétérozygote) dans la population saine belge, la probabilité de former un couple doublement hétérozygote qui pourrait transmettre la maladie est de36 : 1/20 * 1/20 # [1] 0.0025 … soit un couple sur 400. Donc, globalement, les probabilités d’avoir des enfants sains est beaucoup plus grande que 0.75 si nous incluons tous les couples belges de parents sains porteurs ou non. Cette probabilité est de37 : (399 * 1 + 1 * 0.75) / 400 # [1] 0.999375 Pour un couple au hasard sans connaissance a priori du fait que les parents soient porteurs ou non, la probabilité d’avoir un enfant atteint de la mucoviscidose est heureusement très, très faible, mais non nulle (de l’ordre de 1/1600 = 0,000625 = 1 - 0.999375). Si nous considérons maintenant une population suffisamment grande pour pouvoir espérer y trouver “statistiquement” une personne atteinte de mucoviscidose, nous pourrions décider d’étudier un échantillon aléatoire de 1600 enfants belges. La distribution binomiale requiert alors le calcul de \\(C^j_n\\) sur base de \\(n = 1600\\), ce qui revient à devoir calculer le factoriel de 1600 : factorial(1) # [1] 1 factorial(10) # [1] 3628800 factorial(100) # [1] 9.332622e+157 factorial(1600) # Warning in factorial(1600): value out of range in &#39;gammafn&#39; # [1] Inf Or, le factoriel est un nombre qui grandit très, très vite. Déjà le factoriel de 100 est un nombre à 157 chiffres. Nous voyons que R est incapable de calculer précisément le factoriel de 1000. Ce nombre est supérieur au plus grand nombre que l’ordinateur peut représenter pour un double en R (1.797693110^{308}). Donc, nous sommes incapables de répondre à la question à l’aide de la loi binomiale. 7.5.1 Evénements rares La distribution de Poisson permet d’obtenir la réponse à la question posée parce qu’elle effectue le calcul différemment. Cette distribution discrète a un seul paramètre \\(\\lambda\\) qui représente le nombre moyen de cas rares que l’on observe dans un échantillon donné, ou sur un laps de temps fixé à l’avance. Cette distribution est asymétrique pour de faible \\(\\lambda\\). Les conditions d’application sont : résultats binaire, essais indépendants (les probabilités ne changement pas d’un essai à l’autre), taille d’échantillon ou laps de temps que le phénomène est observé fixe, probabilité d’observation de l’évènement \\(\\lambda\\) faible. Pour une variable \\(Y \\sim P(\\lambda)\\), nous pouvons calculer la probabilité que \\(Y = 0\\), \\(Y = 1\\), … de la façon suivante : \\[P(Y=0) = e^{-\\lambda}\\] et \\[P(Y=k) = P(Y=k-1) \\times \\frac{\\lambda}{k}\\] Le calcul se réalise de proche en proche en partant de la probabilité de ne jamais observer l’événement. Comme l’événement est rare, la probabilité tend très rapidement vers une valeur extrêmement faible. Seul le calcul des quelques premiers termes est donc nécessaire. A titre d’exercice, faites le calcul pour notre exemple d’un échantillon de la population belge, avec \\(\\lambda = 1\\) comme paramètre. La densité de probabilité pour cette distribution est représentée à la Fig. 7.8. Figure 7.8: Probabilité d’occurence de mucoviscidose dans un échantillon aléatoire de 1600 belges. 7.5.2 Loi de Poisson dans R Les fonctions dans R relatives à la distribution de Poisson portent des noms &lt;x&gt;pois(), tel que ppois() pour calculer des probabilités, qpois() pour calculer des quantiles ou rpois() pour générer des nombres pseudo-aléatoires selon cette distribution. Voici la liste des snippets à votre disposition dans la SciViews Box pour vous aider (menu (d)istributions: poisson à partir de .ip) : Comme il y a la même proportion d’hommes et de femmes porteurs, nous avons 1/20 des hommes et 1/20 des femmes qui sont porteurs. Nous formons des couples au hasard en piochant un homme dans la population masculine et une femme dans la population féminine de manière indépendante. Dans ce cas, nous obtenons un couple double porteur à une fréquence de 1/20 * 1/20 = 1/400.↩ Sur 400 couples, 399 ont une probabilité d’engendrer des enfants sains (porteurs hétérozygotes ou non porteurs confondus) de 100%. A cela il faut ajouter un couple sur 400 qui aura une probabilité de 75% de faire des enfants non atteints car non homozygotes.↩ "],
["distribution-normale.html", "7.6 Distribution normale", " 7.6 Distribution normale La vidéo suivante vous permettra de récapituler certaines notions étudiées jusqu’ici concernant les types de variables et vous introduira la loi de distribution normale ou distribution de Gauss ou encore, gaussienne. 7.6.1 Une “courbe en cloche” La distribution normale est la distribution la plus utilisée en statistique. Elle se rencontre très souvent en biologie comme dans bien d’autres domaines, à chaque fois qu’une variable continue définie sur tout le domaine des réels est issue d’un nombre important de composantes indépendantes dont les effets sont additifs. La forme de sa densité de probabilité est caractéristique et dite “en cloche” (Fig. 7.9). Figure 7.9: Un exemple de distribution normale. Il s’agit d’une densité de probabilité symétrique et asymptotique à ses deux extrémités en + et -infini. La distribution normale a deux paramètres : la moyenne \\(\\mu\\) et l’écart type \\(\\sigma\\). Sa densité de probabilité est représentée par l’équation suivante : \\[\\Phi(Y) = \\frac{1}{ \\sigma \\sqrt{2 \\pi}} e^{-\\frac{1}{2} \\left( \\frac{Y - \\mu}{\\sigma} \\right)^2}\\] Pour une variable aléatoire \\(Y\\) qui suit une distribution normale avec une moyenne \\(\\mu\\) et un écart type \\(\\sigma\\), nous écrirons : \\[Y \\sim N(μ, σ)\\] 7.6.2 Loi normale réduite Parmi toutes les distributions normales possibles, un est particulière : la distribution normale réduite qui a toujours une moyenne nulle et un écart type unitaire. \\[N(0, 1)\\] Elle représente la distribution des valeurs pour une variable qui a été standardisée, c’est-à-dire, à laquelle on a soustrait la moyenne et que l’on a divisé par son écart type. \\[Z = \\frac{Y - \\mu}{\\sigma}\\] Sa formulation est nettement simplifiée. \\[\\Phi(Z) = \\frac{1}{\\sqrt{2 \\pi}} e^{-\\frac{Z^2}{2}}\\] La probabilité qu’une observation soit dans un intervalle de \\(\\pm 1 \\sigma\\) autour de la moyenne est de 2/3 environ. De même, un intervalle de \\(\\pm 2 \\sigma\\) définit une aire de 95%, et celle-ci devient supérieure à 99% pour des observations se situent dans l’intervalle \\(\\pm 3 \\sigma\\) (Fig. 7.10). Figure 7.10: La distribution normale réduite avec les aires centrales autour de 1 et 2 écarts types mises en évidence. 7.6.3 Fonctions et snippets Les fonctions relatives à la distribution normale dans R sont &lt;x&gt;norm(). Le calcul de probabilités se fait à l’aide de pnorm(), de quantiles à partir de qnorm(). Un échantillon pseudo-aléatoire s’obtient à partir de rnorm(). Une série de snippets est à votre disposition dans la SciViews Box pour vous aider (menu (d)istributions: normal à partir de .in) : 7.6.4 Théorème central limite Une des raisons pour lesquelles la distribution normale est très répandue est liée au fait que beaucoup d’autres distributions tendent vers elle de manière asymptotique. Par exemple, une distribution binomiale symétrique (avec \\(p = 0.5\\)) et pour un \\(n\\) croissant ressemblera de plus en plus à une distribution normale. Le théorème central limite démontre cela quelle que soit la distribution de départ. En pratique, la distribution normale est souvent une bonne approximation d’autres distributions pour des tailles d’échantillons déjà à partir de quelques dizaines d’individus. "],
["distribution-log-normale.html", "7.7 Distribution log-normale", " 7.7 Distribution log-normale La loi log-normale est utilisée pour représenter la distribution d’une variable aléatoire qui résulte de la multiplication d’un grand nombre de petits effets indépendants entre eux. C’est le cas en biologie et en chimie, par exemple, la taille d’un animal, la concentration d’une molécule en solution, la température d’un matériau, etc. Ces variables sont définies uniquement pour des valeurs nulles ou positives. Une taille et une concentration négatives ne sont pas possibles. De même pour une température en dessous du zéro absolu (attention, dans ce cas, la température doit être mesurée en Kelvin). 7.7.1 Transformée log La distribution log-normale devient normale lorsque l’on transforme la variable en son logarithme. Donc, si \\[X \\sim log{\\text -}N(0, 0.5)\\] alors \\[log(X) \\sim N(0, 0.5)\\] Par facilité, on défini ses deux paramètres de manière relative à la moyenne \\(\\mu\\) et à l’écart type \\(\\sigma\\) qu’a la distribution normale obtenue après transformation logarithmique. Voici à quoi ressemble la densité de probabilité de cette distribution (Fig 7.11). C’est une distribution asymétrique qui démarre du quantile zéro et est asymptotique à droite en +infini. Figure 7.11: Un exemple de distribution log-normale. 7.7.2 Snippets Les fonctions relatives à la distribution log-normale dans R sont &lt;x&gt;lnorm(). Le calcul de probabilités se fait à l’aide de plnorm(), les quantiles se déterminent à partir de qlnorm() et un échantillon pseudo-aléatoire se calcule en utilisant rlnorm(). Les snippets relatifs à la loi log-normale dans la SciViews Box sont accessibles à partir du menu (d)istributions: log-normal à partir de .il) : "],
["graphique-quantile-quantile.html", "7.8 Graphique quantile-quantile", " 7.8 Graphique quantile-quantile Il n’est pas toujours facile de déterminer quelle est la loi de distribution qui correspond le mieux à la population étudiée. Par contre, une comparaison est possible entre une distribution observée (sur base d’un échantillon, donc, d’un jeu de données) et une distribution théorique (sur base d’une loi théorique). Nous pouvons calculer les quantiles d’un échantillon via une méthode similaire à celle que nous avons employée pour calculer les quartiles et tracer la boite de dispersion dans le module 4. Un quantile divise des données quantitatives en deux sous-groupes de telle manière que le groupe contenant les observations plus petites que ce quantile représente un effectif équivalent à la fraction considérée. Donc, un quantile 10% correspondra à la valeur qui sépare le jeu de données en 10% des observations les plus petites et 90% des observations les plus grandes. Ce quantile dit observé est comparable au quantile dit théorique que nous pouvons calculer sur base d’une probabilité équivalente à la fraction considérée. Prenons un exemple simple pour fixer les idées. Dans les données relatives au plancton, nous avons 50 œufs allongés mesurés. Nous nous demandons si leur taille mesurée ici par la surface (area) de la particule à l’image suit une distribution log-normale. Dans ce cas, il est plus facile de transformer les données en log et de comparer les valeurs ainsi recalculées à une distribution normale. eggs &lt;- read(&quot;zooplankton&quot;, package = &quot;data.io&quot;) %&gt;.% filter(., class == &quot;Egg_elongated&quot;) %&gt;.% mutate(., log_area = log10(area)) %&gt;.% select(., area, log_area) summary(eggs) # area log_area # Min. :0.4121 Min. :-0.3850 # 1st Qu.:0.4714 1st Qu.:-0.3266 # Median :0.4950 Median :-0.3054 # Mean :0.5100 Mean :-0.2950 # 3rd Qu.:0.5347 3rd Qu.:-0.2719 # Max. :0.6718 Max. :-0.1728 chart(data = eggs, ~ area) + geom_histogram(bins = 12) Sur base de l’histogramme, nous voyons bien que la distribution est soit unimodale et asymétrique, soit bimodale. L’histogramme des données transformées log devrait être plus symétrique si les données originelles suivent bien une distribution log-normale unimodale. chart(data = eggs, ~ log_area) + geom_histogram(bins = 12) C’est légèrement mieux, mais la distribution ne parait pas parfaitement symétrique, voire peut-être encore bimodale (pas flagrant toutefois). L’histogramme est un bon outil pour visualiser globalement une distribution, mais le graphique quantile-quantile offre une représentation plus précise pour comparer précisément deux distributions. Comme nous avons 50 observations à disposition, nous pouvons calculer les quantiles tous les 2% à l’aide de la fonction quantile(). De même, nous pouvons utiliser qnorm() pour calculer les quantiles théoriques selon une distribution normale réduite. Cela donne : (probas &lt;- seq(from = 0.02, to = 0.98, by = 0.02)) # [1] 0.02 0.04 0.06 0.08 0.10 0.12 0.14 0.16 0.18 0.20 0.22 0.24 0.26 0.28 # [15] 0.30 0.32 0.34 0.36 0.38 0.40 0.42 0.44 0.46 0.48 0.50 0.52 0.54 0.56 # [29] 0.58 0.60 0.62 0.64 0.66 0.68 0.70 0.72 0.74 0.76 0.78 0.80 0.82 0.84 # [43] 0.86 0.88 0.90 0.92 0.94 0.96 0.98 # Quantiles observés dans l&#39;échantillon q_obs &lt;- quantile(eggs$log_area, probs = probas) # quantiles theoriques selon la distribution normale réduite q_theo &lt;- qnorm(probas, mean = 0, sd = 1) qq &lt;- tibble(q_obs = q_obs, q_theo = q_theo) qq # # A tibble: 49 x 2 # q_obs q_theo # &lt;dbl&gt; &lt;dbl&gt; # 1 -0.373 -2.05 # 2 -0.367 -1.75 # 3 -0.351 -1.55 # 4 -0.348 -1.41 # 5 -0.346 -1.28 # 6 -0.343 -1.17 # 7 -0.343 -1.08 # 8 -0.336 -0.994 # 9 -0.333 -0.915 # 10 -0.332 -0.842 # # … with 39 more rows Si les deux distributions sont compatibles, nous devrions avoir proportionnalité entre les quantiles théoriques et les quantiles observés. Cela devrait donc se marquer par un alignement des points sur un graphique des quantiles observés en fonction des quantiles théoriques (Fig. 7.12). chart(data = qq, q_obs ~ q_theo) + geom_point() Figure 7.12: Graphique quantile-quantile construit à la main. Cet alignement n’est pas flagrant. Le graphique proposé par la fonction car::qqPlot() (Fig. 7.13) et accessible depuis le snippet .cuqqnorm est le même, mais il ajoute différents éléments qui aident à l’interprétation : Une droite (ligne continue bleue) selon laquelle les points devraient s’aligner en cas de concordance parfaite entre les deux distributions Une enveloppe de confiance (lignes pointillées bleues) qui tient compte de la variabilité aléatoire d’un échantillon à l’autre pour inclure une enveloppe de tolérance avec une fiabilité de 95%. Cela signifie que 95% des points doivent, en principe, se trouver à l’intérieur de l’enveloppe. Une individualisation des points les plus suspects éventuels, en indiquant le numéro de la ligne dans le jeu de donnée de chaque point suspect. car::qqPlot(eggs[[&quot;log_area&quot;]], distribution = &quot;norm&quot;, envelope = 0.95, col = &quot;Black&quot;, ylab = &quot;log(area [mm^2])&quot;) Figure 7.13: Graphique quantile-quantile comparant le log(area) en fonction d’une distribution normale obtenu à l’aide de car::qqPlot(). Interprétation Si quasiment tous les points sont compris dans l’enveloppe de confiance à 95%, le graphique indique que les deux distributions ne sont pas fondamentalement différentes. Ici les points correspondant aux valeurs les plus élevées sortent de l’enveloppe pour un certain nombre d’entre eux d’affilée, et les points 11 et 30 sont considérés comme suspects. Ceci indique que les effectifs observés dans l’échantillon sont plus nombreux en queue droite de distribution que ce que la distribution normale prédit en théorie. Ceci confirme l’impression de distribution asymétrique et/ou bimodale. Il est probable qu’on ait au moins deux types d’œufs allongés différents dans l’échantillon, avec le second type moins nombreux, mais représenté par des œufs plus gros, ce qui enfle la partie droite de la distribution. "],
["chi2.html", "Module 8 Test Chi carré", " Module 8 Test Chi carré Dans ce module, nous entrons dans le monde de l’inférence statistique et des tests d’hypothèses qui nous permettront de répondre à des questions biologiques sur base de données empiriques malgré une incertitude inévitables (hasard de l’échantillonnage, variabilité individuelle, erreurs de mesure, …). Objectifs Appréhender l’inférence statistique Être capable d’effectuer un échantillonnage correctement Comprendre ce qu’est un test d’hypothèse Connaitre la distribution Chi2 et les tests d’hypothèses basés sur cette distribution Développer un regard critique (positif !) sur le travail de ses collègues via l’évaluation d’un rapport par les pairs (initiation au “peer reviewing”) Prérequis Les probabilités et lois de distributions statistiques vues au module 7 doivent être comprises avant d’attaquer cette section. "],
["echantillonnage.html", "8.1 Échantillonnage", " 8.1 Échantillonnage Nous avons déjà abordé cette question dans le chapitre 6. Si nous pouvions mesurer tous les individus d’une population à chaque fois, nous n’aurions pas besoin des statistiques. Mais ce n’est pratiquement jamais possible. Tout d’abord, le nombre d’individus est potentiellement très grand. Le travail nécessaire risque alors d’être démesuré. Afin de limiter les mesures à un nombre raisonnable de cas, nous effectuons un échantillonnage qui consiste à prélever un petit sous-ensemble de taille \\(n\\) donné depuis la population de départ. Il existe différentes stratégies d’échantillonnage, mais nous avons vu que la plus fréquente est l’échantillonnage aléatoire pour lequel : chaque individu dans la population a la même probabilité d’être pris dans l’échantillon, les mesures et les individu sont indépendants les uns des autres. Nous n’avons pas forcément accès à tous les individus d’une population. Dans ce cas, nous devons la limiter à un sous-ensemble raisonnable. Par exemple, il est impossible de mesurer toutes les souris. Par contre, nous pouvons décider d’étudier la ou les souches de souris disponibles dans l’animalerie, ou chez nos fournisseurs. Quoi qu’il en soit, l’échantillon n’est qu’un petit sous-ensemble sélectionné par un mécanisme faisant intervenir le hasard. Donc, deux échantillons de la même population ont un très forte probabilité d’être différents l’un de l’autre. Il en va également des statistiques calculées sur ces échantillons, comme les effectifs observés pour chaque niveau de variables qualitatives ou les valeurs moyennes pour les variables quantitatives, par exemple. Cette variabilité d’un échantillon à l’autre ne nous intéresse pas car elle n’apporte pas d’information sur la population elle-même. Ce qui nous intéresse, c’est d’estimer au mieux les valeurs (effectifs, moyennes, etc.) dans la population. L’estimation de paramètres d’une population par le biais de calculs sur un échantillon représentatif issu de cette population s’appelle l’inférence statistique. Rappelez-vous le schéma qui relie population et échantillon via l’échantillonnage d’une part, et l’inférence d’autre part. Travail préliminaire Avant de vous lancer dans l’inférence statistique, assurez-vous d’avoir effectué soigneusement les trois étapes suivantes : Vous comprenez bien la question posée, en termes biologiques. Vous connaissez ou vous êtes documenté sur l’état de l’art en la matière (bibliographie). Que sait-on déjà du phénomène étudié ? Quels sont les aspects encore inconnus ou à l’état de simples hypothèses ? Vous avez vérifié que la façon dont les mesures ont été prises permettra effectivement de répondre à la question posée. En particulier, vous avez vérifié que l’échantillonnage a été réalisé dans les règles pour qu’il soit représentatif de la population étudiée. En outre, vous cernez clairement quelle est la population effectivement étudiée. C’est important pour éviter plus tard de sur-généraliser les résultats obtenus (les attribuer à une population plus large que celle effectivement étudiée). Vous avez effectué une analyse exploratoire des données. Vous avez représenté les données à l’aide de graphiques appropriés et vous avez interprété ces graphiques afin de comprendre ce que le jeu de données contient. Vous avez également résumé les données sous forme de tableaux synthétiques et vous avez, si nécessaire, remaniés et nettoyés vos données. "],
["test-dhypothese.html", "8.2 Test d’hypothèse", " 8.2 Test d’hypothèse Le test d’hypothèse ou test statistique est l’outil le plus simple pour répondre à une question via l’inférence. Il s’agit ici de réduire la question à sa plus simple expression en la réduisant à deux hypothèses contradictoires (en gros, la réponse à la question est soit “oui”, soit “non” et rien d’autre). L’hypothèse nulle, notée \\(H_0\\) est l’affirmation de base ou de référence que l’on cherchera à réfuter, L’hypothèse alternative, notée \\(H_1\\) ou \\(H_a\\) représente une autre affirmation qui doit nécessairement être vraie si \\(H_0\\) est fausse. Les deux hypothèses ne sont pas symétriques. Notre intention est de rejeter \\(H_0\\). Dans ce cas, nous pourrons considérer que \\(H_1\\) est vraie, avec un certain degré de certitude que nous pourrons également quantifier. Si nous n’y arrivons pas, nous dirons que nous ne pouvons pas rejeter \\(H_0\\), mais nous ne pourrons jamais dire que que nous l’acceptons car dans ce cas, deux explications resteront possibles : (1) \\(H_0\\) est effectivement vraie, ou (2) \\(H_0\\) est fausse mais nous n’avons pas assez de données à disposition pour le démontrer avec le niveau de certitude recherché. A vous de jouer ! Ouvrez RStudio dans votre SciViews Box, puis exécutez l’instruction suivante dans la fenêtre console pour effectuer les exercices d’auto-évaluation en parallèle à la lecture du texte ci-dessous : BioDataScience::run(&quot;08a_chi2&quot;) 8.2.1 Becs croisés des sapins Tout cela reste très abstrait. Prenons un exemple concret simple. Le bec-croisé des sapins Loxia curvirostra Linné 1758 est un passereau qui a la particularité d’avoir un bec dont les deux parties se croisent, ce qui donne un outil particulièrement adapté pour extraire les graines de conifères dont il se nourrit. Bec-croisés des sapins mâles montrant les deux variétés (bec croisé à gauche ou à droite). Photo : Elaine R. Wilson (license CC BY-SA 3.0). Comme des individus à bec croisé à gauche et d’autres à bec croisé à droite se rencontrent dans la même population, Groth (1992) a comptabilisé les deux types dans un échantillon aléatoire et représentatif de plus de 3000 oiseaux. Il a obtenu le tableau suivant : (crossbill &lt;- tibble(cb = c(rep(&quot;left&quot;, 1895), rep(&quot;right&quot;, 1752)))) # # A tibble: 3,647 x 1 # cb # &lt;chr&gt; # 1 left # 2 left # 3 left # 4 left # 5 left # 6 left # 7 left # 8 left # 9 left # 10 left # # … with 3,637 more rows Ce tableau peut être résumé sous la forme d’un tableau de contingence : (crossbill_tab &lt;- table(crossbill$cb)) # # left right # 1895 1752 Les scientifiques pensent que les variétés gauches et droites se rencontrent avec un ratio 1:1 dans la population étudiée suite à une sélection présumée basée sur le rapport des deux variétés. La question se traduit sous forme d’un test d’hypothèse comme ceci (retenez la notation particulière utilisée pour spécifier les hypothèses) : \\(H_0: \\mathrm{P}(left) = \\frac{1}{2}\\ \\mathrm{et}\\ \\mathrm{P}(right) = \\frac{1}{2}\\) \\(H_1: \\mathrm{P}(left) \\neq \\frac{1}{2}\\ \\mathrm{ou}\\ \\mathrm{P}(right) \\neq \\frac{1}{2}\\) Pouvons-nous rejeter \\(H_0\\) ici ? 8.2.2 Test Chi2 univarié Le test Chi2 (ou \\(\\chi^2\\)) de Pearson est un test d’hypothèse qui permet de comparer des effectifs observés notés \\(a_i\\) à des effectifs théoriques \\(\\alpha_i\\) sous l’hypothèse nulle pour les différents niveaux \\(i\\) allant de 1 à \\(n\\) d’une variable qualitative (version dite univariée du test). A noter que par rapport à la définition des hypothèses ci-dessus, ce ne sont pas les probabilités qui sont testées, mais les effectifs. Conditions d’application Tout test d’hypothèse impose des conditions d’application qu’il faudra vérifier avant d’effectuer le test. Pour le test \\(\\chi^2\\), ce sont : échantillonnage aléatoire et observations indépendantes, aucun effectif théorique (ou probabilité) sous \\(H_0\\) nul, aucun effectif observé, si possible, inférieur à 5 (ceci n’est pas une condition stricte ; le test sera “approximativement” bon dans le cas contraire). Ces conditions d’application sont bien rencontrées ici. Réalisation du test Chi2 dans R Dans R, le test du \\(\\chi^2\\) est réalisé facilement à l’aide de la fonction chisq.test(). Voici ce que cela donne et comment on l’interprète : chisq.test(crossbill_tab, p = c(1/2, 1/2), rescale.p = FALSE) # # Chi-squared test for given probabilities # # data: crossbill_tab # X-squared = 5.6071, df = 1, p-value = 0.01789 Le premier argument donné à chisq.test() est le tableau de contingence à une entrée indiquant les effectifs observés, ici crossbill. L’argument p = est la liste des probabilités attendues sous \\(H_0\\) et dont la somme vaut un. On peut aussi donner les effectifs attendus, mais il faut alors préciser rescale.p = TRUE. Ce fragment de code est également disponible dans les snippets à partir du menu hypothesis test : contingency ou .hc (test Chi2 univarié). L’exécution de ce code nous donne un court rapport avec : Un titre qui précise le test d’hypothèse effectué (test \\(\\chi^2\\) avec des probabilités sous \\(H_0\\) fournies via l’argument p =) Un rappel du jeu de données traité (crossbill ici) La dernière ligne qui indique le résultat du test. Les détails et explications concernant cette ligne sont développés ci-dessous. L’interprétation se fait en fonction de la valeur P (p-value = 0.01789). En fonction d’un seuil choisi avant de faire le test, et appelé seuil \\(\\alpha\\). La décision est prise comme suit : Si la valeur P est inférieure à \\(\\alpha\\), nous rejetons l’hypothèse \\(H_0\\), considérée comme trop peu probable, Si la valeur P est supérieur ou égale à \\(\\alpha\\), nous ne rejetons pas \\(H_0\\), et considérons que notre échantillon ne nous permet pas de considérer cette hypothèse nulle comme suffisamment improbable (soit elle est effectivement correcte, soit l’effectif \\(n\\) de notre échantillon est insuffisant pour démontrer qu’elle ne l’est pas au seuil \\(\\alpha\\) choisi). Souvent en biologie, on choisi \\(\\alpha\\) = 5%, mais dans les cas où nous souhaitons avoir plus de “certitude” dans notre réponse, nous pouvons aussi choisir un seuil plus restrictif de 1%, voire de 0,1%. Encore une fois, les explications sont détaillées ci-dessous. Dans notre exemple, nous pouvons donc rejeter \\(H_0\\) et nous dirons que la probabilité d’observer un bec croisé gauche est significativement plus grande qu’un bec croisé droit au seuil \\(\\alpha\\) de 5% dans la population étudiée (test \\(\\chi^2\\) = 5,61, ddl = 1, valeur P = 0,018). A ce stade, notre analyse statistique se termine. Une interprétation biologique du résultat, des hypothèses concernant les mécanismes biologiques que cela implique, une confrontation à ce que d’autres ont observé via la littérature scientifique et des conclusions et/ou perspectives finalisent l’étude. Explications détaillées Voici comment ce test se construit. Notre tableau de contingence à simple entrée crossbill contient nos \\(a_i\\). Nous devons donc calculer quels sont les effectifs théoriques \\(\\alpha_i\\). Le nombre total d’oiseaux observés est : sum(crossbill_tab) # [1] 3647 … et les effectifs attendus sous \\(H_0\\) sont : (alpha_i &lt;- c(left = sum(crossbill_tab)/2, right = sum(crossbill_tab)/2)) # left right # 1823.5 1823.5 Les hypothèses du test \\(\\chi^2\\) univarié se définissent comme ceci : \\(H_0: a_i = \\alpha_i\\) pour tout \\(i\\) \\(H_1: a_i \\neq \\alpha_i\\) pour au moins un \\(i\\) Le principe de la statistique \\(\\chi^2\\) consiste à sommer les écarts au carré par rapport aux \\(\\alpha_i\\) de référence divisés par ces mêmes \\(\\alpha_i\\) pour quantifier l’écart entre les valeurs observées et les valeurs théoriques. Donc : \\[\\chi^2_\\mathrm{obs} = \\sum_{i=1}^n\\frac{(a_i - \\alpha_i)^2}{\\alpha_i}\\] Notez que cette statistique prend la valeur nulle lorsque tous les \\(a_i\\) sont strictement égaux aux \\(\\alpha_i\\). Dans tous les autres cas, des termes positifs (le carré de différences est toujours une valeur positive) apparaissent. Donc la statistique \\(\\chi^2\\) est d’autant plus grande que les observations s’éloignent de la théorie. Calculons \\(\\chi^2_\\mathrm{obs}\\) dans notre cas38 : (chi2_obs &lt;- sum((crossbill_tab - alpha_i)^2 / alpha_i)) # [1] 5.607074 Pour répondre à la question, il nous faut une loi de distribution statistique qui permette d’associer une probabilité au quantile \\(\\chi^2_\\mathrm{obs}\\) sous \\(H_0\\). C’est là que le statisticien Karl Pearson vient à notre secours. Il a, en effet, modélisé la distribution statistique du \\(\\chi^2\\). La loi du même nom admet un seul paramètre, les degrés de libertés (ddl) qui sont égaux au nombre de niveaux de la variable facteur étudiée \\(n\\) moins un. Ici, ddl = 2 - 1 = 1. La Fig. 8.1 représente la densité de probabilité d’une loi \\(\\chi^2\\) typique39. C’est une distribution qui démarre à zéro, passe par un maximum et est asymptotique horizontale à \\(+\\infty\\). Figure 8.1: Allure typique de la densité de probabilité de la distribution Chi2 (ici ddl = 3). 8.2.3 Seuil α du test Le raisonnement du test d’hypothèse pour répondre à notre question est le suivant. Connaissant la densité de probabilité théorique sous \\(H_0\\), nous savons que, plus le \\(\\chi^2_\\mathrm{obs}\\) est grand, moins il est plausible. Nous devons décider d’une limite à partir de laquelle nous considérerons que la valeur observée est suffisamment grande pour que \\(H_0\\) devienne trop peu plausible et nous pourrons alors la rejeter. Cette limite se définit sous la forme d’une probabilité correspondant à une zone ou aire de rejet définie dans la distribution théorique de référence sous \\(H_0\\). Cette limite s’appelle le seuil \\(\\alpha\\) du test. Choix du seuil \\(\\alpha\\) d’un test d’hypothèse. Le seuil \\(\\alpha\\) est choisi avant de réaliser le test. Il est un savant compromis entre le risque de se tromper qui diminue plus \\(\\alpha\\) est petit, et la possibilité d’obtenir le rejet de \\(H_0\\) lorsqu’elle est fausse qui augmentera avec \\(\\alpha\\). Si on veut être absolument certain du résultat, on prend \\(\\alpha = 0\\), mais dans ce cas on ne rejette jamais \\(H_0\\) et on ne tire donc jamais aucune conclusion utile. Donc, nous devons assouplir les règles et accepter un petit risque de se tromper. Généralement, les statisticiens choisissent \\(\\alpha\\) = 5% dans les cas courants, et prennent 1%, ou même 0,1% dans les cas où il faut être plus strict (par exemple, si des vies dépendent du résultat). Nous pouvons nous baser sur ces références, même si nous verrons plus loin que cette pratique est de plus en plus remise en cause dans la littérature scientifique. Poursuivons. Nous choisissons notre seuil \\(\\alpha\\) = 5%. Cela définit l’aire la plus extrême de 5% à droite de la distribution \\(\\chi^2\\) à 1 ddl comme zone de rejet (remplie en rouge sur la Fig. 8.2). Il nous suffit maintenant de voir où se place notre \\(\\chi^2_\\mathrm{obs}\\). S’il se situe dans la zone en rouge, nous rejetterons \\(H_0\\), sinon, nous ne la rejetterons pas. Figure 8.2: Densité de probabilité sous H0 (distribution Chi2 à 1 ddl), zone de rejet de 5% en rouge et position de la valeur observée (trait vertical rouge). Où se situe la limite ? Nous pouvons facilement la calculer : qchisq(0.05, df = 1, lower.tail = FALSE) # [1] 3.841459 Notre \\(\\chi^2_\\mathrm{obs}\\) = 5,61 est plus grand que cette limite à 3,84 et se situe donc dans la zone de rejet de \\(H_0\\) du test. Nous rejetons donc \\(H_0\\) ici. Nous dirons que les becs croisés à gauche sont significativement plus nombreux que ceux à droite au seuil \\(\\alpha\\) de 5% (test \\(\\chi^2\\) = 5,61, ddl = 1, valeur P = 0,018). Notez bien la façon particulière de reporter les résultats d’un test d’hypothèse ! Il nous manque encore juste un élément… qu’est-ce que cette “valeur P” de 0,018 reportée dans le résultat ? En fait, c’est la valeur de probabilité associée au test et correspond ici à l’aire à droite définie depuis le \\(\\chi^2_\\mathrm{obs}\\). Calculons-la : pchisq(5.61, df = 1, lower.tail = FALSE) # [1] 0.01785826 Le test d’hypothèse reporte la valeur P afin qu’un lecteur qui aurait choisi un autre seuil \\(\\alpha\\) pourrait effectuer immédiatement sa propre comparaison sans devoir refaire les calculs. La règle est simple : valeur P &lt; seuil \\(\\alpha\\), \\(=&gt; \\mathrm{R}H_0\\) (on rejette \\(H_0\\)), valeur P ≥ seuil \\(\\alpha\\), \\(=&gt; \\rlap{\\mathrm{R}} \\diagup H_0\\) (on ne rejette pas \\(H_0\\)). 8.2.4 Effet de l’effectif étudié En inférence, la qualité des données (échantillons représentatifs) est importante, mais la quantité aussi. Plus vous pourrez mesurer d’individus, mieux c’est. Par contre, dès que la taille de l’échantillon (ici, l’effectif total mesuré) est suffisant pour rejeter \\(H_0\\), vous n’avez plus besoin d’augmenter la taille de votre échantillon40. Voyons l’effet de la taille de l’échantillon sur l’étude des becs croisés des sapins. Nous n’avons pas besoin d’un effectif plus grand que celui mesuré, car nous rejetons \\(H_0\\) ici. Qu’aurait donné notre test \\(\\chi^2\\), par contre, si l’auteur avait mesuré disons 10 fois moins d’oiseaux, les proportions restant par ailleurs identiques entre becs croisés à gauche et à droite ? # Proportions équivalentes, mais échantillon 10x plus petit (crossbill_tab2 &lt;- as.table(c(left = 190, right = 175))) # left right # 190 175 chisq.test(crossbill_tab2, p = c(1/2, 1/2), rescale.p = FALSE) # # Chi-squared test for given probabilities # # data: crossbill_tab2 # X-squared = 0.61644, df = 1, p-value = 0.4324 Nous constatons que la valeur du \\(\\chi^2_{obs}\\) dépend de l’effectif. Sa valeur est plus petite ici. Par conséquent, la valeur P a également changé et elle vaut à présent 43%. Cette valeur est supérieure maintenant à notre seuil \\(\\alpha\\) de 5%. Donc, nous ne pouvons pas rejeter \\(H_0\\). Dans un pareil cas, nous conclurons que les becs croisés à gauche ne sont pas significativement plus nombreux que ceux à droite au seuil \\(\\alpha\\) de 5% (test \\(\\chi^2\\) = 0,62, ddl = 1, valeur P = 0,43). Notez, c’est important, que nous n’avons pas écrit “ne sont pas”, mais nous avons précisé “ne sont pas significativement” plus nombreux. C’est un détail très important. En effet, cela veut dire que l’on ne peut pas conclure qu’il y ait des différences sur base de l’échantillon utilisé, mais il se peut aussi que l’échantillon ne soit pas suffisamment grand pour mettre en évidence une différence. Or, nous avons analysé en réalité un plus grand échantillon (crossbill), et nous savons bien que c’est effectivement le cas. Est-ce que vous saisissez bien ce que le mot significativement veut dire, et la subtilité qui apparaît lorsqu’un test d’hypothèse ne rejette pas \\(H_0\\) ? Les conclusions tirées avec crossbill et crossbill2 et le même test d’hypothèse sont diamétralement opposées car l’un rejette et l’autre ne rejette pas \\(H_0\\). Pourtant ces deux analyses ne se contredisent pas ! Les deux interprétations sont simultanément correctes. C’est l’interprétation asymétrique du test qui permet cela, et l’adverbe significativement est indispensable pour introduire cette nuance dans le texte ! 8.2.5 Test Chi2 d’indépendance Dans le cas d’un tableau de contingence à double entrée, qui croise les niveaux de deux variables qualitatives, nous pouvons effectuer également un test \\(\\chi^2\\). Celui-ci sera calculé légèrement différemment et surtout, les hypothèses testées sont différentes. A vous de jouer ! Une séance d’exercice vous est proposée en lien avec le test Chi2 d’indépendance. Vous pouvez réaliser ces exercices en parallèle à la lecture de la présente section. Ouvrez RStudio dans votre SciViews Box, puis exécutez l’instruction suivante dans la fenêtre console : BioDataScience::run(&quot;08b_chi2&quot;) Conditions d’application Comme toujours, le test \\(\\chi^2\\) d’indépendance est assorti de conditions d’application que nous devons vérifier avant de considérer d’utiliser ce test : échantillon représentatif (échantillonnage aléatoire et individus indépendants les uns des autres), attribution des traitements aux individus de manière aléatoire, aucun effectif théorique nul, Si possible, aucun effectif observé inférieur à 5 (pas règle stricte, mais voir à utiliser un test exact de Fisher ci-dessous dans la section “pour en savoir plus” en base de page dans ce cas). Example et résolution dans R Prenons le jeu de données concernant le test d’une molécule potentiellement anti-cancéreuse, le timolol : (timolol &lt;- tibble( traitement = c( rep(&quot;timolol&quot;, 160), rep(&quot;placebo&quot;, 147)), patient = c( rep(&quot;sain&quot;, 44), rep(&quot;malade&quot;, 116), rep(&quot;sain&quot;, 19), rep(&quot;malade&quot;, 128)) )) # # A tibble: 307 x 2 # traitement patient # &lt;chr&gt; &lt;chr&gt; # 1 timolol sain # 2 timolol sain # 3 timolol sain # 4 timolol sain # 5 timolol sain # 6 timolol sain # 7 timolol sain # 8 timolol sain # 9 timolol sain # 10 timolol sain # # … with 297 more rows Nous pouvons résumer ce tableau cas par variable en un tableau de contingence à double entrée : (timolol_table &lt;- table(timolol$traitement, timolol$patient)) # # malade sain # placebo 128 19 # timolol 116 44 Nous avons ici un tableau de contingence à double entrée qui répertorie le nombre de cas attribués aléatoirement au traitement avec placebo (somme de la première colonne, soit 128 + 19 = 147 patients) et le nombre de cas qui ont reçu du timolol (116 + 44 = 160), tout autre traitement étant par ailleurs équivalent. Nous avons donc un total général de 307 patients étudiés. Les conditions d’application du test sont rencontrées ici. La répartition dans le tableau selon les ligne est, elle, tributaire des effets respectifs des deux traitements ? La clé ici est de considérer comme \\(H_0\\) un partitionnement des cas équivalent entre les deux traitements. Ceci revient au même que de dire que l’effet d’une variable (le traitement administré) est indépendant de l’effet de l’autre variable (le fait d’être guéri ou non). C’est pour cette raison qu’on parle de test \\(\\chi^2\\) d’indépendance. Les hypothèses sont : \\(H_0:\\) indépendance entre les deux variables \\(H_1:\\) dépendance entre les deux variables Toute la difficulté est de déterminer les \\(\\alpha_i\\), les effectifs qui devraient être observés sous \\(H_0\\). Si le partitionnement était identique selon tous les niveaux des deux variables, nous aurions autant de cas dans chaque cellule du tableau, soit 307/4 = 76,75. Mais n’oublions pas que nous avons attribués plus de patients au traitement timolol qu’au traitement placebo. De même, nous ne contrôlons pas le taux de guérison de la maladie qui n’est d’ailleurs généralement pas d’un patient sur 2. Il faut donc pondérer les effectifs dans les lignes et les colonnes par rapport aux totaux dans les différents niveaux des variables (en colonne pour la variable traitement, en ligne pour la variable patient). Donc, si nous indiçons les lignes avec \\(i = 1 ..m\\) et les colonnes avec \\(j = 1..n\\), nos effectifs théoriques \\(\\alpha_{i,j}\\) sous hypothèse d’indépendance entre les deux variables sont : \\[\\alpha_{i, j} =\\frac{total\\ ligne_i \\times total\\ colonne_j}{total\\ général}\\] Nous pouvons dès lors calculer le \\(\\chi^2_{obs}\\) pratiquement comme d’habitude via : \\[\\chi^2_{obs} = \\sum_{i=1}^m{\\sum_{j=1}^n{\\frac{(a_{i,j} - \\alpha_{i,j})^2}{\\alpha_{i,j}}}}\\] Enfin, nous comparons cette valeur à la distribution théorique de \\(\\chi^2\\) à \\((m - 1) \\times (n - 1)\\) degrés de liberté. Dans le cas d’un tableau 2 par 2, nous avons 1 degré de liberté. Voici le test effectué à l’aide de la fonction chisq.test() suivi de l’affichage des effectifs théoriques. Vous accédez facilement à ce code depuis le snippet Chi2 test (independence) dans le menu hypothesis tests: contingency à partir de .hc. Mais avant toute chose, nous devons choisir le seuil \\(\\alpha\\) avant de réaliser le test. Nous prendrons ici, par exemple 1% puisque l’analyse est effectuée dans un contexte critique (maladie mortelle). (chi2. &lt;- chisq.test(timolol_table)); cat(&quot;Expected frequencies:\\n&quot;); chi2.[[&quot;expected&quot;]] # # Pearson&#39;s Chi-squared test with Yates&#39; continuity correction # # data: timolol_table # X-squared = 9.1046, df = 1, p-value = 0.00255 # Expected frequencies: # # malade sain # placebo 116.8339 30.16612 # timolol 127.1661 32.83388 Interprétation La valeur P de 0,0026 est inférieure au seuil \\(\\alpha\\) choisi de 0,01. Donc, nous rejetons \\(H_0\\). Il n’y a pas indépendance entre les deux variables. Pour voir quels sont les effets de la dépendance entre les variables, nous devons comparer les effectifs théoriques affichés ci-dessus avec les effectifs observés. Dans le cas du placebo, sous \\(H_0\\), nous aurions du obtenir 117 malades contre 30 patients guéris. Or, nous en avons 128 malades et seulement 19 guéris. D’un autre côté, sous \\(H_0\\) nous aurions du observer 127 patients malades et 33 sains avec le timolol. Or, nous en observons 116 malades et 44 sains. Donc, les valeurs observées sont en faveur d’un meilleur effet avec le timolol. Nous pourrons dire : le timolol a un effet positif significatif sur la guérison de la maladie au seuil \\(\\alpha\\) de 1% (\\(\\chi^2\\) d’indépendance = 9,10, ddl = 1, valeur P = 0,0026). Correction de Yates Si nous calculons le \\(\\chi^2_{obs}\\) à la main, nous obtenons : alpha_ij &lt;- chi2.[[&quot;expected&quot;]] # Les a_i,j sont dans timolol_table sum((timolol_table - alpha_ij)^2 / alpha_ij) # [1] 9.978202 Cela donne 9,98. Or notre test renvoie la valeur de 9,10. A quoi est due cette différence ? Lisez bien l’intitulé du test réalisé. Il s’agit de “Pearson’s Chi-squared test with Yates’ continuity correction”. Il s’agit d’une correction introduite par R dans le cas d’un tableau 2 par 2 uniquement et qui tient compte de ce que la distribution sous \\(H_0\\) est estimée à partir des mêmes données que celle utilisées pour le test, ce qui introduit un biais ainsi corrigé. Il est donc déconseillé de désactiver cette correction, même si nous pouvons le faire en indiquant correct = FALSE (ci-dessous, juste pour vérifier notre calcul du \\(\\chi^2_{obs}\\) qui est maintenant identique, 9,98). # Test d&#39;indépendance sans correction de Yates chisq.test(timolol_table, correct = FALSE) # # Pearson&#39;s Chi-squared test # # data: timolol_table # X-squared = 9.9782, df = 1, p-value = 0.001584 8.2.6 Autres tests Chi2 Les test du \\(\\chi^2\\) est également utilisé, dans sa forme univariée, pour comparer les effectifs observés par rapport à des effectifs théoriques suivant un loi de distribution discrète. Ce test s’appelle un test de qualité d’ajustement (“goodness-of-fit test” en anglais). Dans ce cas, le nombre de degrés de liberté est le nombre de catégories moins le nombre de paramètres de la distribution moins un. Pour l’ajustement à une loi de distribution continue, il est possible de découper les données en classes et d’appliquer un test \\(\\chi^2\\) dessus ensuite. Il existe cependant d’autres tests considérés comme plus efficaces dans ce cas, comme le test de Komogorov-Smirnov, notamment avec les corrections introduites par Lillefors. Pour l’ajustement à une distribution normale, des tests spécialisés existent comme le test de Shapiro-Wilk. Ce dernier est disponible depuis les snippets de la SciViews Box dans le menu Hypothesis tests: distribution ou .hd, et puis Shapiro-Wilk test of normality. Gardez toujours à l’esprit que, quelle que soit la qualité d’un test d’ajustement, vous n’aurez jamais qu’une réponse binaire (oui ou non l’échantillon s’ajuste à telle distribution théorique). Les causes de dérive sont innombrables et seules des bonnes représentations graphiques (histogramme, graphe en violon, et surtout, graphique quantile-quantile) sont suffisamment riche en information pour explorer pourquoi et comment la distribution diffère d’une distribution théorique. A vous de jouer ! Employez le test d’hypothèse que vous venez d’apprendre dans votre rapport sur la biométrie humaine : sdd1_biometry-...(nom du groupe) Pour en savoir plus Le test de Chi2 avec R, Le test G est considéré comme une bonne alternative dans certains cas (voir aussi Chi-square vs. G-test). Le test exact de Fisher comme test alternatif, en particulier lorsque les effectifs sont faibles. Faites également le calcul manuellement à la calculatrice pour vérifier que vous avez bien compris.↩ Les fonctions qui permettent les calculs relatifs à la distribution \\(\\chi^2\\) dans R sont &lt;x&gt;chisq(), et les snippets correspondants dans la SciViews Box sont disponibles à partir de .ic. Leur utilisation est similaire à celle des distributions vues au module 7.↩ Attention ! Vous devez fixer la taille de l’échantillon dès le départ a priori. Vous ne pouvez pas accumuler des données jusqu’à obtenir un rejet de \\(H_0\\), sans quoi votre analyse sera biaisée.↩ "],
["metriques-etudiants-de-lumons.html", "8.3 Métriques (étudiants de l’UMONS)", " 8.3 Métriques (étudiants de l’UMONS) En matière de gestion des données, nous avons vu jusqu’ici comment encoder ses données dans un tableau cas par variables, comment importer des données dans R, et comment remanier les tableaux de données et les variables (numériques, tranformation en variable factor, encodage et gestion des valeurs manquantes, etc.) Toutes les variables présentes dans le tableau de départ à l’importation sont dites variables brutes… mais les possibilités ne sont pas seulement limitées à ces variables de départ. En science des données, les variables brutes ne sont pas toujours les plus utiles par rapport aux questions que nous nous posons. Au delà de la simple transformation des données (logarithme, puissance, racine, inverse, …) pour linéariser un nuage de points, nous sommes amenés à élaborer des variables calculées ou métriques qui vont caractériser ou quantifier un aspect particulier présent dans les données. 8.3.1 Morphométrie de crabes Partons d’un exemple concret. Le jeu de données crabs du package MASS rassemble des données relatives à la morphométrie de la carapace d’un crabe. SciViews::R crabs &lt;- read(&quot;crabs&quot;, package = &quot;MASS&quot;, lang = &quot;fr&quot;) L’aide en ligne de ce jeu de données (voir .?crabs) nous indique qu’il s’agit de mesures réalisées sur des crabes de l’espèce Leptograpsus variegatus (Fabricius, 1793) collectés à Freemantle à l’ouest de l’Australie. Deux variétés co-existent (incorrectement libellées species dans le jeu de données) : la variété bleue (B) et la variété orange (O). Crabe Leptograpsus variegatus variété bleue. Photo : Neville Coleman, license CC By 4.0 Museums Victoria. Nous pouvons explorer ce jeu de données en vue de déterminer si des différences morphologiques de la carapace existent entre sexes (variable sex, soit M, soit F) ou entre variétés (variable species, soit B, soit O). Un tableau de contingence à deux entrées peut être obtenu à l’aide de la fonction table() : table(crabs$species, crabs$sex) # # F M # B 50 50 # O 50 50 Un snippet existe pour obtenir quelque chose de similaire (entrer ..., puis exploratory stats puis contingency, puis contingency table - 2 entries). La sortie n’est pas très belle, mais la réalisation d’un table mieux formattée nécessite pour l’instant plus de travail dans R (ne cherchez pas à retenir ce code) : crabs %&gt;.% mutate(., # Changer les labels de species et sex pour des valeurs plus explicites species = fct_recode(species, `**Variété bleue**` = &quot;B&quot;, `**Variété orange**` = &quot;O&quot;), sex = fct_recode(sex, `Femelle` = &quot;F&quot;, Mâle = &quot;M&quot;) ) %&gt;.% with(., table(species, sex)) %&gt;.% # Tableau de contingence à 2 entrées knitr::kable(., caption = &quot;Nombre de crabes mesurés par variété et par sexe.&quot;, align = &quot;c&quot;, escape = FALSE) Tableau 8.1: Nombre de crabes mesurés par variété et par sexe. Femelle Mâle Variété bleue 50 50 Variété orange 50 50 Notre échantillon est bien balancé entre les variétés et les sexes avec 100 individus pour chacun répartis en sous-groupes d’effectifs égaux (n = 50). On parle de plan balancé lorsqu’un échantillonnage stratifié a été réalisé pour s’assurer d’avoir le même nombre d’individus pour chaque niveau d’une ou plusieurs variables qualitatives, quelle que soit la proportion de ces différents niveaux dans la population de départ. C’est une situation optimale pour bien comparer les variétés et/ou les sexes ici. Cinq mesures sont réalisées (toutes exprimées en mm) sur la carapace de ces crabes : front (taille du lobe frontal), rear (largeur à l’arrière), length (longueur), width (largeur à l’endroit le plus large) et depth (épaisseur). Il n’y a pas de valeurs manquantes dans le tableau. Aucune de ces variables morphométriques ne permet de discerner les deux variétés de couleur ou les sexes comme le montrent les cinq graphiques en violon ci-dessous. chart(data = crabs, front ~ species %fill=% sex) + geom_violin(draw_quantiles = c(0.25, 0.5, 0.75), trim = FALSE) chart(data = crabs, rear ~ species %fill=% sex) + geom_violin(draw_quantiles = c(0.25, 0.5, 0.75)) chart(data = crabs, length ~ species %fill=% sex) + geom_violin(draw_quantiles = c(0.25, 0.5, 0.75)) chart(data = crabs, width ~ species %fill=% sex) + geom_violin(draw_quantiles = c(0.25, 0.5, 0.75)) chart(data = crabs, width ~ species %fill=% sex) + geom_violin(draw_quantiles = c(0.25, 0.5, 0.75)) Des tendances générales peuvent être notées. Par exemple, le lobe frontal tend à être légèrement plus grand pour la variété orange, ou la largeur à l’arrière tend à être plus grande pour les femelles, surtout chez la variété orange. Cependant, aucun de ces critères ne peut être retenu pour différencier les variétés ou les espèces pour un individu en particulier car les distributions se chevauchent toutes très largement. Nous pouvons aussi représenter des nuages de points afin de visualiser la variation d’une variable par rapport à une autre. Parmi tous les graphiques réalisables (toutes les combinaisons deux à deux des cinq variables morphologiques), examinons plus en détails les représentations suivantes : chart(data = crabs, rear ~ length %shape=% species %col=% sex) + geom_point() Ce graphique sépare relativement bien les mâles des femelles pour les deux variétés. chart(data = crabs, front ~ width %shape=% species %col=% sex) + geom_point() Ce graphique, en revanche, sépare les deux variétés, quel que soit leur sexe (la variété bleue en bas, et la varété orange en haut). Cela signifie donc que les données morphométriques contiennent une information permettant de discerner les sexes et les variétés, mais cette information n’est pas visible lorsqu’une seule variable quantitative est représentée en fonction des sous-populations comme dans les graphiques en violons. En réalité, les différences de forme sont masquées sur les mesures individuelles par une variation encore plus grande liée à la taille des animaux. Comment pouvons-nous mettre en évidence un facteur de forme en faisant abstraction de la taille ? Pour expliquer cela simplement, considérons un cas facile. Imaginons que nous voulons classer un ensemble de quadrilatères à angles droits. Nous savons tous que ce sont des rectangles. Un cas particulier est le carré avec ses côtés égaux. Tant que nous représentons la longueur ou la largeur de nos quatrilatères à angles droits de toutes tailles, nous ne pouvons pas distinguer les carrés des rectangles. Par contre, si nous calculons le ratio longueur/largeur, nous faisons abstraction de la taille pour quantifier la forme (allongée ou pas). Tous les quadrilatères à angles droits dont le ratio longueur/largeur vaut un est un carré ! C’est exactement le même raisonnement que nous pouvons appliquer à nos données crabs : nous pouvons calculer des variables qui feront abstraction de la taille pour quantifier des facteurs de forme particuliers. Les femelles ayant une carapace plus large à l’arrière proportionnellement à leur taille, le ratio rear/length calculé et nommé rear_length donne ceci : crabs %&gt;.% mutate(., rear_length = rear / length) %&gt;.% # Calcul de rear_length chart(data = ., rear_length ~ species %fill=% sex) + geom_violin(draw_quantiles = c(0.25, 0.5, 0.75)) + ylab(&quot;Ratio largeur arrière/longueur&quot;) Notre métrique rear_length est naturellement ultra-simple. Elle ne permet pas de différencier tous les mâles de toutes les femelles, mais la séparation est déjà bien meilleure qu’en utilisant soit rear soit length seuls. A l’aide de techniques statistiques que vous étudierez au cours de science des données biologiques II l’an prochain, nous pouvons montrer qu’une meilleure métrique (ou indice) pour séparer les mâles des femelles est en réalité : rear / (0.3 * length + 2.4)41. La séparation entre les sexes n’est pas totale, mais s’en rapproche fortement, surtout pour la variété orange. crabs %&gt;.% mutate(., rear_length2 = rear / (0.3 * length + 2.4)) %&gt;.% # Calcul de rear_length2 chart(data = ., rear_length2 ~ species %fill=% sex) + geom_violin(draw_quantiles = c(0.25, 0.5, 0.75)) + ylab(&quot;Ratio largeur arrière/(0.3*longueur + 2.4)&quot;) De même, nous pouvons utiliser l’indice front_width = front / (0.43 * width) pour séparer les variétés qui donne ceci : crabs %&gt;.% mutate(., front_width = front / (0.43 * width)) %&gt;.% # Calcul de front_width chart(data = ., front_width ~ species %fill=% sex) + geom_violin(draw_quantiles = c(0.25, 0.5, 0.75)) + ylab(&quot;Ratio lobe frontal/(0.43*largeur)&quot;) Avec cette nouvelle variable calculée, nous pouvons séparer pratiquement parfaitement les crabes de la variété orange de ceux de la variété bleue sur base uniquement de la forme de la carapace. Admettons que le critère de couleur ne soit pas fiable à 100% avec des individus pouvant arborer des colorations intermédiaires qui rendent la discrimination des variétés sur base uniquement du criètre de couleur hazardeuse. Si c’est le cas, notre indice front_width est très utile pour séparer ces variétés ou en tous cas, pour aider à le faire. Les variables calculées transforment les données brutes pour les rendre plus utilisables dans le cadre d’analyses statistiques. C’est un processus utile et important. Elles permettent de calculer des indices, des ratios, des attributs, … qui mettent en évidence une partie de l’information contenue dans les données de départ, mais mal exprimée au niveau des variables brutes individuelles. Les tranformations de variables (logarithme, puissances, racines, fonction inverse, …) constituent une “famille de calculs” que nous pouvons appliquer pour rendre les données plus facile à traiter, typiquement pour linéariser un nuage de points curvilinéaire, ou pour transformer une distribution log-Normale en distribution Normale. Les métriques sont des variables issues de calculs plus complexes et qui visent à faire émerger une propriété particulièrement intéressante en rapport avec une question que nous nous posons. Bien définir et calculer des métriques est un art complexe, mais c’est aussi la clé d’une bonne analyse. La capacité à définir correctement ses métriques distingue un bon scientifique des données de quelqu’un qui utilise les outils statistiques de manière machinale sans réfléchir suffisamment à ce qu’il fait. Devenez une/une bon(ne) scientifique des données : créez et utilisez des métriques adéquates le plus souvent possible. 8.3.2 Biométrie humaine Le jeu de données sur la biométrie humaine que vous avez vous-mêmes réalisé est un fantastique terrain de jeu pour définir des métriques. La question centrale étant ici d’étudier la question de l’obésité, les métriques les plus importantes sont celles qui permettent de bien quantifier cela. Rappelons nous que nous avons déjà utilisé une métrique avec l’imc et les différentes classes proposées par l’OMS. Prenons le jeu de données biometry du package BioDataScience comme exemple. biometry &lt;- read(&quot;biometry&quot;, package = &quot;BioDataScience&quot;, lang = &quot;FR&quot;) %&gt;.% select(., height, weight) biometry # # A tibble: 395 x 2 # height weight # &lt;dbl&gt; &lt;dbl&gt; # 1 182 69 # 2 190 74 # 3 185 83 # 4 175 60 # 5 167 48 # 6 179 52 # 7 167 72 # 8 180 74 # 9 189 110 # 10 160 82 # # … with 385 more rows L’utilisation d’un nuage de points de la taille en fonction de la masse ne nous permet pas de quantifier l’obésité au sein de notre échantillon comme le montre le graphique ci-dessous : chart(biometry, height ~ weight) + geom_point() L’utilisation de l’IMC comme indice afin de quantifier l’obésité est bien plus intéressant. biometry %&gt;.% mutate(., bmi = weight / (height / 100)^2, bmi_schedule = case_when( bmi &lt; 18.5 ~ &quot;sous-poids&quot;, bmi &gt;= 18.5 &amp; bmi &lt; 25 ~ &quot;poids normal&quot;, bmi &gt;= 25 &amp; bmi &lt; 30 ~ &quot;surpoids&quot;, bmi &gt;= 30 ~ &quot;obèse&quot;), bmi_schedule = factor( bmi_schedule, levels = c(&quot;sous-poids&quot;, &quot;poids normal&quot;, &quot;surpoids&quot;, &quot;obèse&quot;), ordered = TRUE) ) -&gt; biometry biometry # # A tibble: 395 x 4 # height weight bmi bmi_schedule # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt; # 1 182 69 20.8 poids normal # 2 190 74 20.5 poids normal # 3 185 83 24.3 poids normal # 4 175 60 19.6 poids normal # 5 167 48 17.2 sous-poids # 6 179 52 16.2 sous-poids # 7 167 72 25.8 surpoids # 8 180 74 22.8 poids normal # 9 189 110 30.8 obèse # 10 160 82 32.0 obèse # # … with 385 more rows Le graphique en barres de l’imc est plus intéressant que le graphique précendent afin de mettre en avant les individus obèses. chart(biometry, ~ bmi_schedule) + geom_bar() + labs(x = &quot;Echelle de l&#39;IMC&quot;, &quot;Dénombrement&quot;) A vous de jouer ! Dans le projet portant sur le biométrie humaine, réalisez les instructions proposées via le lien suivant : https://github.com/BioDataScience-Course/sdd_lesson/blob/2019-2020/sdd1_08/presentations/indices.md Afin de vous aider dans la recherche d’indices intéressants et pertinents, des documents sont mis à votre disposition via le lien suivant: https://github.com/BioDataScience-Course/sdd_lesson/tree/2019-2020/sdd1_08/biometry_doc_supp Débutez votre recherche d’indice avec la lecture de l’article : Comment mesurer la corpulence et le poids idéal ? Histoire, intérêts et limites de l’indice de masse corporelle Pour le lecteur plus avancé, il s’agit en fait de la droite de régression ajustée dans le nuage de points.↩ "],
["evaluation-par-les-pairs-etudiants-de-charleroi.html", "8.4 Evaluation par les pairs (étudiants de Charleroi)", " 8.4 Evaluation par les pairs (étudiants de Charleroi) En science, l’évaluation par les pairs (“peer-reviewing” en anglais) est le mécanisme le plus efficace pour améliorer la qualité des travaux publiés (articles scientifiques ou ouvrages plus conséquents). Par définition, les résultats publiés en science sont à la frontière de l’inconnu. Il est donc difficile de vérifier si le travail est correct. Les personnes les plus à même de le faire sont les collègues qui travaillent sur le même sujet, ou dans un domaine proche, les “pairs”. Avant d’être rendus publics, les travaux scientifiques font l’objet d’un ou plusieurs rapports par des pairs. Cette phase est la révision de l’article. Le rapport se veut constructif dans le but d’améliorer la qualité du travail. Il ne s’agit pas d’“enfoncer” les auteurs initiaux, mais le “referee” se doit d’être honnête et donc, de mettre le doigt sur les défauts et lacunes du travail également. A vous de jouer ! Vous allez vous initier au travail d’arbitrage (“referee” scientifique) sur base du rapport sur la biométrie de l’oursin violet. Vous partagerez votre dépôt Github avec un de vos collègue. Vous recevrez également un accès au dépôt de quelqu’un d’autre. Votre travail consistera à lire avec un œil critique et constructif le travail que vous recevrez. Vous ajouterez un fichier review.md à la racine du projet où vous consignerez vos remarques générales. Pour les remarques particulières directement dans le rapport, utilisez la balise de citation de Markdown (commencez le paragraphe par &gt;), par exemple : Ceci est un commentaire dans le texte. N’effacer, ni ne modifiez aucun texte directement dans ce rapport. Si vous devez suggérer l’élimination de texte, utilisez la balise Markdown qui sert à biffer ce texte sous forme de deux tildes devant et derrière (~~, ce qui donne texte biffé). Ensuite, effectuez un “commit” de vos commentaires sur le dépôt Github de votre collègue. De votre côté, lorsque vous recevrez le rapport relatif à votre propre projet, lisez les commentaires. Ne modifiez pas le fichier review.md, mais par contre, éditez le texte et éliminez les commentaires directement dans le rapport au fur et à mesure que vous le corriger en tenant compte des remarques. Vous pourrez éventuellement apporter des réponses ou des justifications aux commentaires globaux du fichier review.md. Les intructions sont détaillées ici : https://github.com/BioDataScience-Course/sdd_lesson/blob/master/sdd1_08/presentations/correction.md "],
["moyenne.html", "Module 9 Moyenne", " Module 9 Moyenne Objectifs De manière générale, pouvoir répondre à différentes questions concernant une ou deux moyennes Découvrir la distribution t de Student Comprendre le principe de la distribution d’un échantillon Appréhender l’intervalle de confiance, savoir le calculer et l’utiliser Comprendre les différentes variantes du test t de Student et être capable de l’utiliser pour résoudre des questions pratiques en biologie Connaître également le test de Wilcoxon-Mann-Withney, et pouvoir déterminer quand l’utiliser à la place du test de Student Prérequis Ce module est élaboré sur les notions vues au module 7 concernant les lois de distribution statistiques et sur le concept de test d’hypothèse abordé dans le module 8. Ces deux précédents modules doivent donc être maîtrisés avant d’aller plus avant ici. "],
["une-histoire-de-biere.html", "9.1 Une histoire de bière…", " 9.1 Une histoire de bière… Les belges, c’est connu, apprécient la bière. Mais ils ne sont pas les seuls, et c’est très heureux ! Car c’est en effet grâce à un certain William Sealy Gosset, brasseur et statisticien (et oui, ça ne s’invente pas) que l’un des tests d’hypothèses des plus utilisés en biologie a vu le jour : le test de “Student” qui permet de comparer des moyennes. Pour la petite histoire, Gosset a travaillé pour une certaine brasserie irlandaise du nom de Guiness au début du 20èmesiècle. C’est en étudiant la variabilité de sa bière d’un cru à l’autre que Gosset a découvert la façon dont la moyenne d’un échantillon se distribue. Il a pu dériver une formulation mathématique de cette distribution, la distribution t de Student, et à partir de là, nous verrons que de nombreuses applications en découlent. Nous pourrons, par exemple, dire si deux moyennes diffèrent significativement l’une de l’autre ou pas. Mais au fait, pourquoi, cette distribution porte-t-elle le nom de “Student” ? Visionnez la vidéo suivante (malheureusement en anglais) pour le découvrir42. Le contrat que Gosset a signé avec son employeur l’empêchait de publier des résultats scientifiques sous son vrai nom. Ainsi, il décida de publier sa trouvaille qui occupe aujourd’hui une place très importante en statistiques sous le pseudonyme de “Student” (l’étudiant). Ce n’est qu’à sa mort, en 1937, que l’on pu révéler le nom de l’auteur qui est derrière cette fantastique trouvaille. Mais au fait, de quoi s’agit-il exactement ? Nous allons le découvrir dans la section suivante. Vous pouvez activer les sous-titres en anglais via la barre de boutons en bas de la vidéo pour vous aider à comprendre l’histoire.↩ "],
["distribution-dechantillonnage.html", "9.2 Distribution d’échantillonnage", " 9.2 Distribution d’échantillonnage A vous de jouer ! Afin d’appliquer directement les concepts vu dans ce module, ouvrez RStudio dans votre SciViews Box, puis exécutez l’instruction suivante dans la fenêtre console : BioDataScience::run(&quot;09a_ttest&quot;) Pour rappel, nous faisons de l’inférence sur base d’un échantillon parce que nous sommes incapables de mesurer tous les individus d’une population. Il faut au préalable que l’échantillon soit représentatif, donc réalisé dans les règles de l’art (par exemple, un échantillonnage aléatoire simple de la population). Nous pouvons calculer la moyenne d’un échantillon facilement (eq. (9.1)). \\[\\begin{equation} \\bar{x}=\\sum_{i=1}^n{\\frac{x_i}{n}} \\tag{9.1} \\end{equation}\\] où \\(x\\) est une variable quantitative (donc numeric dans R) et \\(n\\) est la taille de l’échantillon, donc le nombre d’individus mesurés. On notera \\(\\bar{x}\\) la moyenne de \\(x\\), que l’on prononcera “x barre”. Nous utiliserons également l’écart type, noté \\(\\sigma_x\\) pour la population et \\(s_x\\) pour l’échantillon qui se calcule sur base de la somme des écarts à la moyenne au carré (eq. (9.2)) : \\[\\begin{equation} s_x = \\sqrt{\\sum_{i=1}^n{\\frac{(x_i - \\bar{x})^2}{n-1}}} \\tag{9.2} \\end{equation}\\] A noter que \\(s^2\\) est également appelée la variance43. En fait, ce qui nous intéresse, ce n’est pas vraiment la moyenne de l’échantillon, mais celle de la population que l’on notera \\(\\mu\\)44. D’où la question : comment varie la moyenne d’un échantillon à l’autre ? Nous pouvons répondre à cette question de manière empirique en utilisant le générateur pseudo-aléatoire de R. Partons d’une distribution théorique de la population qui soit normale, de moyenne \\(\\mu\\) = 8 et d’écart type \\(\\sigma\\) = 2. Nous pouvons échantillonner neuf individus. Cela donne : set.seed(8431641) smpl1 &lt;- rnorm(9, mean = 8, sd = 2) smpl1 # [1] 9.138562 8.496824 9.573743 7.276562 8.300520 5.176688 4.209415 # [8] 10.700260 7.264703 mean(smpl1) # [1] 7.793031 Dans ce cas-ci, nous obtenons une moyenne de 7,8. Ce n’est pas égal à 8. Le hasard de l’échantillonnage en est responsable. La moyenne de l’échantillon tendra vers la moyenne de la population seulement lorsque \\(n \\longrightarrow \\infty\\). Réalisons un second échantillonnage fictif. mean(rnorm(9, mean = 8, sd = 2)) # [1] 8.660309 Cette fois-ci, nous obtenons une moyenne de 8,7. Nous savons que la moyenne \\(\\mu\\) qui nous intéresse est très probablement différente de la moyenne de notre échantillon, mais de combien ? Pour le déterminer, nous devons définir comment la moyenne de l’échantillon varie d’un échantillon à l’autre, c’est ce qu’on appelle la distribution d’échantillonnage. Nous pouvons le déterminer expérimentalement en échantillonnant un grand nombre de fois. On appelle cela une méta-expérience. En pratique, c’est difficile à faire, mais avec notre ordinateur et le générateur de nombres pseudo-aléatoires de R, pas de problèmes. Donc, comment se distribue la moyenne entre, … disons dix mille échantillons différents de neufs individus tirés de la même population45 ? means_n9 &lt;- numeric(10000) # Vecteur de 10000 valeurs for (i in 1:10000) means_n9[i] &lt;- mean(rnorm(9, mean = 8, sd = 2)) chart(data = NULL, ~ means_n9) + geom_histogram(bins = 30) Nous obtenons une distribution symétrique centrée autour de 8. Elle ressemble à une distribution normale, mais ce n’en est pas une. C’est précisément ici que William Gosset intervient. Il est, en effet, arrivé à décrire cette loi de distribution de la moyenne d’échantillonnage. C’est la distribution t de Student qui admet trois paramètres : une moyenne \\(\\mu_x\\), un écart type \\(\\sigma_x\\), et des degrés de liberté ddl ou \\(\\nu\\). Les degrés de liberté sont en lien avec la taille de l’échantillon. Ils valent : \\[ddl = n-1\\] Concernant la moyenne, et l’écart type, nous pouvons les calculer sur base de notre distribution d’échantillonnage empirique contenue dans le vecteur means : mean(means_n9) # [1] 8.007725 sd(means_n9) # [1] 0.6611474 La moyenne de la distribution d’échantillonnage est donc égale à la moyenne de la population. Elle peut donc être approximée par la moyenne d’un échantillon. Quant à l’écart type, il vaut 2/3 environ, soit l’écart type de la population divisé par 3. Effectuons une autre méta-expérience toujours à partir de la même population, mais avec des échantillons plus petits, par exemple, avec \\(n = 4\\) : means_n4 &lt;- numeric(10000) # Vecteur de 10000 valeurs for (i in 1:10000) means_n4[i] &lt;- mean(rnorm(4, mean = 8, sd = 2)) chart(data = NULL, ~ means_n4) + geom_histogram(bins = 30) La distribution est plus étalée. Ses paramètres sont : mean(means_n4) # [1] 7.995668 sd(means_n4) # [1] 1.002102 La moyenne vaut toujours 8, mais cette fois-ci, l’écart type est plus grand, et il vaut 1, soit 2/2. Qu’est-ce que cela donne avec un échantillon nettement plus grand, disons \\(n = 100\\) ? means_n100 &lt;- numeric(10000) # Vecteur de 10000 valeurs for (i in 1:10000) means_n100[i] &lt;- mean(rnorm(100, mean = 8, sd = 2)) chart(data = NULL, ~ means_n100) + geom_histogram(bins = 30) mean(means_n100) # [1] 7.999136 sd(means_n100) # [1] 0.2005426 On obtient toujours 8 comme moyenne, mais cette fois-ci, l’écart type est de 0,2, soit 2/10. Pouvez-vous deviner comment l’écart type de la distribution t de Student varie sur base de ces trois méta-expériences ? Réfléchissez un petit peu avant de lire la suite. La première bonne nouvelle, c’est que la moyenne des moyennes des échantillons vaut \\(\\mu_x = \\mu\\), la moyenne de la population que nous recherchons. La seconde bonne nouvelle, c’est que la distribution des moyennes des échantillons est plus resserrée que la distribution d’origine. En fait, son écart type dépend à la fois de l’écart type de la population de départ et de \\(n\\), la taille de l’échantillon. Elle varie, en fait, comme \\(\\sigma_x = \\frac{\\sigma}{\\sqrt{n}}\\). Ainsi, avec \\(n = 9\\) nous obtenions \\(\\sigma_x = \\frac{2}{\\sqrt{9}} = \\frac{2}{3}\\) ; avec \\(n = 4\\), nous avions \\(\\sigma_x = \\frac{2}{\\sqrt{4}} = \\frac{2}{2}\\) ; enfin, avec \\(n = 100\\), nous observions \\(\\sigma_x = \\frac{2}{\\sqrt{100}} = \\frac{2}{10}\\). 9.2.1 Loi de distribution de Student On dira : \\[\\mu_x \\sim t(\\mu, \\frac{\\sigma}{\\sqrt{n}}, n-1)\\] La moyenne de l’échantillon suit une distribution t de Student avec pour moyenne, la moyenne de la population, pour écart type, l’écart type de la population divisé par la racine carrée de n, et comme degrés de liberté n moins un. La distribution t de Student dans R est représentée par &lt;x&gt;t(). Donc, qt() calcule un quantile à partir d’une probabilité, pt() une probabilité à partir d’un quantile, rt() renvoie un ou plusieurs nombres pseudo-aléatoires selon une distribution t, et dt() renvoie la densité de probabilité de la distribution. Dans la SciViews Box, vous y accédez également via les “snippets” à partie de .it pour (d)istribution: t (Student) : Le calcul est un peu plus complexe car les fonctions &lt;x&gt;t() ne considèrent que les distributions t de Student réduites (donc avec moyenne valant zéro et écart type de un). Nous devons ruser pour transformer le résultat en fonction des valeurs désirées. Mais heureusement, les “snippets” nous aident en nous prémâchant la besogne. Considérons le cas \\(n = 9\\) avec un moyenne de 8 et un écart type de 2/3. Voici quelques exemples de calculs réalisables : Quelle est la probabilité que la moyenne d’un échantillon soit égale ou supérieure à 8,5 ? .mu &lt;- 8; .s &lt;- 2/3; pt((8.5 - .mu)/.s, df = 8, lower.tail = FALSE) # [1] 0.2373656 Elle est de 24% environ. Notez que nous avons renseigné la moyenne et l’écart type de la distribution t dans .mu et .s, respectivement. Ensuite, les degrés de liberté (9 - 1) sont indiqués dans l’argument df =. Enfin, nous avons précisé lower.tail = FALSE pour obtenir l’aire à droite dans la distribution. Considérant une aire à gauche de 5%, quelle est la moyenne de l’échantillon qui la délimite ? .mu &lt;- 8; .s &lt;- 2/3; .mu + .s * qt(0.05, df = 8, lower.tail = TRUE) # [1] 6.760301 Il s’agit du quantile 6,76. Le graphique correspondant est le suivant : Figure 9.1: Une distribution de Student avec aire à gauche de 5% mise en évidence en rouge. La distribution normale équivalente est superposée en bleu clair. Nous pouvons voir sur la Fig. 9.1 que la distribution t de Student est plus resserrée en son centre, mais plus étalée aux extrémités que la distribution normale de même moyenne et écart type. Néanmoins, elle est d’autant plus proche d’une normale que les degrés de libertés sont grands. On dit qu’elle converge vers une normale lorsque \\(dll \\longrightarrow \\infty\\). En pratique, pour des degrés de liberté égaux ou supérieurs à 30, nous pourrons considérer que les deux distributions sont pratiquement confondues. Revenons à nos calculs de quantiles et probabilités. Les questions que l’on se posera seront plutôt : Quelle est la probabilité que la moyenne d’un échantillon diffère de 0,5 unités de la vraie valeur ? Au lieu de considérer l’aire à gauche ou à droite, on considèrera une aire répartie symétriquement à moitié à gauche et à moitié à droite. La réponse à la question est : # Aire à gauche de 8 -0.5 : .mu &lt;- 8; .s &lt;- 2/3 (left_area &lt;- pt((7.5 - .mu)/.s, df = 8, lower.tail = TRUE)) # [1] 0.2373656 # Aire à droite de 8 + 0.5 : (right_area &lt;- pt((8.5 - .mu)/.s, df = 8, lower.tail = FALSE)) # [1] 0.2373656 # Résultat final left_area + right_area # [1] 0.4747312 Vous avez remarqué quelque chose de particulier ? Oui, les deux aires sont identiques. C’est parce que la distribution est symétrique. On peut donc simplifier le calcul en calculant d’un seul côté et en multipliant le résultat par deux : .mu &lt;- 8; .s &lt;- 2/3 pt((7.5 - .mu)/.s, df = 8, lower.tail = TRUE) * 2 # [1] 0.4747312 Dans l’autre sens, il suffit donc de diviser la probabilité (= l’aire) par deux, parce qu’elle se répartit à parts égales à gauche et à droite dans les régions les plus extrêmes de la distribution. Ainsi, les quantiles qui définissent une aire extrême de 5% dans notre distribution sont (notez que la valeur de probabilité utilisée ici est 0,025, soit 2,5%) : # Quantile à gauche .mu &lt;- 8; .s &lt;- 2/3; .mu + .s * qt(0.025, df = 8, lower.tail = TRUE) # [1] 6.462664 # Quantile à droite .mu &lt;- 8; .s &lt;- 2/3; .mu + .s * qt(0.025, df = 8, lower.tail = FALSE) # [1] 9.537336 On pourra aussi dire que la moyenne d’un échantillon de neuf observations issu de notre population théorique de référence sera comprise entre 6,5 et 9,5 (ou 8 ± 1,5) dans 95% des cas. La Fig. 9.2 le montre graphiquement. Figure 9.2: Une distribution de Student avec aire extrême de 5% mise en évidence en rouge. 9.2.2 Intervalle de confiance Le dernier exemple que nous venons de calculer (Fig. 9.2) n’est rien d’autre que l’intervalle de confiance à 95% de la moyenne. Un intervalle de confiance à x% autour d’une valeur estimée définit une zone à gauche et à droite de la valeur estimée telle que la vraie valeur se situe x% du temps dans cet intervalle. En fait, la distribution est centrée sur \\(\\mu\\), la valeur inconnue que l’on recherche, mais l’intervalle peut être translaté sur l’axe pour se centrer sur la moyenne \\(\\bar{x}\\) d’un échantillon en particulier. Il définit alors une région sur l’axe qui comprend avec une probabilité correspondante, \\(\\mu\\) la moyenne inconnue. Avec ce nouvel outil, nous pouvons donc préciser nos estimations de la moyenne de la population \\(\\mu\\) en associant à la valeur estimée via la moyenne de l’échantillon \\(\\bar{x}\\) un intervalle de confiance. Si nous notons \\(t_p^{n-1}\\) le quantile correspondant à l’aire à gauche valant p pour une distribution t réduite de \\(n-1\\) degrés de liberté, on pourra écrire : \\[\\mathrm{IC}(1 - \\alpha) = \\mu_x \\pm t_{\\alpha/2}^{n-1} \\cdot \\sigma_x\\] On notera aussi \\(\\hat{\\mu}\\) ou “mu chapeau” comme l’estimateur de \\(\\mu\\), c’est-à-dire, la valeur que nous utilisons pour l’approximer au mieux. Ici, il s’agit de \\(\\bar{x}\\), la moyenne de notre échantillon. De même, \\(\\hat{\\sigma}\\) est l’estimateur de l’écart type de la population. La valeur que nous avons à disposition est \\(s_x\\), l’écart type de notre échantillon. Nous pourrons aussi écrire : \\[\\mathrm{IC}(1 - \\alpha) \\simeq \\hat{\\mu} \\pm t_{\\alpha/2}^{n-1} \\cdot \\frac{\\hat{\\sigma}}{\\sqrt{n}}\\] … et en remplaçant les estimateurs : \\[\\mathrm{IC}(1 - \\alpha) \\simeq \\bar{x} \\pm t_{\\alpha/2}^{n-1} \\cdot \\frac{s_x}{\\sqrt{n}}\\] Etant donné l’importance que revet \\(\\frac{s_x}{\\sqrt{n}}\\), nous appelerons cette quantité erreur standard de x et nous la noterons \\(SE_x\\). Nous pouvons tout aussi bien écrire plus simplement : \\[\\mathrm{IC}(1 - \\alpha) \\simeq \\bar{x} \\pm t_{\\alpha/2}^{n-1} \\cdot SE_x\\] Ce qui est intéressant avec ces deux dernières formulations, c’est que l’IC est calculable sur base de notre échantillon uniquement. Analogie avec l’homme invisible qui promène son chien. Si vous avez des difficultés à comprendre l’IC, imaginez plutôt que vous recherchez l’homme invisible (c’est \\(\\mu\\)). Vous ne savez pas où il est, mais vous savez qu’il promène son chien en laisse. Or, le chien est visible (c’est \\(\\bar{x}\\) la moyenne de l’échantillon). La laisse est également invisible, mais vous connaissez sa longueur maximale (c’est votre IC). Donc, vous pouvez dire, voyant le chien que l’homme invisible est à distance maximale d’une longueur de laisse du chien. Valeur α Quel est l’impact du choix de \\(\\alpha\\) sur le calcul de l’IC ? Plus \\(\\alpha\\) sera petit, plus le risque de se tromper sera faible. Cela peut paraître intéressant, donc, de réduire \\(\\alpha\\) le plus possible. Mais alors, la longueur de l’IC augmente. Si nous poussons à l’extrême, pour \\(\\alpha\\) = 0%, nous aurons toujours un IC compris entre \\(-\\infty\\) et \\(+\\infty\\). Et cela, nous en sommes certains à 100% ! Trivial, non? Et pas très utile. Comme pour tout en statistique, nous devons accepter un certain risque de nous tromper si nous voulons obtenir des résultats utilisables. Plus ce risque est grand, plus la réponse est précise (ici, plus l’IC sera petit, voir Fig. 9.3), mais plus le risque de se tromper augmente. On cherchera alors un compromis qui se matérialise souvent par le choix de \\(\\alpha\\) = 5%. Nous nous tromperons une fois sur vingt, et nous aurons un IC généralement raisonnable pour ce prix. Naturellement, rien ne vous oblige à utiliser 5%. Vous pouvez aussi choisir 1% ou 0,1% si vous voulez limiter les risques. Figure 9.3: Une distribution de Student avec comparaison de l’IC 95% (entre les aires en rouge) et l’IC 90% (entre les aires en orange). 9.2.3 Théorème central limite (encore) Jusqu’ici, nous avons considéré une population au départ qui a une distribution normale, mais rien ne dit que ce soit le cas. Que se passe-t-il lorsque la distribution est différentes ? Ici encore, nous pouvons effectuer une méta-expérience. Considérons, par exemple, une distribution uniforme de même moyenne = 8 et écart type = 2. Sachant que l’écart type d’une distribution uniforme vaut \\(\\frac{max - min}{\\sqrt{12}}\\), voir ici, l’intervalle est de : \\(2 \\cdot \\sqrt{12} = 6,928\\). Nous avons donc : (xmin &lt;- 8 - sqrt(12)) # [1] 4.535898 (xmax &lt;- 8 + sqrt(12)) # [1] 11.4641 Vérification : sd(runif(10000, min = xmin, max = xmax)) # [1] 1.986923 Quelle est la distribution de la moyenne d’échantillonnage lorsque \\(n\\) = 4 ? set.seed(678336) m_unif_n4 &lt;- numeric(10000) # Vecteur de 10000 valeurs for (i in 1:10000) m_unif_n4[i] &lt;- mean(runif(4, min = xmin, max = xmax)) # Distribution de Student correspondante pour comparaison .mu &lt;- 8; .s &lt;- 2/2; .df &lt;- 3 # .mu, .s (sigma) and .df .x &lt;- seq(-4.5*.s + .mu, 4.5*.s + .mu, l = 1000) # Quantiles .d &lt;- function(x) dt((x - .mu)/.s, df = .df)/.s # Distribution function chart(data = NULL, ~ m_unif_n4) + geom_histogram(bins = 30) + geom_line(aes(x = .x, y = .d(.x) * 3000)) Cette distribution n’est pas une Student. Par contre, elle y ressemble plus qu’à la distribution uniforme de départ. Avec \\(n\\) = 9 elle s’en rapproche très, très fort, et pour \\(n\\) = 100, nous avons une t de Student parfaite. Figure 9.4: Distribution d’échantillonnage à partir d’une distribution uniforme, n = 9. Ajustement d’une distribution de Student équivalente par dessus l’histogramme. Figure 9.5: Distribution d’échantillonnage à partir d’une distribution uniforme, n = 100. Ajustement d’une distribution de Student équivalente par dessus l’histogramme. Nous venons de montrer de manière empirique que lorsque la distribution de la population est différente d’une distribution normale, la distribution d’échantillonnage tend vers une t de Student pour un \\(n\\) grand. Ceci se démontre de manière mathématique par le fameux théorème central limite que nous avons déjà abordé et qui est si cher aux statisticiens (nous vous épargnons cette démonstration ici). Conditions de validité de l’IC L’IC sera pertinent si : l’échantillon est représentatif (par exemple, échantillonnage aléatoire), les observations au sein de l’échantillon sont indépendantes les unes des autres, la distribution de la population… est normale, alors l’IC basé sur la distribution t de Student sera exact, est approximativement normale, l’IC sera approximativement exact, est non normale, l’IC sera approximativement exact si \\(n\\) est grand. L’équation proposée est, en fait, valable pour un échantillon, et est calculé comme tel par R à l’aide des fonctions sd() pour l’écart type ou var() pour la variance. Pour la population ou pour un échantillon de taille très grande, voire infinie, nous pourrions plutôt diviser par \\(n\\) au lieu de \\(n - 1\\), … mais puisque \\(n\\) est très grand, cela ne change pas grand chose au final.↩ Notez que les lettres latines sont utilisées pour se référer aux variables et aux descripteurs statistiques telle que la moyenne pour l’échantillon, alors que les paramètres équivalents de la population, qui sont inconnus, sont représentés par des lettres grecques en statistiques.↩ Nous utilisons pour se faire une boucle for dans R qui réitère un calcul sur chaque élément d’un vecteur, ici, une séquence 1, 2, 3, …, 10000 obtenue à l’aide de l’instruction 1:10000.↩ "],
["test-t-de-student.html", "9.3 Test t de Student", " 9.3 Test t de Student Nous allons également pouvoir utiliser la distribution t de Student comme distribution de référence pour comparer une moyenne par rapport à une valeur cible ou pour comparer deux moyennes. C’est le test t de Student… ou plutôt les tests de Student puisqu’il en existe plusieurs variantes. Partons d’un exemple concret. Imaginez que vous êtes des biologistes ouest-australiens travaillant à Freemantle. Vous y étudiez le crabe Leptographus variegatus (Fabricius, 1793). C’est un crabe qui peut se trouver en populations abondantes sur les côtes rocheuses fortement battues. Il a un régime alimentaire partiellement détritivore et partiellement carnivore. Crabe Leptograpsus variegatus par Johnragla. Ce crabe est rapide et difficile à capturer… mais vous avez quand même réussi à en attraper et mesurer 200 d’entre eux, ce qui constitue un échantillon de taille raisonnable. Comme deux variétés co-existent, la variété bleue (B) et la variété orange (O) sur votre site d’étude, vous vous demandez si elles diffèrent d’un point de vue morphométrique. Naturellement, nous pouvons également supposer des différences entre mâles et femelles. Vous avez donc décidé de réaliser un échantillonnage stratifié consistant à capturer et mesurer autant de bleus que d’oranges et autant de mâles que de femelles. Vous avez donc 50 mâles bleus, 50 femelles bleues, 50 mâles oranges et 50 femelles oranges. crabs &lt;- read(&quot;crabs&quot;, package = &quot;MASS&quot;, lang = &quot;fr&quot;) skimr::skim(crabs) # Skim summary statistics # n obs: 200 # n variables: 8 # # ── Variable type:factor ───────────────────────────────────────────────────────────────────────────────────────────────────── # variable missing complete n n_unique top_counts ordered # sex 0 200 200 2 F: 100, M: 100, NA: 0 FALSE # species 0 200 200 2 B: 100, O: 100, NA: 0 FALSE # # ── Variable type:integer ──────────────────────────────────────────────────────────────────────────────────────────────────── # variable missing complete n mean sd p0 p25 p50 p75 p100 hist # index 0 200 200 25.5 14.47 1 13 25.5 38 50 ▇▇▇▇▇▇▇▇ # # ── Variable type:numeric ──────────────────────────────────────────────────────────────────────────────────────────────────── # variable missing complete n mean sd p0 p25 p50 p75 p100 # depth 0 200 200 14.03 3.42 6.1 11.4 13.9 16.6 21.6 # front 0 200 200 15.58 3.5 7.2 12.9 15.55 18.05 23.1 # length 0 200 200 32.11 7.12 14.7 27.28 32.1 37.23 47.6 # rear 0 200 200 12.74 2.57 6.5 11 12.8 14.3 20.2 # width 0 200 200 36.41 7.87 17.1 31.5 36.8 42 54.6 # hist # ▂▃▆▇▇▆▅▂ # ▂▃▇▇▇▇▅▃ # ▁▃▅▇▇▆▅▂ # ▂▃▇▆▇▃▂▁ # ▁▃▅▇▇▆▃▁ Toutes les variables qualtitatives sont des mesures effectuées sur la carapace des crabes. Nous nous posons la question suivante : Les femelles ont-elle une carapace plus large à l’arrière, en moyenne que les mâles ? Voici une comparaison graphique : chart(data = crabs, rear ~ sex) + geom_boxplot() Sur le graphique, il semble que les femelles (sex == &quot;F&quot;) tendent à avoir une carapace plus large à l’arrière -variable rear- que les mâles (sex == &quot;M&quot;), mais cette différence est-elle significative ou est-elle juste liée au hasard de l’échantillonnage ? Pour y répondre, nous devons élaborer un test d’hypothèse qui confrontera les hypothèses suivantes (en se basant sur les moyennes) : \\(H_0: \\overline{rear_F} = \\overline{rear_M}\\) \\(H_1: \\overline{rear_F} \\neq \\overline{rear_M}\\) Ici, nous n’avons aucune idée a priori pour \\(H_1\\) si les femelles sont sensées avoir une carapace plus large ou non que les mâles à l’arrière. Donc, nous considérons qu’elle peut être aussi bien plus grande que plus petite. On parle ici de test bilatéral car la différence peut apparaître des deux côtés. Pour ce test, nous pouvons partir de la notion d’intervalle de confiance et de notre idée de calculer les quantiles de part et d’autre de la distribution théorique à parts égales, comme dans la Fig. 9.2. Une idée serait de calculer \\(\\overline{rear_F} - \\overline{rear_M}\\), la différence des moyennes entre mesures pour les femelles et pour les mâles. Les hypothèses deviennent alors : \\(H_0: \\overline{rear_F} - \\overline{rear_M} = 0\\) \\(H_1: \\overline{rear_F} - \\overline{rear_M} \\neq 0\\) Appelons cette différence \\(\\Delta rear\\). Nous pouvons définir un intervalle de confiance pour \\(\\Delta rear\\) si nous pouvons calculer la valeur t ainsi que l’erreur standard \\(SE_{\\Delta rear}\\) associées à cette variables calculée. Après avoir interrogé des statisticiens chevronnés, ceux-ci nous proposent l’équation suivante pour \\(SE_{\\Delta rear}\\) (avec \\(n_F\\) le nombre de femelles et \\(n_M\\) le nombre de mâles) : \\[SE_{\\Delta rear} = \\sqrt{SE_{rear_F}^2 + SE_{rear_M}^2} = \\sqrt{\\frac{s_{rear_F}^2}{n_F} + \\frac{s_{rear_M}^2}{n_M}}\\] Il nous reste à déterminer les degrés de liberté associés à la distribution t. Les statisticiens nous disent qu’il s’agit de n moins deux degrés de libertés. Nous obtenons alors l’équation suivante pour l’intervalle de confiance : \\[\\mathrm{IC}(1 - \\alpha)_{\\Delta rear} \\simeq \\Delta rear \\pm t_{\\alpha/2}^{n-2} \\cdot SE_{\\Delta rear}\\] Dans notre cas, cela donne : crabs %&gt;.% group_by(., sex) %&gt;.% summarise(., mean = mean(rear), var = var(rear), n = n()) -&gt; crabs_stats crabs_stats # # A tibble: 2 x 4 # sex mean var n # &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; # 1 F 13.5 7.51 100 # 2 M 12.0 4.67 100 # Calcul de Delta rear et de son intervalle de confiance à 95% (delta_rear &lt;- crabs_stats$mean[1] - crabs_stats$mean[2]) # [1] 1.497 (t &lt;- qt(0.025, nrow(crabs) - 2)) # [1] -1.972017 (se &lt;- sqrt(crabs_stats$var[1] / crabs_stats$n[1] + crabs_stats$var[2] / crabs_stats$n[2])) # [1] 0.3489874 (ic_95 &lt;- c(delta_rear + t * se, delta_rear - t * se)) # [1] 0.8087907 2.1852093 Un premier raisonnement consiste à dire que si la valeur attendue sous \\(H_0\\) est comprise dans l’intervalle de confiance, nous ne pouvons pas rejetter l’hypothèse nulle, puisqu’elle représente une des valeurs plausibles à l’intérieur l’IC. Dans le cas présent, l’intervalle de confiance à 95% sur \\(\\Delta rear\\) va de 0.81 à 2.19. Il ne contient donc pas zéro. Donc, nous pouvons rejetter \\(H_0\\) au seuil \\(\\alpha\\) de 5%. Nous pouvons effectivement interpréter le test de cette façon, mais le test t de Student se définit de manière plus classique en comparant la valeur \\(t_{obs}\\) à la distribution théorique, et en renvoyant une valeur P associée au test. Ainsi, le lecteur peut interpréter les résultats avec son propre seuil \\(\\alpha\\) éventuellement différent de celui choisi par l’auteur de l’analyse. Le raisonnement est le suivant. Sous \\(H_0\\), la distribution de \\(\\Delta rear\\) est connue. Elle suit une distribution t de Student de moyenne égale à la vraie valeur de la différence des moyennes, d’écart type égal à l’erreur standard sur cette différence, et avec \\(n - 2\\) degrés de liberté. En pratique, nous remplaçons les valeurs de la population pour la différence des moyennes et pour les erreurs standard par celles estimées par l’intermédiaire de l’échantillon. Comme dans le cas du test \\(\\chi^2\\), nous définissons les zones de rejet et de non rejet par rapport à cette distribution théorique. Dans le cas du test de Student bilatéral, l’aire \\(\\alpha\\) est répartie à moitié à gauche et à moitié à droite (Fig. 9.6). Figure 9.6: Visualisation de la distribution de Student réduite sous l’hypothèse nulle du test bilatéral au seuil de 5%. Nous pouvons calculer la valeur P nous-même comme ceci, sachant la valeur de \\(t_{obs} = \\frac{\\Delta rear}{SE_{\\Delta rear}}\\) parce que nous travaillons avec une distribution t réduite : (t_obs &lt;- delta_rear / se) # [1] 4.289553 (p_value &lt;- pt(t_obs, df = 198, lower.tail = FALSE) * 2) # [1] 2.797369e-05 Ne pas oublier de multiplier la probabilité obtenue par deux, car nous avons un test bilatéral qui considère une probabilité égale à gauche et à droite de la distribution ! Naturellement, R propose une fonction toute faite pour réaliser ce test afin que nous ne devions pas détailler les calculs à chaque fois. Il s’agit de la fonction t.test(). Dans la SciViews Box, le snippet équivalent est accessible depuis .hm pour hypothesis tests: means. Dans le menu qui apparait, vous choisissez independant Student's t-test. Les arguments de la fonction sont les suivants. Le jeu de données dans data =, une formule qui reprend le nom de la variable quantitative à gauche (rear) et celui de la variable qualitative à deux niveaux à droite (sex), l’idication du type d’hypothèse alternative, ici alternative = &quot;two-sided&quot; pour un test bilatéral, le niveau de confiance égal à \\(1 - \\alpha\\), donc conf.level = 0.95 et enfin si nous considérons les variances comme égales pour les deux sous-populations var.equal = TRUE. t.test(data = crabs, rear ~ sex, alternative = &quot;two.sided&quot;, conf.level = 0.95, var.equal = TRUE) # # Two Sample t-test # # data: rear by sex # t = 4.2896, df = 198, p-value = 2.797e-05 # alternative hypothesis: true difference in means is not equal to 0 # 95 percent confidence interval: # 0.8087907 2.1852093 # sample estimates: # mean in group F mean in group M # 13.487 11.990 Nous retrouvons exactement toutes les valeurs que nous avons calculées à la main. Dans le cas présent, rappelez-vous la façon d’interpréter le test. Nous comparons la valeur P à \\(\\alpha\\). Si elle est plus petit, nous rejettons \\(H_0\\), sinon, nous ne la rejettons pas. Ici, nous rejettons \\(H_0\\) et pourrons dire que la largeur à l’arrière de la carapace de L. variegatus diffère de manière significative entre les mâles et les femelles au seuil \\(\\alpha\\) de 5% (test t bilatéral, t = 4,29, ddl = 198, valeur P &lt;&lt; 10-3). Conditions d’application échantillon représentatif (échantillonnage aléatoire et individus indépendants les uns des autres), observations indépendantes les unes des autres, une variable numérique et une variable facteur à deux niveaux, distribution de la population… normale, alors le test basé sur la distribution t de Student sera exact, approximativement normale, le test sera approximativement exact, non normale, le test sera approximativement exact si \\(n\\) est grand. Petite astuce… les mesures morphométriques sont dépendantes de la taille globale de l’animal qui varie d’un individu à l’autre, il vaut donc mieux étudier des rapports de tailles plutôt que des mesures absolues. Refaites le calcul sur base du ratio rear / length comme exercice et déterminez si la différence est plus ou moins nette entre les mâles et les femelles que dans le cas de rear seul. Vous pouvez également comparer les crabes bleus (species = “B”) avec les crabes oranges (species = “O”) à l’aide du même test. A vous de jouer ! Appliquez les test de student dans votre projet portant sur la biométrie humaine. Pour en savoir plus Une vidéo en anglais qui explique le test t de Student un peu différemment. "],
["variantes-du-test-t-de-student.html", "9.4 Variantes du test t de Student", " 9.4 Variantes du test t de Student A vous de jouer ! Afin d’appliquer directement les concepts vu dans ce module, ouvrez RStudio dans votre SciViews Box, puis exécutez l’instruction suivante dans la fenêtre console : BioDataScience::run(&quot;09b_ttest_wmw&quot;) Nous venons de voir ce qu’on appelle très précisément le test t de Student indépendant bilatéral avec variances égales. Nous allons maintenant étudier d’autres variantes. 9.4.1 Variances inégales Dans le test précédent, nous avons supposé que les variances entre les valeurs \\(rear_F\\) et \\(rear_M\\) étaient égales, mais rien ne dit que cela soit le cas46. Si nous ne voulons pas de cette contrainte, une variante du test permet de comparer deux moyennes même en présence de variances inégales : le test de Welch. Il consiste à ajuster les degrés de liberté en cas de variances inégales. Il suffit de préciser var.equal = FALSE. t.test(data = crabs, rear ~ sex, alternative = &quot;two.sided&quot;, conf.level = 0.95, var.equal = FALSE) # # Welch Two Sample t-test # # data: rear by sex # t = 4.2896, df = 187.76, p-value = 2.862e-05 # alternative hypothesis: true difference in means is not equal to 0 # 95 percent confidence interval: # 0.8085599 2.1854401 # sample estimates: # mean in group F mean in group M # 13.487 11.990 9.4.2 Test unilatéral Pour rappel, nous avions considéré ceci : \\(H_0: \\overline{rear_F} - \\overline{rear_M} = 0\\) \\(H_1: \\overline{rear_F} - \\overline{rear_M} \\neq 0\\) L’hypothèse alternative \\(H_1\\) est ici la plus générale. Parfois, nous avons plus d’information qui nous permet de dire que si \\(H_0\\) n’est pas rencontrée, \\(\\overline{rear_F} - \\overline{rear_M}\\) ne peut alors qu’être négatif (on parle de test unilatéral à gauche) ou positif (test unilatéral à droite). En effet, dans ce cas nous placerons la zone de rejet complètement à la gauche ou à la droite de la distribution. En épluchant la littérature, nous réalisons que les crabes de la famille des Grapsidae dont L. variegatus fait partie ont systématiquement un arrière plus large chez la femelle lorsqu’un dymorphisme sexuel existe. Nous pouvons modifier nos hypothèses comme suit : \\(H_0: \\overline{rear_F} - \\overline{rear_M} = 0\\) \\(H_1: \\overline{rear_F} - \\overline{rear_M} &gt; 0\\) Notez la différence pour \\(H_1\\). Nous avons alors ici un test unilatéral à droite. Nous indiquons alternative = &quot;greater&quot;. Pour un test unilatéral à gauche, nous utilisons alternative = &quot;less&quot;. t.test(data = crabs, rear ~ sex, alternative = &quot;greater&quot;, conf.level = 0.95, var.equal = FALSE) # # Welch Two Sample t-test # # data: rear by sex # t = 4.2896, df = 187.76, p-value = 1.431e-05 # alternative hypothesis: true difference in means is greater than 0 # 95 percent confidence interval: # 0.9201205 Inf # sample estimates: # mean in group F mean in group M # 13.487 11.990 Notez que la valeur P a été divisée par deux par rapport au test bilatéral. Ceci est le résultat d’une répartition différente de l’aire de rejet qui est placée ici entièrement sur la droite (Fig. 9.7). Nous n’avons donc plus dû multiplier la valeur calculée par deux pour la répartir également de l’autre côté de la distribution. Figure 9.7: Visualisation de la distribution de Student réduite sous l’hypothèse nulle d’un test unilatéral à droite au seuil de 5%. Toute la zone de rejet est à droite. Un autre exemple évident de test unilatéral : si nous mesurons la concentration d’une substance en solution \\([S]\\) et que nous nous demandons si cette substance est présente, nous aurons : \\(H_0: \\overline{[S]} = 0\\) \\(H_1: \\overline{[S]} &gt; 0\\) Ce test sera nécessairement unilatéral à droite car des concentrations négatives ne sont pas possibles. 9.4.3 Test t apparié Dans le cas du test indépendant, nous comparons des individus différents dans les deux conditions. Le tableau de données se présente comme suit : Un test t indépendant nécessite une variable numérique et une variable facteur à deux niveaux. Un autre cas de figure se présente si nous comparons des mesures réalisées sur les mêmes individus. Dans ce cas, le tableau de données se présente différemment : Un test t apparié nécessite deux variables numériques mesurées sur les mêmes individus, donc les mêmes lignes dans un tableau cas par variables. Par exemple, si nous voulons déterminer si la largeur de la carapace de L. variegatus diffère entre l’avant (variable front) et l’arrière, nous testerons (test bilatéral par défaut sans plus d’information) : \\(H_0: \\overline{front - rear} = 0\\) \\(H_1: \\overline{front - rear} \\neq 0\\) A chaque fois, les dimensions front et rear sont mesurées sur les mêmes individus. Nous soustrayons l’un de l’autre d’abord individu par individu. Attention ! Les hypothèses se formulent différemment. Il s’agit de la difference des moyennes pour le test t indépendant et la moyenne des différences pour le test t apparié. Si le résultat du calcul en ce qui concerne la moyenne ne diffère pas, la distribution des valeurs est différente (variance, écart type, erreur standard, …). Nous calculons la moyenne de cette différence notée delta_f_r. Nous résumons ces résultats en calculant successivement : la moyenne des différences dans mean_f_r, l’erreur standard dans se_f_r (pour rappel, \\(SE = \\frac{s_x}{\\sqrt{n}}\\)), la valeur de t_obs reportée selon la distribution t réduite, donc, en divisant mean_f_r par se_f_r. crabs %&gt;.% mutate(., delta_f_r = front - rear) %&gt;.% summarise(., mean_f_r = mean(delta_f_r), se_f_r = sd(delta_f_r) / sqrt(n()), t_obs = mean_f_r / se_f_r) # # A tibble: 1 x 3 # mean_f_r se_f_r t_obs # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 2.84 0.112 25.3 La distribution de référence sous \\(H_0\\) à partir de ces calculs est une \\(t(0, 1, 199)\\), donc avec \\(n - 1\\) comme degrés de liberté. Donc, la valeur P peut se calculer comme suit (n’oubliez pas de multiplier par deux pour reporter l’aire des deux côtés de la distribution pour un test bilatéral) : pt(25.324, df = 199, lower.tail = FALSE) * 2 # [1] 3.667686e-64 Avant d’effectuer le test, pensez toujours à visualiser vos données avec la représentation la plus judicieuse. Ici, la boite de dispersion parallèle n’est pas très indiquée car elle ne prend pas en compte l’appariement des données. Un graphique en nuage de points avec la bissectrice (pente de un et ordonnée à l’origine de zéro) ajoutée comme référence convient bien mieux. Sous \\(H_0\\), le nuage de points est sensé se concentrer sur cette bissectrice. chart(data = crabs, rear ~ front) + geom_point() + geom_abline(slope = 1, intercept = 0) Nous voyons très clairement que tous les points se situent du même côté de la bissectrice, ce qui suggère très fortement que \\(H_0\\) ne tient pas la route ici. Le test correspondant s’obtient dans la SciViews Box à partir du menu .hm pour hypothesis tests: means et dans le menu qui apparait, on sélectionne le “snippet” paired Student's t test. Il s’agit toujours de la fonction t.test() mais cette fois-ci au lieu d’une formule, nous indiquons les deux variables suivant la notation &lt;data.frame&gt;$&lt;variable&gt;, et nous spécifions paired = TRUE. Fixons notre seuil \\(\\alpha\\) à 5% avant de faire le test, puis exécutons l’instruction suivante : t.test(crabs$front, crabs$rear, alternative = &quot;two.sided&quot;, conf.level = 0.95, paired = TRUE) # # Paired t-test # # data: crabs$front and crabs$rear # t = 25.324, df = 199, p-value &lt; 2.2e-16 # alternative hypothesis: true difference in means is not equal to 0 # 95 percent confidence interval: # 2.623004 3.065996 # sample estimates: # mean of the differences # 2.8445 Nous rejettons \\(H_0\\) car la valeur P est inférieure à \\(\\alpha\\). Nous en concluons que le crabe L. variegatus a une carapace significativement plus large à l’avant qu’à l’arrière (t test apparié = 25,3, ddl = 199, valeur P &lt;&lt; 10-3). Naturellement, les variantes unilatérales à gauche et à droite existent aussi pour ce test. Par contre, la question des variances égales ou non ne se pose pas dans ce cas47. Notez que le test se limite à indiquer que la valeur P &lt; 2,2 . 10-16. Effectivement, notre calcul donne 3.7 . 10-64. Conditions d’application échantillon représentatif (échantillonnage aléatoires), individus appariés (mesures sur les mêmes individus pour les deux conditions comparées), observations indépendantes les unes des autres, deux variables numériques, distribution de la population… normale, alors le test basé sur la distribution t de Student sera exact, approximativement normale, le test sera approximativement exact, non normale, le test sera approximativement exact si \\(n\\) est grand. Avant de réaliser une expérience, vous vous demanderez certainement s’il vaut mieux vous orienter vers une design pour un test apparié ou indépendant. Le test apparié n’est pas toujours possible. Par exemple, si vous comparez deux espèces, il est évident qu’un même individu ne peut appartenir simultanément aux deux espèces ! Mais dans tous les cas où c’est possible, le design apparié est à préférer. Il permet d’éliminer une bonne part de la variabilité inter-individuelle de l’analyse, puisque cet effet joue de manière équivalente sur les deux mesures pour chaque paire d’observations sur le même individu. Le choix du test apparié permet d’observer des différentes plus subtiles indétectables en mode indépendant pour un même nombre de réplicats, ou alors, il permet d’observer les mêmes effets mais avec un nombre de mesures bien plus faible. Dans tous les cas, on est gagnant. Encodage correct Faites très attention à l’encodage des données. Il vous faut un tableau cas par variables correct, sinon vous risquez de vous orienter vers un mauvais test. Prenez le jeu de données sleep du package datasets. sleep &lt;- read(&quot;sleep&quot;, package = &quot;datasets&quot;) rmarkdown::paged_table(sleep) Nous avons extra, le nombre d’heures supplémentaires de sommeil suite à l’administration de médicaments, soit le 1, soit le 2. Le tableau se présente comme pour effectuer un test t indépendant (une variable numérique extra et une variable facteur à deux niveaux group). Cependant, regardez aussi la variable ID qui est l’identifiant de la personne testée. Vous constatez que les mêmes identifiants reviennent parmi les deux groupes. C’est en réalité un design apparié, mais mal encodé. Nous allons d’abord restructurer ce tableau correctement avant d’appliquer le test adéquat. sleep2 &lt;- spread(sleep, key = group, value = extra) names(sleep2) &lt;- c(&quot;id&quot;, &quot;med1&quot;, &quot;med2&quot;) rmarkdown::paged_table(sleep2) Dans sleep2nous avons un véritable tableau cas par variable c’est-à-dire, une ligne par individu. Nous avons alors deux variables numériques med1 et med2 mesurées sur les mêmes individus. Le test apparié suivant est correct. Un test indépendant effectué sur sleep aurait été incorrect ! \\(H_0: \\overline{med1 - med2} = 0\\) \\(H_1: \\overline{med1 - med2} \\neq 0\\) chart(data = sleep2, med2 ~ med1) + geom_point() + geom_abline(slope = 1, intercept = 0) t.test(sleep2$med1, sleep2$med2, alternative = &quot;two.sided&quot;, conf.level = 0.95, paired = TRUE) # # Paired t-test # # data: sleep2$med1 and sleep2$med2 # t = -4.0621, df = 9, p-value = 0.002833 # alternative hypothesis: true difference in means is not equal to 0 # 95 percent confidence interval: # -2.4598858 -0.7001142 # sample estimates: # mean of the differences # -1.58 Pour en savoir plus Une autre explication avec un autre exemple et une explication de la façon d’accéder aux résultats stockés dans l’objet htest renvoyé par la fonction t.test(). 9.4.4 Test t univarié Dans le cas du test t de Student univarié, nous comparons la moyenne d’une seule variable à une constante de référence. Le schéma suivant indique qu’une seule variable quantitative (numeric) est nécessaire. Le calcul est le même que dans le cas du test t apparié, avec l’étape de soustration x1 - x2 en moins. Par exemple dans le jeu de données sleep2 nous pouvons nous demander si le médicament med1 change la durée du sommeil. Les hypothèses sont les suivantes : \\(H_0: \\overline{med1} = 0\\) \\(H_1: \\overline{med1} \\neq 0\\) N’oublions pas une représentation graphique adéquate. Ici, une boite de dispersion avec un trait horizontal superposé pour indiquer la valeur cible convient très bien. chart(data = sleep2, med1 ~ &quot;&quot;) + geom_boxplot() + geom_hline(yintercept = 0, col = &quot;red&quot;) + xlab(&quot;&quot;) + ylab(&quot;Sommeil supplémentaire avec med1 [h]&quot;) Le graphique suggère peut-être que les patient dorment plus, mais la boite de dispersion chevauche quand même le trait horizontal à zéro. Donc, qu’en est-il exactement ? Le test s’obtient à partir du “snippet” univariate Student's t-test depuis .hm pour hypothesis tests: means. Notez l’argument supplémentaire mu = qui permet de spécifier la valeur de référence (ici zéro). Fixons \\(\\alpha\\) avant d’effectuer le test. Ici, nous prenons toujours 5%. t.test(sleep2$med1, alternative = &quot;two.sided&quot;, mu = 0, conf.level = 0.95) # # One Sample t-test # # data: sleep2$med1 # t = 1.3257, df = 9, p-value = 0.2176 # alternative hypothesis: true mean is not equal to 0 # 95 percent confidence interval: # -0.5297804 2.0297804 # sample estimates: # mean of x # 0.75 La valeur P est supérieure à \\(\\alpha\\). Nous ne rejettons pas \\(H_0\\). Nous ne pouvons pas dire que le médicament 1 change de manière significative la durée de sommeil chez les patients au seuil \\(\\alpha\\) de 5% (test t univarié = 1,33, ddl = 9, valeur P = 0,22). Notez également que dans le cas présent, l’intervalle de confiance [-0,53, 2,03] contient zéro. Conditions d’application échantillon représentatif (échantillonnage aléatoires), observations indépendantes les unes des autres, une variable numérique, distribution de la population… normale, alors le test basé sur la distribution t de Student sera exact, approximativement normale, le test sera approximativement exact, non normale, le test sera approximativement exact si \\(n\\) est grand. Il existe des tests pour le vérifier, comme le test de Bartlett, mais ce n’est pas le propos ici.↩ Pouvez-vous expliquer pourquoi ? Réfléchissez, c’est assez évident.↩ "],
["test-de-wilcoxon.html", "9.5 Test de Wilcoxon", " 9.5 Test de Wilcoxon Nous avons vu que le test t de Student se réfère à une distribution qui n’est exacte que si la population de départ est normale. Même si le théorème central limite permet encore d’utiliser le test dans le cas contraire lorsque n est grand, tous les cas ne sont pas rencontrés. Si n est petit et que l’on soupçonne une distribution non normale (pensez au graphique quantile-quantile pour visualiser la distribution) que faire ? En fait, il existe un test similaire qui ne fait aucune hypothèse sur la distribution des données : le test de Wilcoxon. Comme il ne se réfère pas aux paramètres d’une distribution théorique, ce type de test est appelé non paramétrique, par opposition au test t de Student qui est qualifié de paramétrique. Les mêmes variantes existent (test indépendant, test apparié, bilatéral versus unilatéral). Ce test se réalise à partir de la fonction wilcox.test() accessible à partir des “snippets” dans le menu .hn pour hypothesis tests: nonparametric. Bien que ce test ne travaille plus sur les moyennes mais est associé aux médianes, nous le présentons ici parce qu’il représente une alternative au test t de Student. Pour les données relatives à med1 de sleep2, cela donne48 : \\(H_0: mediane_{med1} = 0\\) \\(H_1: mediane_{med1} \\neq 0\\) Et dans R, nous faisons (à part le nom de la fonction, les arguments sont identiques) : wilcox.test(sleep2$med1, alternative = &quot;two.sided&quot;, mu = 0, conf.level = 0.95) # Warning in wilcox.test.default(sleep2$med1, alternative = &quot;two.sided&quot;, mu = # 0, : cannot compute exact p-value with zeroes # # Wilcoxon signed rank test with continuity correction # # data: sleep2$med1 # V = 31, p-value = 0.3433 # alternative hypothesis: true location is not equal to 0 Le test n’aime pas les ex-aequos et les valeurs nulles, mais ce n’est pas dramatique. C’est la raison pour laquelle R nous averti seulement mais effectue quand même le calcul. La conclusion est la même que pour le test t de Student, nous ne pouvons pas rejetter \\(H_0\\). Dans le cas d’un test indépendant (comparaison de la largeur de carapace à l’arrière rear en fonction du sexe chez L. variegatus en considérant que les femelles sont potentiellement plus grosses), nous obtenons : \\(H_0: P(rear_F &gt; rear_M) = P(rear_F &lt; rear_M)\\) \\(H_1: P(rear_F &gt; rear_M) &gt; P(rear_F &lt; rear_M)\\) wilcox.test(data = crabs, rear ~ sex, alternative = &quot;greater&quot;, conf.level = 0.95) # # Wilcoxon rank sum test with continuity correction # # data: rear by sex # W = 6710, p-value = 1.473e-05 # alternative hypothesis: true location shift is greater than 0 Nous obtenons un résultat similaire au test t de Student indépendant. Nous rejettons \\(H_0\\). Concernant les conditions d’application, elles sont les mêmes que pour le test t de Student équivalent à part les contraintes sur la distribution qui disparaissent. Quand choisir le test t de Student ou le test de Wilcoxon ? Nous serions tentés d’effectuer un test de Wilcoxon à chaque fois puisqu’ainsi nous sommes certains de ne pas comettre d’impair concernant la distribution des données. Mais en fait, le test t de Student est à préférer à chaque fois que c’est possible car il est plus puissant. Cela signifie qu’il sera capable de détecter un effet significatif plus tôt que le Wilcoxon. La bonne stratégie est donc d’utiliser le test t de Student autant que possible et de ne se rabattre sur le test de Wilcoxon que si on ne peut pas faire autrement, soit n petit et distribution non normale ou inconnue. A noter que quand n est vraiment petit, il est impossible d’étudier valablement la distribution des données sur cet échantillon. On doit alors se baser sur un échantillon plus grand prélevé sur des données similaires ou alors jouer la sécurité et effectuer un test de Wilcoxon. Cette analyse est valable pour toute paire de test paramétrique versus non paramétrique. Le tableau suivant résume cela. Test paramétrique Test non paramétrique Contrainte (distribution) à vérifier indépendant de toute distribution Puissance (même jeu de données) plus puissant moins puissant Choix à privilégier à utiliser seulement si test paramétrique non applicable Mais au fait, c’est quoi la “puissance” d’un test d’hypothèse ? Considérons les quatres cas possibles. Ne pas rejetter \\(H_0\\) lorsque \\(H_0\\) est vraie (correct) Rejetter \\(H_0\\) lorsque \\(H_0\\) est fausse (correct) Rejetter \\(H_0\\) lorsque \\(H_0\\) est vraie (erreur de 1ère espèce \\(\\alpha\\)) Ne pas rejetter \\(H_0\\) lorsque \\(H_0\\) est fausse (erreur de 2ème espèce \\(\\beta\\)) L’erreur de 1ère espèce est associée au risque \\(\\alpha\\). L’erreur de seconde espèce est associée au risque \\(\\beta\\). Plus \\(\\beta\\) est petit, plus le test est puissant. La puissance est \\(1 - \\beta\\). La puissance d’un test peut se calculer à l’aide des fonctions pwr.xxx() du package pwr. Par exemple dans le test t univarié concernant med1 nous ferons : pwr::pwr.t.test(n = 10, d = 1.3, sig.level = 0.05, type = &quot;one.sample&quot;, alternative = &quot;two.sided&quot;) # # One-sample t test power calculation # # n = 10 # d = 1.3 # sig.level = 0.05 # power = 0.9538774 # alternative = two.sided La puissance est de 0,954. Donc, \\(\\beta\\) vaut 1 - 0,954 = 0,046 ou pratiquement 5%. On a un test bien équilibré entre \\(\\alpha\\) et \\(\\beta\\), et de plus, \\(\\beta\\) nous concerne particulièrement puisqu’ici nous ne rejettons pas \\(H_0\\). Le système judiciaire des statistiques, par Hadley Wickham. A vous de jouer ! Appliquez les test de student et ses variantes dans vos projets portant sur la biométrie humaine, sur la croissance des oursins et sur le zooplankton. Dans le cas d’un test de Wilcoxon apparié, on a \\(H_0: mediane_x = mediane_y\\).↩ "],
["representation-graphique.html", "9.6 Représentation graphique", " 9.6 Représentation graphique Il n’existe pas un graphique de référence afin de présenter un test t de Student ou un test de Wilcoxon. On retrouve malheureusement dans la littérature plusieurs graphiques qui coexistent. Afin de présenter 3 graphiques courants, nous utilisons le jeu de données crabs. On retrouve généralement 2 graphiques montrant la moyenne et l’intervalle de confiance. a &lt;- chart(data = crabs, rear ~ sex) + stat_summary(geom = &quot;col&quot;, fun.y = &quot;mean&quot;) + stat_summary(geom = &quot;errorbar&quot;, width = 0.1, fun.data = &quot;mean_cl_normal&quot;, fun.args = list(conf.int = 0.95)) b &lt;- chart(data = crabs, rear ~ sex) + geom_jitter(alpha = 0.3, width = 0.2) + stat_summary(geom = &quot;point&quot;, fun.y = &quot;mean&quot;, size = 2) + stat_summary(geom = &quot;errorbar&quot;, width = 0.1, fun.data = &quot;mean_cl_normal&quot;, fun.args = list(conf.int = 0.95), size = 1) combine_charts(list(a,b)) La graphique en barre et le nuage de point représentent identiquement la information. Ils représentent la moyenne avec des barres erreurs qui représentent l’intervalle de confiance à 0.95. Le graphe en dynamite avec des barres d’erreurs ne donne aucune information sur le nombre d’observation. Nous avons déja abordé rapidement cette problématique dans la section 4.1.3. Il n’est donc pas le graphique optimal pour présenter un test t de Student. Malgré le fait que le test t de Student est un test paramétrique, on retrouve dans la littérature scientifique la boite de dispersion qui est pourtant un graphique associé à des valeurs non paramétriques. Pour le test de wilcoxon, la boite de dispersion est l’outil graphique recommandé. chart(data = crabs, rear ~ sex) + geom_boxplot() Parmi ces 3 choix, nous vous conseillons d’employer le nuage de point avec la valeur moyenne et l’intervalle de confiance. chart(data = crabs, rear ~ sex) + geom_jitter(alpha = 0.3, width = 0.2) + stat_summary(geom = &quot;point&quot;, fun.y = &quot;mean&quot;) + stat_summary(geom = &quot;errorbar&quot;, width = 0.1, fun.data = &quot;mean_cl_normal&quot;, fun.args = list(conf.int = 0.95)) Pièges et astuces Les barres d’erreurs Que vous utilisiez le nuage de points ou le graphique en barres, vous devez être extrêmement vigilant aux barres d’erreurs. Vous devez toujours préciser et bien comprendre ce que les barres d’erreurs cachent. Voici 4 graphiques qui présentent différentes barres d’erreurs (la taille de l’axe y a volontairement été figée). p &lt;- chart(data = crabs, rear ~ sex) + geom_jitter(alpha = 0.1, width = 0.2) + stat_summary(geom = &quot;point&quot;, fun.y = &quot;mean&quot;) + scale_y_continuous(limits = c(5,22)) a &lt;- p + stat_summary(geom = &quot;errorbar&quot;, width = 0.1, fun.data = &quot;mean_cl_normal&quot;, fun.args = list(conf.int = 0.95)) b &lt;- p + stat_summary(geom = &quot;errorbar&quot;, width = 0.1, fun.data = &quot;mean_sdl&quot;, fun.args = list(mult = 1)) c &lt;- p + stat_summary(geom = &quot;errorbar&quot;, width = 0.1, fun.data = &quot;mean_sdl&quot;, fun.args = list(mult = 2)) d &lt;- p + stat_summary(geom = &quot;errorbar&quot;, width = 0.1, fun.data = &quot;mean_se&quot;, fun.args = list(mult = 1)) combine_charts(list(a,b,c,d)) Figure 9.8: Nuage de points de la largeur de l’arrière de la carapace en focntion du sexe avec la moyenne et des barres d’erreurs. Graphe A : moyenne et intervalle de confiance 0.95. Graphe B : moyenne et écart-type. Graphe C : moyenne et 2*écart-type. Graphe D : moyenne et erreur standard Les formules associées aux barres d’erreurs sont les suivantes de A à D: Graphe A : l’intervalle de confiance 0.95 \\[\\mathrm{IC}(1 - \\alpha) \\simeq \\bar{x} \\pm t_{\\alpha/2}^{n-1} \\cdot SE_x\\] mean_cl_normal(crabs$rear) # y ymin ymax # 1 12.7385 12.37968 13.09732 Graphe B : l’écart-type \\[s_x = \\sqrt{\\sum_{i=1}^n{\\frac{(x_i - \\bar{x})^2}{n-1}}}\\] mean_sdl(crabs$rear, mult = 1) # y ymin ymax # 1 12.7385 10.16516 15.31184 Graphe C : deux fois la valeur de l’écart-type \\[2 \\times s_x\\] mean_sdl(crabs$rear, mult = 2) # y ymin ymax # 1 12.7385 7.59182 17.88518 Graphe D : l’erreur standard \\[SE_x = \\frac{s_x}{\\sqrt{n}}\\] mean_se(crabs$rear) # y ymin ymax # 1 12.7385 12.55654 12.92046 Comme nous venons de le voir, les barres d’erreurs sont calculées à partir de différentes fonctions. Il est donc indispensable de préciser explicitement ce que les barres d’erreurs représentent. Pour en savoir plus Beware of dynamite. Démonstration de l’impact d’un graphe en barres pour représenter la moyenne (et l’écart type) = graphique en “dynamite”. Dynamite plots : unmitigated evil? Une autre comparaison du graphe en dynamite avec des représentations alternatives qui montre que le premier peut avoir quand même quelques avantages dans des situations particulières. Comparaison de moyennes : indiquez la significativité des différences sur le graph. Tutoriel sur la comparaison de moyennes avec ggpubr Pour terminer, bien que la moyenne soit un descripteur statistique très utile, il est parfois utilisé de manière abusive. Une distribution statistique ne se résume pas à un nombre, fût-ce la moyenne. De plus, si la distribution est asymétrique, la moyenne est un mauvais choix (préférer alors la médiane, ou transformer les données pour rendre la distribution plus symétrique). La vidéo suivante détaille le problème qui peut se produire : "],
["variance.html", "Module 10 Variance", " Module 10 Variance Objectifs Pouvoir comparer plus de deux populations simultanément en utilisant des techniques de décomposition de la variance Découvrir le modèle linéaire, anciennement analyse de variance (ANOVA) Savoir effectuer des tests de comparaison multiples Connaitre l’équivalent non paramétrique à un facteur (test de Kruskal-Wallis) Prérequis Ce module continue la comparaison de moyennes entamée, pour deux populations au module 9. Assurez-vous d’avoir bien compris le test t de Student et les subtilités des tests d’hypothèse avant d’entamer la présente section. "],
["le-danger-des-tests-multiples.html", "10.1 Le danger des tests multiples", " 10.1 Le danger des tests multiples Les tests t de Student et de Wilcoxon sont limités à la comparaison de deux populations. Poursuivons notre analyse des crabes L. variegatus. Rappelez-vous, nous avons deux variétés (variable species, B pour bleue et Opour orange). Si nous voulons comparer simultanément les mâles et les femelles des deux variétés, cela nous fait quatre sous-populations à comparer (nous utilisons ici la fonction paste() qui rassemble des chaînes de caractère avec trait comme caractère séparateur sep =&quot;-&quot; pour former une variable facteur à quatre niveaux, B-F, B-M, O-F, O-M). Nous transformaons cette variable en facteur à l’aide de factor() et nous ajoutons un label avec labelise(). crabs &lt;- read(&quot;crabs&quot;, package = &quot;MASS&quot;, lang = &quot;fr&quot;) crabs %&gt;.% mutate(., group = labelise( factor(paste(species, sex, sep = &quot;-&quot;)), &quot;Groupe espèce - sexe&quot;, units = NA)) -&gt; crabs2 La Fig. 10.1 montre la largeur à l’arrière de la carapace chez les quatre groupes ainsi individualisés. Une représentation graphique adéquate avant de réaliser notre analyse ici lorsque le nombre de répliquats est important est le graphique en violon sur lequel nous superposons au moins les moyennes, et de préférence, les points également. Si le nombre de répliquats est plus faible, mais toujours supérieur à 7-8, nous pourrions utiliser le même type de graphique mais avec des boites de dispersion plutôt (voir plus loin, Fig. 10.4). Avec encore moins de répliquats nous présenterons les points et les moyennes uniquement. chart(data = crabs2, rear ~ group) + geom_violin() + geom_jitter(width = 0.05, alpha = 0.5) + geom_point(data = group_by(crabs2, group) %&gt;.% summarise(., means = mean(rear, na.rm = TRUE)), f_aes(means ~ group), size = 3, col = &quot;red&quot;) Figure 10.1: Largeur arrière en fonction du groupe de crabes L. variegatus. Graphique adéquat pour comparer les moyennes et distributions dans le cas d’un nombre important de répliquats (moyennes en rouge + observations individuelles en noir semi-transparent superposées à des graphiques en violon). Nous en profitons également pour essayer l’astuce proposée au module précédent. Au lieu de travailler sur la variable rear seule, nous allons étudier l’aspect ratio entre largeur à l’arrière (rear) et largeur maximale (width) de la carapace afin de nous débarrasser d’une source de variabilité triviale qui est qu’un grand crabe est grand partout, et de même un petit crabe est petit pour toutes ses mesures. Nous prenons soin également de libeller cette nouvelle variable correctement avec labelise(). Enfin, nous ne conservons que les variables species, sex, group et aspect avec `select() : crabs2 %&gt;.% mutate(., aspect = labelise( as.numeric(rear / width), &quot;Ratio largeur arrière / max&quot;, units = NA)) %&gt;.% select(., species, sex, group, aspect) -&gt; crabs2 skimr::skim(crabs2) # Skim summary statistics # n obs: 200 # n variables: 4 # # ── Variable type:factor ───────────────────────────────────────────────────────────────────────────────────────────────────── # variable missing complete n n_unique top_counts # group 0 200 200 4 B-F: 50, B-M: 50, O-F: 50, O-M: 50 # sex 0 200 200 2 F: 100, M: 100, NA: 0 # species 0 200 200 2 B: 100, O: 100, NA: 0 # ordered # FALSE # FALSE # FALSE # # ── Variable type:numeric ──────────────────────────────────────────────────────────────────────────────────────────────────── # variable missing complete n mean sd p0 p25 p50 p75 p100 hist # aspect 0 200 200 0.35 0.03 0.28 0.32 0.36 0.38 0.41 ▂▅▅▃▅▇▆▁ Nous avons 50 individus dans chacun des quatre groupes. Lorsqu’il y a le même nombre de réplicats dans tous les groupes, on appelle cela un plan balancé. C’est une situation optimale. Nous devons toujours essayer de nous en rapprocher le plus possible car, si le nombre d’individus mesurés diffère fortement d’un groupe à l’autre, nous aurons forcément moins d’information disponible dans le ou les groupes moins nombreux, ce qui déforcera notre analyse. Nous voyons également que la variable aspect semble avoir une distribution bimodale d’après le petit histogramme représenté dans le résumé. La Fig. 10.2 avec aspect montre une différence plus qu'avecrear` seul, à la Fig. @ref(fig:crabs_rear). chart(data = crabs2, aspect ~ group) + geom_violin() + geom_jitter(width = 0.05, alpha = 0.5) + geom_point(data = group_by(crabs2, group) %&gt;.% summarise(., means = mean(aspect, na.rm = TRUE)), f_aes(means ~ group), size = 3, col = &quot;red&quot;) Figure 10.2: Ratio largeur arrière/largeur max en fonction du groupe de crabes L. variegatus. Graphique adéquat pour comparer les moyennes et distributions dans le cas d’un nombre important de répliquat (moyennes en rouge + observations individuelles en noir semi-transparent superposées à des graphiques en violon). Nous voyons ici beaucoup mieux que la distribution bimodale est essentiellement dûe au dymorphiqme sexuel plutôt qu’à des différences entre les variétés, mais qu’en est-il plus précisément car si nous regardons attentivement, il semble que les moyennes pour les crabes bleus sont légèrement inférieures à ces des crabes oranges. Comment comparer valablement ces quatre groupes ? Comme nous savons maintenant comparer deux groupes à l’aide d’un test t de Student, il est tentant d’effectuer toutes les comparaisons deux à deux et de résumer l’ensemble, par exemple, dans un tableau synthétique. Ca fait quant même beaucoup de comparaisons (B-F &lt;-&gt; B-M, B-F &lt;-&gt; O-F, B-F &lt;-&gt; O-M, B-M &lt;-&gt; O-F, B-M&lt;-&gt; O-M, et finalement O-F &lt;-&gt; O-M). Cela fait six comparaisons à réaliser. N’oublions pas que, à chaque test, nous prenons un risque de nous tromper. Le risque de se tromper au moins une fois dans l’ensemble des tests est alors décuplé en cas de tests multiples. Prenons un point de vue naïf, mais qui suffira ici pour démontrer le problème qui apparaît. Admettons que le risque de nous tromper est constant, que nous rejettons ou non \\(H_0\\), et qu’il est de l’ordre de 10% dans chaque test individuellement49. La seule solution acceptable est que tous les tests soeijnt corrects. Considérant chaque interprétation indépendante, nous pouvons multiplier les probabilités d’avoir un test correct (90%) le nombre de fois que nous faisons le test, soit \\(0,9 \\times 0,9 \\times 0,9 \\times 0,9 \\times 0,9 \\times 0,9 = 0,9^6 = 0,53\\). Tous les autres cas ayant au moins un test faux, nous constatons que notre analyse globale sera incorrecte \\(1 - 0,53 = 47\\%\\) du temps50. Notre analyse sera incorrecte une fois sur deux environ. De manière générale, le nombre de combinaisons deux à deux possibles dans un set de n groupes distincts sera calculé à l’aide du coefficient binomial que nous avions déjà rencontré avec la distribution du même nom, ici avec \\(j\\) valant deux. \\[C^j_n = \\frac{n!}{j!(n-j)!}\\] Toujours avec notre approche naïve du risque d’erreur individuel pour un test \\(r\\) de 10%, le risque de se tromper au moins une fois est alors : \\[1 - (1 - r)^{C^2_n}\\] Voici ce que cela donne comme risque de se tromper dans au moins un des tests en fonction du nombre de groupes à comparer  : Groupes comparés 2 à 2 2 3 4 6 8 10 Risque individuel = 10% 10% 27% 47% 79% 95% 99% Clairement, on oublie cette façon de faire ! Prendre le risque de se tromper 99 fois sur 100 en comparant 10 groupes différents n’est pas du tout intéressante comme perspective. Nous allons donc travailler différemment… Ci-après nous verrons qu’une simplification des hypothèses et l’approche par décomposition de la variance est une option bien plus intéressante (ANalysis Of VAriance ou ANOVA). Ensuite, nous reviendrons vers ces comparaisons multiples deux à deux, mais en prenant des précautions pour éviter l’inflation du risque global de nous tromper. Attention ! vous savez bien que c’est plus compliqué que cela. D’une part, le risque de se tromper est probablement différent si on rejette \\(H_0\\) (\\(\\alpha\\)) ou non (\\(\\beta\\)), et ces risques sont encore à moduler en fonction de la probabilité a priori, un cas similaire au dépistage d’une maladie plus ou moins rare, rappelez-vous, au module 7.↩ Dans R, vous pouvez utiliser choose(n, j) pour calculer le coefficient binomial. Donc votre calcul du risque de se tromper au moins une fois dans un ensemble de n tests dont le risque individuel est r sera 1 - (1 - r)^choose(n, 2).↩ "],
["anova-a-un-facteur.html", "10.2 ANOVA à un facteur", " 10.2 ANOVA à un facteur Au lieu de s’attaquer aux comparaisons deux à deux, nous pouvons aussi considérer une hypothèse unique que les moyennes de \\(k\\) populations (nos quatre groupes différents de crabes, par exemple) sont égales. L’hypothèse alternative sera qu’au moins une des moyennes diffère des autres. En formulation mathématique, cela donne : \\(H_0: \\mu_1 = \\mu_2 = ... = \\mu_k\\) \\(H_1: \\exists(i, j) \\mathrm{\\ tel\\ que\\ } \\mu_i \\neq \\mu_j\\) Notre hypothèse nulle est très restrictive, mais par contre, l’hypothèse alternative est très vague car nous ne savons pas où sont les différences à ce stade si nous rejettons \\(H_0\\), mais nous nous en occuperons plus tard. Propriété d’additivité des parts de variance. La variance se calcule comme : \\[var_x = \\frac{SCT}{ddl}\\] Avec \\(SCT\\), la somme des carrés totaux, soit \\(\\sum_{i = 1}^n (x_i - \\bar{x})^2\\), la somme des carrés des écarts à la moyenne générale. Les ddl sont les degrés de liberté déjà rencontrés à plusieurs reprises qui valent \\(n - 1\\) dans le cas de la variance d’un échantillon. Cette variance peut être partitionnée. C’est-à-dire que, si la variance totale se mesure d’un point A à un point C, l’on peut mesurer la part de variance d’un point A à un point B, puis l’autre part d’un point B à un point C, et dans ce cas, \\[SCT = SC_{A-C} = SC_{A-B} + SC_{B-C}\\] Cette propriété, dite d’additivité des variances, permet de décomposer la variance totale à souhait tout en sachant que la somme des différentes composantes donne toujours la même valeur que la variance totale. 10.2.1 Modèle de l’ANOVA Mais qu’est-ce que cette propriété d’additivité des variances vient faire ici ? Nous souhaitons comparer des moyennes, non ? Effectivement, mais considérons le modèle mathématique suivant : \\[y_{ij} = \\mu + \\tau_j + \\epsilon_i \\mathrm{\\ avec\\ } \\epsilon \\sim N(0, \\sigma)\\] Avec l’indice \\(j = 1 .. k\\) populations et l’indice \\(i = 1 .. n\\) observations du jeu de données. Chaque observation \\(y_{ij}\\) correspond à deux écarts successifs de la moyenne globale \\(\\mu\\) : une constante “tau” par population \\(\\tau_j\\) d’une part et un terme \\(\\epsilon_i\\) que l’on appelle les résidus et qui est propre à chaque observation individuelle. C’est ce dernier terme qui représente la partie statistique du modèle avec une distribution normale centrée sur zéro et avec un écart type \\(\\sigma\\) que nous admettrons constant et identique pour toutes les populations par construction. Le graphique à la Fig. 10.3 représente une situation typique à trois sous-populations. Figure 10.3: Décomposition de la variance dans un cas à trois populations A, B et C fictives. Notons que ce modèle à trois termes représente bien la situation qui nous intéresse, mais aussi, qu’il décompose la variance totale (entre \\(\\mu\\) et chaque point observé) en deux : ce que nous appelerons le terme inter représentant l’écart entre la moyenne globale \\(\\mu\\) et la moyenne de la sous-population concernées (\\(\\tau_j\\)) et le terme intra depuis cette moyenne de la sous-population jusqu’au point observé (\\(\\epsilon_i\\)). D’une part, nous nous trouvons dans une situation d’additivité de la variance si nous décidons de calculer ces “variance inter” et “variance intra”. D’autre part, sous \\(H_0\\) nous sommes sensés avoir toutes les moyennes égales à \\(\\mu\\), et donc, tous les \\(\\tau_j = 0\\). Donc, les valeurs non nulles de \\(\\tau_j\\) ne doivent qu’être dus au hasard de l’échantillonnage et être par conséquent largement inférieurs à la variabilité entre les individus, ou variance intra \\(\\epsilon_i\\). La Fig. 10.4 représente deux cas avec à gauche une situation où \\(H_0\\) est plausible, et à droite une situation où elle est très peu plausible. Notez qu’à gauche la variation entre les observations (intra) est bien plus grande que l’écart entre les moyennes (inter), alors qu’à droite c’est l’inverse. # Warning: `data_frame()` is deprecated, use `tibble()`. # This warning is displayed once per session. Figure 10.4: A. Cas fictif avec moyennes probablement égales entre populations (étalement des points bien plus large que l’écart entre les moyennes), B. cas où les moyennes sont probablement différentes (écart des moyennes “inter” bien plus grand que l’étalement des points en “intra”-population). Intuitivement, une comparaison de “inter” et “intra” permet de différencier la situation de gauche de celle de droite dans la Fig. 10.4. Si cette comparaison est faite sous forme d’un ratio “inter”/“intra”, alors ce ratio sera faible et tendra vers zéro sous \\(H_0\\) (cas A), alors qu’il sera d’autant plus élevé que \\(H_0\\) devient de moins en moins plausible (cas B). Calcul de l’ANOVA Calcul des sommes des carrés (inter- et intragroupes). Considérant : i = indice des observations au sein du jeu de données de 1 à n, j = facteurs (sous-populations de 1 à k), \\(\\bar{y}\\) = moyenne generale de l’échantillon, \\(\\bar{y_j}\\) = moyenne de la jème population. La somme des carrés inter \\(SC_{inter}\\) et la somme des carrés intra \\(SC_{intra}\\) se calculent comme suit : \\[ \\begin{aligned} SC_{inter} = \\sum_{i=1}^n{(\\bar{y_j} - \\bar{y})^2} &amp;&amp; SC_{intra} = \\sum_{i=1}^n{(y_{ij} - \\bar{y_j})^2} \\end{aligned} \\] A ces sommes des carrés, nous pouvons associer les degrés de liberté suivants : k – 1 pour l’intergroupe n – k pour l’intragroupe Sachant que les parts de variances sont les \\(\\frac{SC}{ddl}\\) et sont appelés “carrés moyens”, nous construisons ce qu’on appelle le tableau de l’ANOVA de la façon suivante : Type Ddl Somme carrés Carré moyen (CM) Statistique Fobs P (&gt;F) Inter (facteur) k - 1 SCinter SCinter/ddlinter CMinter/CMintra … Intra (résidus) n - k SCintra SCintra/ddlintra La statistique Fobs est le rapport des carrés moyens inter/intra. Elle représente donc le ratio que nous avons évoqué plus haut comme moyen de quantifier l’écart par rapport à \\(H_0\\). Le test consiste à calculer la valeur P associée à cette statistique. Pour cela, il nous faut une distribution statistique théorique de F sous \\(H_0\\). C’est un biologiste - statisticien célèbre nommé Ronald Aylmer Fisher qui l’a calculée. C’est la distribution F de Fisher. 10.2.2 Distribution F A vous de jouer ! Afin d’appliquer directement les concepts vu dans ce module, ouvrez RStudio dans votre SciViews Box, puis exécutez l’instruction suivante dans la fenêtre console : BioDataScience::run(&quot;10a_anova&quot;) La distribution F est une distribution asymétrique n’admettant que des valeurs nulles ou positives, d’une allure assez similaire à la distribution du \\(\\chi^2\\) que nous avons étudiée au module 8. Elle est appelée loi de Fisher, ou encore, loi de Fisher-Snedecor. Elle a une asymptote horizontale à \\(+\\infty\\). La distribution F admet deux paramètres, respectivement les degrés de liberté au numérateur (inter) et au dénominateur (intra). La Fig. 10.5 représente la densité de probabilité d’une loi F typique51. Figure 10.5: Allure typique de la densité de probabilité de la distribution F (ici ddl inter = 5 et ddl intra = 20). Plus F~obs est grand, plus l’hypothèse nulle est suspecte. La zone de rejet est donc positionnée à droite (en rouge). Nous commençons à avoir l’habitude maintenant. La valeur P est calculée comme l’aire à droite du quantile correspondant à Fobs. Enfin, nous rejetterons \\(H_0\\) seulement si la valeur P est inférieure au seuil \\(\\alpha\\) qui a été choisi préalablement au test. Ceci revient à constater que, graphiquement, Fobs vient se positionner dans la zone de rejet en rouge comme sur la Fig. 10.5. Conditions d’application échantillon représentatif (par exemple, aléatoire), observations indépendantes, variable dite réponse quantitative, une variable dite explicative qualitative à trois niveaux ou plus, distribution normale des résidus \\(\\epsilon_i\\), homoscédasticité (même variance intragroupes, “homoscedasticity” en anglais, opposé à hétéroscédasticité = variance différente entre les groupes). Les deux dernières conditions d’applications doivent être vérifiées. La nomralité des résidus doit être rencontrée aussi bien que possible. Un graphique quantile-quantile des résidus permet de se faire une idée, comme sur la Fig. 10.6. Néanmoins, le test étant relativement robuste à des petites variations par rapport à la distribution normale, surtout si ces variations sont symétriques, nous ne seront pas excessivement stricts ici. Figure 10.6: Graphique quantile-quantile appliqué aux résidus d’une ANOVA pour déterminer si leur distribution se rapproche d’un loi normale. La condition d’homoscédasticité est plus sensible. Elle mérite donc d’être vérifiée systématiquement et précisément. Différents tests d’hypothèse existent pour le vérifier, comme le test de Batlett, le test de Levene, etc. Nous vous proposns ici d’utiliser le test de Batlett. Ses hypothèses sont : \\(H_0: var_1 = var_2 = ... = var_k\\) (homoscédasticité) \\(H_1: \\exists(i, j) \\mathrm{\\ tel\\ que\\ } var_i \\neq var_j\\) (hétéroscédasticité) Si la valeur P est inférieure au seuil \\(\\alpha\\) fixé au préalable, nous devrons rechercher une transformation des variables qui stabilisera la variance. La première transformation à essayer en biologie et la transformation logarithmique surtout si les valeurs négatives de la variable réponse ne sont pas possibles, signe d’une distribution qui peut être plutôt de type log-normale pour cette variable. Si aucune transformation ne stabilise la variance, nous devrons nous rabattre vers un test non paramétrique équivalent, le test de Kruskal-Wallis que nous aborderons plus loin dans ce module. La SciViews Box propose des snippets pour accéder à ces différentes analyses. Dans le menu hypothesis tests: variances ou .hv nous trouvons trois tests dont celui de Bartlett. Dans le menu hypothesis tests: means ou .hm se trouvent les templates pour l’ANOVA, ainsi que les graphiques d’analyse des résidus dont le graphique quantile-quantile. Résolution de notre exemple Nous commençons par déterminer si nous avons homoscédasticité. Cosidérons un seuil \\(\\alpha\\) de 5% pour tous nos tests. Ensuite : bartlett.test(data = crabs2, aspect ~ group) # # Bartlett test of homogeneity of variances # # data: aspect by group # Bartlett&#39;s K-squared = 24.532, df = 3, p-value = 1.935e-05 Nous rejettons \\(H_0\\). Il n’y a pas homoscédasticité. Calculons par exemple le logarithme népérien de aspect et réessayons : crabs2 %&gt;.% mutate(., log_aspect = ln(aspect)) -&gt; crabs2 bartlett.test(data = crabs2, log_aspect ~ group) # # Bartlett test of homogeneity of variances # # data: log_aspect by group # Bartlett&#39;s K-squared = 37.891, df = 3, p-value = 2.981e-08 Ici cela ne fonctionne pas. Cela fait pire qu’avant. La transformation inverse (exp()) peut être essayée mais ne stabilise pas suffisamment la variance. Après divers essais, il s’avère qu’une transformation puissance cinq stabilise bien la variance. crabs2 %&gt;.% mutate(., aspect5 = aspect^5) -&gt; crabs2 bartlett.test(data = crabs2, aspect5 ~ group) # # Bartlett test of homogeneity of variances # # data: aspect5 by group # Bartlett&#39;s K-squared = 1.7948, df = 3, p-value = 0.6161 La Fig. 10.7 montre la distribution dans les différents groupes de la variable transformée. chart(data = crabs2, aspect5 ~ group) + geom_violin() + geom_jitter(width = 0.05, alpha = 0.5) + geom_point(data = group_by(crabs2, group) %&gt;.% summarise(., means = mean(aspect5, na.rm = TRUE)), f_aes(means ~ group), size = 3, col = &quot;red&quot;) + ylab(&quot;(ratio largeur arrière/max)^5&quot;) Figure 10.7: Transformation puissance cinq du ratio largeur arrière/largeur max en fonction du groupe de crabes L. variegatus. Nous poursuivons sur une description des données utile pour l’ANOVA52 : crabs2 %&gt;.% group_by(., group) %&gt;.% summarise(., mean = mean(aspect5), sd = sd(aspect5), count = sum(!is.na(aspect5))) # # A tibble: 4 x 4 # group mean sd count # &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; # 1 B-F 0.00727 0.00115 50 # 2 B-M 0.00363 0.00124 50 # 3 O-F 0.00811 0.00138 50 # 4 O-M 0.00427 0.00130 50 Ensuite l’ANOVA proprement dite : anova(anova. &lt;- lm(data = crabs2, aspect5 ~ group)) # Analysis of Variance Table # # Response: aspect5 # Df Sum Sq Mean Sq F value Pr(&gt;F) # group 3 0.00072741 2.4247e-04 150.53 &lt; 2.2e-16 *** # Residuals 196 0.00031572 1.6110e-06 # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Nous retrouvons ici le tableau de l’ANOVA. La valeur P est très faible et inférieure à \\(\\alpha\\). Nous rejettons \\(H_0\\). Nous pouvons dire que le ratio largeur arrière / max à la puissance cinq diffère significativement entre les groupes (ANOVA, F = 150, ddl = 3 &amp; 196, valeur P &lt;&lt; 10-3). Ce n’est qu’une fois l’ANOVA réalisée que nous pouvons calculer et visionner la distribution des résidus (Fig. 10.8). Cette distribution s’éloigne aux extrêmes de la distribution normale. Nous sommes dans un cas un peu limite. Soit nous considérons que l’écart est suffisamment léger pour ne pas trop influer sur l’ANOVA, soit nous devons passer à un test non paramétrique. Nous ferons les deux ici à des fins de comparaison. Figure 10.8: Graphique quantile-quantile des résidus pour l’analyse de aspect^5. # [1] 65 103 Pour en savoir plus L’ANOVA expliquée en trois minutes Introduction to ANOVA (en anglais). Part I, part II, part III, part IV, and part V. Explication de l’analyse de variance en détaillant le calcul par la Kahn academy. Partie I, partie II et partie III. Assez long : près de 3/4h en tout. Ne regardez que si vous n’avez pas compris ce que sont les sommes des carrés. Les fonctions qui permettent les calculs relatifs à la distribution F dans R sont &lt;x&gt;f(), et les snippets correspondants dans la SciViews Box sont disponibles à partir de .if. Leur utilisation est similaire à celle des distributions vues au module 7.↩ Un snippet dédié est disponible dans le menu hypothesis tests: means à partir de .hm.↩ "],
["test-post-hoc.html", "10.3 Test “post-hoc”", " 10.3 Test “post-hoc” Lorsque nous rejettons \\(H_0\\) dans l’ANOVA comme dans le cas de notre exemple, nous savons qu’il y a au moins deux moyennes qui diffèrent l’une de l’autre, mais nous ne savons pas lesquelles. Notre analyse n’est pas terminée. Nous allons revisiter les tests de comparaison multiples deux à deux, mais en prenant des précautions particulières pour éviter l’inflation du risque d’erreur. Tout d’abort, nous mettons en place un garde-fou. Nous effectuons toujours une ANOVA en premier lieu, et nous n’envisageons les comparaisons multiples que lorsque \\(H_0\\) est rejettée. Cela évite beaucoup de faux positifs. On appelle cela des tests “post hoc” car ils ne sont pas planifiés d’amblée, mais suivent un conclusion préalable (ici, le rejet de \\(H_0\\) dans un test ANOVA). Une approche simple consisterait à modifier notre seuil \\(\\alpha\\) pour chaque test individuel vers le bas afin que le risque de se tromper dans au moins un des tests ne dépasse pas la valeur de \\(/alpha\\) que nous nous sommes fixée. C’est la correction de Bonferroni. Elle consiste à diviser la valeur de \\(/alpha\\) par le nombre de tests simultanés nécessaires, et d’utiliser cette valeur corrigée comme seuil \\(\\alpha\\) de chaque test de Student individuel. Dans le cas de quatre populations, nous avons vu qu’il y a six comparaisons multiples deux à deux. Donc, en appliquant un seuil corrigé de \\(\\alpha/6 = 0,05 / 6 = 0,00833\\) pour chaque test, on aura la probabilité suivante pour \\(1 - \\alpha\\) : \\[(1 - 0,05/6)^6 = 0,951\\] Donc, le risque global pour l’ensemble des six tests est bien de 1 - 0,951 = 0,049, soit 5%. Si elle a le mérite d’être simple, cette façon de faire n’est pas considérée comme la plus efficace. Actuellement, la méthode HSD de Tukey est préférée. HSD veut dire “Honest Significant Difference”. La technique consiste à calculer l’écart minimal des moyennes pour considérer qu’elles sont significativement différentes l’une de l’autre. Ensuite, pour chaque comparaison deux à deux, l’écart entre les moyennes est comparée à cette valeur de référence53. Le test de comparaisons multiples de Tukey est accessible à partir des “snippets” dans la SciViews Box (.hm pour le menu hypothesis tests: means, puis anova - multiple comparisons). Nous obtenons à la fois une version textuelle et une version graphique qui résume les comparaisons. summary(anovaComp. &lt;- confint(multcomp::glht(anova., linfct = multcomp::mcp(group = &quot;Tukey&quot;)))) # # Simultaneous Tests for General Linear Hypotheses # # Multiple Comparisons of Means: Tukey Contrasts # # # Fit: lm(formula = aspect5 ~ group, data = crabs2) # # Linear Hypotheses: # Estimate Std. Error t value Pr(&gt;|t|) # B-M - B-F == 0 -0.0036378 0.0002538 -14.331 &lt; 0.001 *** # O-F - B-F == 0 0.0008441 0.0002538 3.326 0.00589 ** # O-M - B-F == 0 -0.0029979 0.0002538 -11.810 &lt; 0.001 *** # O-F - B-M == 0 0.0044820 0.0002538 17.657 &lt; 0.001 *** # O-M - B-M == 0 0.0006399 0.0002538 2.521 0.05938 . # O-M - O-F == 0 -0.0038420 0.0002538 -15.136 &lt; 0.001 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # (Adjusted p values reported -- single-step method) .oma &lt;- par(oma = c(0, 5.1, 0, 0)); plot(anovaComp.); par(.oma); rm(.oma) Si nous gardons notre seuil \\(\\alpha\\) de 5%, seuls les mâles ne diffèrent (tout juste) pas entre eux (`O-M - B-M) avec une valeur P de 6%. Toutes les autres comparaisons deux à deux sont significativement différentes. Sur le graphique, si les intervalles autour des différences de moyennes comprennent zéro, matérialisé par un trait pointillé vertical, les différences ne sont pas significatives. En conclusion, tous les groupes diffèrent de manière siginificative sauf les mâles (test HSD de Tukey, valeur P &lt; 5%). Conditions d’application Les conditions d’application pour le test post hoc de Tukey sont les mêmes que pour l’ANOVA. A vous de jouer Suite à la lecture de l’ANOVA à un facteur, réalisez ce projet individuel pour appliquer vos nouvelles connaissances. https://classroom.github.com/a/bHQ01LHm Lisez attentivement le README. Ce projet doit être terminé à la fin de ce module Nous ne détaillons pas le calcul du test de Tukey ici, mais vous pouvez aller voir ici.↩ "],
["test-de-kruskal-wallis.html", "10.4 Test de Kruskal-Wallis", " 10.4 Test de Kruskal-Wallis Les conditions d’application de l’ANOVA sont assez restrictives, en particuliers l’homoscédasticité et le normalité des résidus. Dans notre example, nous avons pu stabiliser la variance par une transformation puissance cinq, mais la distribution des résidus n’est pas optimale. Nous pouvons tout aussi bien décider de nous orienter vers la version non paramétrique équivalente : le test de Kruskal-Wallis. Le raisonnement entre ANOVA (test paramétrique) et Kruska-Wallis (test non paramétrique) est le même qu’entre le test de Student ou celui de Wilcoxon. Lorsque les conditions sont remplies, l’ANOVA est un test plus puissant à utiliser en priorité. Nous le préfèrerons donc, sauf dans les cas impossibles où aucune transformation des données ne permet d’obtenir une distribution acceptable des résidus et des variances. Le test de Kruskal-Wallis va comparer la localisation des points sur l’axe plutôt que les moyennes. Nous travaillons ici sur les rangs. La transformation en rangs consiste à ranger les observations de la plus petite à la plus grande et à remplacer les valeurs par leur position (leur rang) dans ce classement. Les ex-aequos reçoivent des rangs équivalents. Un petit exemple : # Un échantillon x &lt;- c(4.5, 2.1, 0.5, 2.4, 2.1, 3.5) # Tri par ordre croissant sort(x) # [1] 0.5 2.1 2.1 2.4 3.5 4.5 # Remplacement par les rangs rank(sort(x)) # [1] 1.0 2.5 2.5 4.0 5.0 6.0 Donc 0,5 donne 1, 2,1 apparait deux fois en position 2 et 3, donc on leurs attribue à tous les deux le rang 2,5. 2,4 est remplacé par 4, 3,5 par 5 et enfin 4,5 est remplacé par 6. Dans le test de Kruskal-Wallis, sous \\(H_0\\) nous avons le même rang moyen (noté \\(mr\\)) pour chaque groupe parmi \\(k\\). Sous \\(H_1\\), au moins deux groupes ont des rangs moyens différents. \\(H_0:mr_1 = mr_2 = ... = mr_k\\) \\(H_1: \\exists(i, j) \\mathrm{\\ tel\\ que\\ } mr_i \\neq mr_j\\) Nous ne détaillons pas les calculs (mais voyez ici). Le calcul aboutit en fait à une valeur de \\(\\chi^2_{obs}\\) à \\(k - 1\\) degrés de liberté lorsque k populations sont comparées. Ainsi, nous avons déjà un loi de distribution théorique pour le calcul de la valeur P. La zone de rejet est située à la droite de la distribution comme dans le cas de l’ANOVA et du test de \\(\\chi^2\\). Le test est réalisé dans R avec la fonction kruskal.test(). Notez que les arguments sont les mêmes que pour l’ANOVA à un facteur. Un “snippet” est disponible dans la SciViews Box (.hn pour hypothesis tests: nonparametric, ensuite Kruskal-Wallis test). kruskal.test(data = crabs2, aspect ~ group) # # Kruskal-Wallis rank sum test # # data: aspect by group # Kruskal-Wallis chi-squared = 139.68, df = 3, p-value &lt; 2.2e-16 Ici, nous rejettons \\(H_0\\) au seuil \\(\\alpha\\) de 5%. Nous pouvons dire qu’il y a au moins une différence significative entre les ratios au seuil \\(\\alpha\\) de 5% (test Kruskal-Wallis, \\(\\chi^2\\) = 140, ddl = 3, valeur P &lt;&lt; 10-3). Conditions d’application Le test de Kruskal-Wallis est naturellement moins contraignant que l’ANOVA. échantillon représentatif (par exemple, aléatoire), observations indépendantes, variable dite réponse quantitative, une variable dite explicative qualitative à trois niveaux ou plus, les distributions au sein des différentes sous-population sont, si possible, similaires mais de forme quelconque. 10.4.1 Test “post hoc” non paramétrique Nous devrions également réaliser des tests “pot hoc” lorsque nous rejettons \\(H_0\\) du Kruskal-Wallis. Les version non paramétriques des tests de comparaisons multiples sont moins connus. Nous pourrions effectuer des comparaisons deux à deux avec des tests de Wilcoxon en appliquant une correction de Bonferroni. Une autre option consiste à utiliser une version non paramétrique du test de Tukey basée sur les rangs, dit test pot hoc non paramétrique asymptotique. Les explications assez techniques sont disponibles ici. Ce test est également disponible dans la SciViews Box depuis .hn, en sélectionnant l’entrée Kruskal-Wallis: multiple comparisons dans le menu déroulant. summary(kw_comp. &lt;- nparcomp::nparcomp(data = crabs2, aspect ~ group)) # # #------Nonparametric Multiple Comparisons for relative contrast effects-----# # # - Alternative Hypothesis: True relative contrast effect p is less or equal than 1/2 # - Type of Contrast : Tukey # - Confidence level: 95 % # - Method = Logit - Transformation # - Estimation Method: Pairwise rankings # # #---------------------------Interpretation----------------------------------# # p(a,b) &gt; 1/2 : b tends to be larger than a # #---------------------------------------------------------------------------# # # # #------------Nonparametric Multiple Comparisons for relative contrast effects----------# # # - Alternative Hypothesis: True relative contrast effect p is less or equal than 1/2 # - Estimation Method: Global Pseudo ranks # - Type of Contrast : Tukey # - Confidence Level: 95 % # - Method = Logit - Transformation # # - Estimation Method: Pairwise rankings # # #---------------------------Interpretation--------------------------------------------# # p(a,b) &gt; 1/2 : b tends to be larger than a # #-------------------------------------------------------------------------------------# # # #----Data Info-------------------------------------------------------------------------# # Sample Size # 1 B-F 50 # 2 B-M 50 # 3 O-F 50 # 4 O-M 50 # # #----Contrast--------------------------------------------------------------------------# # B-F B-M O-F O-M # B-M - B-F -1 1 0 0 # O-F - B-F -1 0 1 0 # O-M - B-F -1 0 0 1 # O-F - B-M 0 -1 1 0 # O-M - B-M 0 -1 0 1 # O-M - O-F 0 0 -1 1 # # #----Analysis--------------------------------------------------------------------------# # Comparison Estimator Lower Upper Statistic p.Value # 1 p( B-F , B-M ) 0.017 0.004 0.070 -7.229567 1.618261e-12 # 2 p( B-F , O-F ) 0.695 0.542 0.815 3.257267 5.905270e-03 # 3 p( B-F , O-M ) 0.051 0.018 0.137 -7.014700 7.867484e-12 # 4 p( B-M , O-F ) 0.990 0.949 0.998 7.210499 2.279288e-12 # 5 p( B-M , O-M ) 0.654 0.501 0.781 2.606445 4.792701e-02 # 6 p( O-F , O-M ) 0.027 0.008 0.088 -7.385440 4.057865e-13 # # #----Overall---------------------------------------------------------------------------# # Quantile p.Value # 1 2.593391 4.057865e-13 # # #--------------------------------------------------------------------------------------# plot(kw_comp.) La présentation des résultats est plus détaillée que pour le HSD de Tukey. Le graphique est très imilaire. Ici, toutes les différences sont considérées comme significatives, même si la comparaison des mâles B-M - O-M avec une valeur P tout juste significative de 0,049 est à prendre avec des pincettes étant donné sa proximité du seuil. Au final que conclure ? Lorsque l’ANOVA peut être utilisée, elle est à préférer. Ici avec une transformation puissance cinq, nous stabilisons la variance qui est le critère le plus sensible. De plus tant avec l’ANOVA qu’avec le Kruskal-Wallis, nous détectons des différences significatives. Les tests “post hoc” nous indiquent des différences significatives au seuil \\(\\alpha\\) de 5% sauf peut-être pour les mâles (il faudrait prévoir des mesures supplémentaires pour confirmer ou infirmer à ce niveau). A vous de jouer Afin d’appliquer directement les concepts vu au cours dans ce module, ouvrez RStudio dans votre SciViews Box, puis exécutez l’instruction suivante dans la fenêtre console : BioDataScience::run(&quot;10b_anova_kruskal&quot;) Appliquez l’analyse de variances ou le test de Kruskal-Wallis dans vos projets portant sur la biométrie humaine et sur le zooplankton. "],
["sciences-des-donnees-et-litterature.html", "10.5 Sciences des données et littérature", " 10.5 Sciences des données et littérature Vous avez à présent vu l’utilisation de plusieurs tests statistiques très courant en science (t-test, Anova, \\(\\chi^2\\),…). La restitution correcte de ce test au sein d’un publication scientifique est très importante. Intéressez-vous à la restitution de résultat statistique au sein de la littérature. Un projet individuel est mis à votre disposition https://classroom.github.com/g/bc3Lh7C6 "],
["variance2.html", "Module 11 Variance II", " Module 11 Variance II Objectifs Maîtriser différentes variantes d’ANOVA à deux facteurs Distinguer dans quel cas utiliser l’une ou l’autre de ces variantes Comprendre différents types de syntaxe dans R Prérequis Ce module présente la suite de l’ANOVA initiée au module 10. Vous devez avoir bien compris l’ANOVA à un facteur avant d’entamer le présent chapitre. A vous de jouer En lien avec ce module vous avez une série d’exercices à réaliser. Vous avez à : réaliser un learnR sur l’anova à 2 facteurs Afin d’appliquer directement les concepts vu au cours dans ce module, ouvrez RStudio dans votre SciViews Box, puis exécutez l’instruction suivante dans la fenêtre console : BioDataScience::run(&quot;11a_anova2&quot;) Ce projet doit être terminé à la fin de ce module compléter des fichiers RMD au sein du projet ci-dessous : Suite à la lecture de l’ANOVA à un 2 facteurs, réalisez ce projet individuel pour appliquer vos nouvelles connaissances. https://classroom.github.com/a/evNsJgOs Lisez attentivement le README. Ce projet doit être terminé à la fin de ce module réaliser un learnR sur les différentes syntaxe dans R Afin d’appliquer directement les concepts vus au cours dans ce module, ouvrez RStudio dans votre SciViews Box, puis exécutez l’instruction suivante dans la fenêtre console : BioDataScience::run(&quot;11b_syntaxr&quot;) Ce projet doit être terminé à la fin de ce module réaliser au moins 1 ANOVA à deux facteurs dans votre projet transversal sur la biométrie humaine "],
["anova-a-deux-facteurs.html", "11.1 ANOVA à deux facteurs", " 11.1 ANOVA à deux facteurs Dans le cadre de l’ANOVA à un facteur, nous avions une variable réponse numérique étudiée pour différents niveaux d’une seule variable facteur à j niveaux ou modalités. Le modèle utilisé était : \\[y_{ij} = \\mu + \\tau_j + \\epsilon_i \\mathrm{\\ avec\\ } \\epsilon \\sim N(0, \\sigma)\\] Les \\(\\tau_j\\) représent les variations entre la moyenne générale \\(µ\\) et les moyennes respectives des \\(j\\) sous-populations. En R, nous avons utilisé la formule suivante : \\[y \\sim fact\\] avec \\(y\\) la variable numérique réponse et \\(fact\\) la variable facteur explicative unique. Si nous prenons notre exemple des crabes L. variegatus, nous avions travaillé un peu artificiellement sur une seule variable facteur en regroupant les variables species et sex en une seule variable group. Qu’en est-il si nous voulons quand même considérer les deux variables species et sex séparément ? c’est possible avec une ANOVA à deux facteurs. Les sections suivantes vous présentent quelques variantes possibles de cette analyse. "],
["modele-sans-interactions.html", "11.2 Modèle sans interactions", " 11.2 Modèle sans interactions La version la plus simple consiste à considérer simplement deux facteurs successivement, c’est-à-dire que la variance est décomposée d’abord selon le premier facteur, et ensuite selon le second. \\[y_{ijk} = \\mu + \\tau1_j + \\tau2_k + \\epsilon_i \\mathrm{\\ avec\\ } \\epsilon \\sim N(0, \\sigma)\\] avec \\(\\tau1_j\\) correspondant à l’écart de la moyenne générale \\(µ\\) à la moyenne selon la jème population pour la variable fact1, et \\(\\tau2_k\\) correspondant à l’écart vers le kème niveau d’une seconde variable fact2. La formule qui spécifie ce modèle dans R avec les trois variables y, fact1 et fact2 s’écrit : \\[y \\sim fact1 + fact2\\] Notez que, quel que soit le niveau considéré pour \\(\\tau1\\), un niveau donné de \\(\\tau2\\) est constant dans l’équation qui décrit ce modèle. Cela signifie que l’on considère que les écarts pour les moyennes de la variable fact2 sont toujours les mêmes depuis les moyennes de fact1. Donc, si une sous-population de fact2 tend à avoir une moyenne, disons, supérieure pour la première sous-population de fact1, elle sera considérée comme ayant les mêmes écarts pour toutes les autres sous-populations de fact1. Evidemment, cette condition n’est pas toujours rencontrée dans la pratique. Le graphique des interactions (Fig. 11.1) permet de visualiser les écarts des moyennes respectives des différentes sous-populations. read(&quot;crabs&quot;, package = &quot;MASS&quot;, lang = &quot;fr&quot;) %&gt;.% mutate(., aspect = labelise( as.numeric(rear / width), &quot;Ratio largeur arrière / max&quot;, units = NA)) %&gt;.% mutate(., aspect5 = labelise( aspect^5, &quot;(Ratio largeur arrière /max)^5&quot;, units = NA)) %&gt;.% select(., species, sex, aspect, aspect5) -&gt; crabs2 # Graphique de base pour visualiser les interactions #chart$base(interaction.plot(crabs2$species, crabs2$sex, crabs2$aspect5)) # Version avec ggplot2 crabs2 %&gt;.% group_by(., species, sex) %&gt;.% summarise(., aspect5_groups = mean(aspect5)) %&gt;.% print(.) %&gt;.% # Tableau des moyennes par groupes chart(data = ., aspect5_groups ~ species %col=% sex %group=% sex) + geom_line() + geom_point() # # A tibble: 4 x 3 # # Groups: species [2] # species sex aspect5_groups # &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; # 1 B F 0.00727 # 2 B M 0.00363 # 3 O F 0.00811 # 4 O M 0.00427 Figure 11.1: Graphique des interactions entre les variables facteurs (espèce et sexe). Les traits (pratiquement) parallèles indiquent qu’il n’y a pas d’interactions, comme c’est le cas ici. Au niveau de la description préliminaire des données, nous pourrons utiliser un tableau qui résume la moyenne, l’écart type et le nombre d’observations pout chaque combinaison des deux variables facteurs. Le template de ce code est disponible dans un “snippet” à partir du menu hypothesis tests: means ou .hm, et ensuite two-way ANOVA (description). crabs2 %&gt;.% group_by(., species, sex) %&gt;.% summarise(., mean = mean(aspect5), sd = sd(aspect5), count = sum(!is.na(aspect5))) # # A tibble: 4 x 5 # # Groups: species [2] # species sex mean sd count # &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; # 1 B F 0.00727 0.00115 50 # 2 B M 0.00363 0.00124 50 # 3 O F 0.00811 0.00138 50 # 4 O M 0.00427 0.00130 50 Pour la visualisation graphique, nous sommes tributaires du nombre d’observations. Avec moins d’une petite dizaine d’observations, nous représenterons des points pour chaque observation et superposerons les moyennes. Lorsque le nombre est plus grand nous pourrons utiliser soit les boites de dispersion, soit le graphique en violon si ce nombre est encore plus grand. Voyons cela sur notre exemple (les “snippets” dans le menu chart: bivariate peuvent être utilisés comme point de départ auquel nous ajoutons la seconde variable facteur pour les facettes multi-graphiques séparée par un |). La Fig. 11.2 montre ce que cela donne si l’on opte pour les boites de dispersion. chart(data = crabs2, aspect5 ~ species | sex) + geom_boxplot() Figure 11.2: Taille relative de la carapace à l’arrière de crabes L. variegatus (deux variétés et deux sexes), version simple. La Fig. 11.3 est une version améliorée avec les observations et les moyennes pour chaque sous-groupe ajoutées au graphique selon la même technique que nous avions utilisé pour représenter les données pour l’ANOVA à un facteur. chart(data = crabs2, aspect5 ~ species | sex) + geom_boxplot() + geom_jitter(width = 0.05, alpha = 0.5) + geom_point(data = group_by(crabs2, species, sex) %&gt;.% summarise(., means = mean(aspect5, na.rm = TRUE)), f_aes(means ~ species), size = 3, col = &quot;red&quot;) Figure 11.3: Taille relative de la carapace à l’arrière de crabes L. variegatus (deux variétés et deux sexes), version annotée. Maintenant que nous avons décrit correctement nos données par rapport à l’analyse que nous souhaitons faire, nous pouvons réaliser notre ANOVA à deux facteurs. Nous devons vérifier l’homoscédasticité, mais le test de Batlett que nous réalisons revient au même que celui que nous avons fait en décomposant toutes les sous-populations. Comme nous n’avons pas nécessairement ce calcul réalisé (la variable group que nous avions calculée au module 10), nous utilisons la fonction interaction() qui effectue ce calcul pour nous directement dans la formule : bartlett.test(data = crabs2, aspect5 ~ interaction(species, sex)) # # Bartlett test of homogeneity of variances # # data: aspect5 by interaction(species, sex) # Bartlett&#39;s K-squared = 1.7948, df = 3, p-value = 0.6161 Si vous comparez avec le test que nous avions fait dans le cas de l’ANOVA à un facteur sur la variable group, vous constaterez qu’il donne exectement le même résultat. Nous continuons avec notre ANOVA. Nous avons un “snippet” pour cela dans le menu hypothesis tests: means à partir de .hm qui se nomme two-way ANOVA (without interactions). anova(anova. &lt;- lm(data = crabs2, aspect5 ~ species + sex)) # Analysis of Variance Table # # Response: aspect5 # Df Sum Sq Mean Sq F value Pr(&gt;F) # species 1 0.00002753 0.00002753 17.15 5.12e-05 *** # sex 1 0.00069935 0.00069935 435.66 &lt; 2.2e-16 *** # Residuals 197 0.00031624 0.00000161 # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Comme la variance est décomposée en trois étapes (selon l’espèce, puis selon le sexe, puis les résidus), nous avons trois lignes dans le tableau de l’ANOVA. Nous effectuons deux tests. Le premier consiste à comparer les carrés moyens (Mean Sq) pour l’espèce par rapport aux résidus. Donc, la valeur F est le ratio de la somme des carrés species divisé par la somme des carrés des résidus, et cette valeur est reportée sur la loi de distribution théorique F pour obtenir une première valeur P (ici 5,12 . 10-5). De même, le second test qui s’intéresse au sexe calcule la valeur F via la ratio de la somme des carrés pour sex divisé par la somme des carrés des résidus, et la loi F nous permet de calculer une seconde valeur P (ici 2,2 . 10-16). Nous interprétons chacun des deux tests séparément. Dans notre cas, nous pouvons dire avec un seuil \\(\\alpha\\) de 5% que nous rejettons \\(H_0\\) dans les deux cas. Donc le rapport largeur arrière sur largeur max de la carapace est significativement différent au seuil \\(\\alpha\\) de 5% à la fois en fonction de l’espèce (F = 17,15, ddl = 197 et 1, valeur P &lt;&lt; 0,001) et du sexe (F = 436, ddl = 197 et 1, valeur P &lt;&lt; 0,001)54. La suite logique consiste à réaliser des tests “post hoc”. Ils ne sont pas vraiment nécessaires ici puisque nous n’avons que deux niveaux pour chacune des deux variables, mais nous les réalisons quand même pour montrer le code correspondant. Un template est accessible via le “snippet” anova - multiple comparisons [multcomp] du menu .hm. Pensez juste à rajouter le second facteur sex dans les arguments de la fonction mcp(). summary(anovaComp. &lt;- confint(multcomp::glht(anova., linfct = multcomp::mcp(species = &quot;Tukey&quot;, sex = &quot;Tukey&quot;)))) # Add a second factor if you want # # Simultaneous Tests for General Linear Hypotheses # # Multiple Comparisons of Means: Tukey Contrasts # # # Fit: lm(formula = aspect5 ~ species + sex, data = crabs2) # # Linear Hypotheses: # Estimate Std. Error t value Pr(&gt;|t|) # species: O - B == 0 0.0007420 0.0001792 4.141 0.000102 *** # sex: M - F == 0 -0.0037399 0.0001792 -20.872 &lt; 1e-10 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # (Adjusted p values reported -- single-step method) .oma &lt;- par(oma = c(0, 5.1, 0, 0)); plot(anovaComp.); par(.oma); rm(.oma) Ceci confirme que les différences sont significatives au seuil \\(\\alpha\\) de 5%. Il ne nous reste plus qu’à vérifier la distribution des résidus de l’ANOVA pour que notre analyse soit complète (Fig. 11.4). Figure 11.4: Graphique quantile-quantile des résidus pour l’ANOVA à deux facteurs sans interactions de aspect^5. # [1] 65 103 Encore une fois, nous voyons que les résidus sont quasiment les mêmes que précédemment, mais cela n’aurait pas été le cas si une interaction existait. La distribution s’éloigne un peu d’une Gaussienne pour les valeurs élevées surtout. Mais comme l’ANOVA est robuste à ce critère, et que l’homoscédasticité a été vérifiée sur la tranformation puissance 5 de notre variable, nous pouvons conserver notre analyse moyennant une précaution supplémentaire : vérifier que les valeurs P sont beaucoup plus petites que notre seuil comme sécurité supplémentaires contre les approximations liées à la légère violation de la contrainte de distribution normale des résidus. C’est le cas ici, et nous pouvons donc conclure notre analyse. Conditions d’application échantillon représentatif (par exemple, aléatoire), observations indépendantes, variable réponse quantitative, deux variables explicatives qualitatives à deux niveaux ou plus, distribution normale des résidus \\(\\epsilon_i\\), homoscédasticité (même variance intragroupes), pas d’interactions entre les deux variables explicatives. Notez que si vous incluez le tableau de l’ANOVA dans votre rapport ou dans une publication, il n’est pas nécessaire de répéter les résultats des tests entre parenthèses. Vous pouvez juste vous référer au tableau en question.↩ "],
["modele-croise-complet.html", "11.3 Modèle croisé complet", " 11.3 Modèle croisé complet Le modèle ANOVA que nous venons de faire s’appelle un modèle croisé parce que les mesures sont effectuées pour chaque combinaison des niveaux des deux variables facteurs explicatives, et ce, de manière indépendante (les observations d’un niveau ne sont pas dépendantes de celles d’un autre niveau)55. crabs2 %&gt;.% count(., species, sex) # # A tibble: 4 x 3 # species sex n # &lt;fct&gt; &lt;fct&gt; &lt;int&gt; # 1 B F 50 # 2 B M 50 # 3 O F 50 # 4 O M 50 Le modèle croisé sans interactions que nous avions utilisés est cependant incomplet puisque, pour considérer tous les cas possibles, il faut aussi considérer que ces interactions puissent exister et les inclure directement dans le modèle. Le modèle complet s’écrit comme ceci : \\[y_{ijk} = \\mu + \\tau1_j + \\tau2_k + \\tau1\\tau2_{jk} + \\epsilon_i \\mathrm{\\ avec\\ } \\epsilon \\sim N(0, \\sigma)\\] avec le nouveau terme \\(\\tau1\\tau2_{jk}\\) qui correspond à la distance entre la kème moyenne générale (la moyenne quel que soit j) et la moyenne particulière pour les observations des populations particulières à k et j simultanément. Ce modèle permet ainsi que chaque moyenne \\(\\bar{y}_{jk}\\) puisse différer librement, et donc, autorise les interactions. Toujours considérant les trois variables y, fact1 et fact2, ce modèle s’écrit dans R comme suit : \\[y \\sim fact1 + fact2 + fact1:fact2\\] Avec \\(fact1:fact2\\) étant le terme d’interactions. On peut aussi le simplifier en utilisant * à la place de + entre les deux variables facteurs, ce qui signifie implicitement de tenir également compte des interactions : \\[y \\sim fact1 * fact2\\] Cette fois-ci, la décomposition de la variable se fait en quatre étapes : (1) depuis la moyenne générale µ vers les jèmes moyennes pour fact1, ensuite (2) de ces moyennes vers les kèmes moyennes pour fact2, puis (3) de ces dernières vers la moyenne particulière pour le sous-groupe jk, et enfin (4) les résidus \\(\\epsilon_i\\) pour chaque observation. Voyons ce que donne ce modèle complet sur nos données crabs2. Un “snippets” est utilisable (two-way ANOVA (complete model)). anova(anova. &lt;- lm(data = crabs2, aspect5 ~ species * sex)) # Analysis of Variance Table # # Response: aspect5 # Df Sum Sq Mean Sq F value Pr(&gt;F) # species 1 0.00002753 0.00002753 17.0913 5.279e-05 *** # sex 1 0.00069935 0.00069935 434.1610 &lt; 2.2e-16 *** # species:sex 1 0.00000052 0.00000052 0.3236 0.5701 # Residuals 196 0.00031572 0.00000161 # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Notre analyse confirme qu’il n’y a pas d’interactions. La valeur P (0,57) en regard du terme species:sex correspondant est très largement supérieure à \\(\\alpha\\) de 5%. Notez aussi que les tests relatifs à species et sex donnent des valeurs différentes de notre modèle sans interactions. Les différences entre les deux seront d’autant plus importantes que les interactions sont fortes. Les conclusions restent les mêmes que précédemment, et ici, nous démontrons par un test d’hypothèse que les interactions ne sont pas significatives. Naturellement, la description des données, les vérifications (homoscédasticité, distribution normale ou quasi-normale des résidus) et les analyses “post-hoc” en cas de rejet de \\(H_0\\) sont à réaliser ici aussi. Nous les avons déjà faites plus haut à peu de choses prêt (les résutats seront ici très proches de ceux du modèle sans interactions, puisque ces dernières sont négligeables). Faites attention à un piège fréquent lorsque vous avez des mesures multiples sur les mêmes individus. Par exemple, si vous étudiez trois populations avec disons, cinq réplicats par population et que vous dénombrez des cellules marquées sur dix coupes histologiques réalisées chaque fois dans un organe du même individu, vous aurez 3x5x10 = 150 mesures, mais vous ne pouvez pas utiliser une ANOVA à deux facteurs croisés car les 150 observations ne sont pas indépendantes les unes des autres. Vous n’avez jamais mesuré que 15 individus au total. Si vous analysez ces données comme si vous en aviez mesuré 150, votre analyse sera incorrecte. Il s’agit ici d’une erreur qui s’appelle la pseudo-réplication. Vous devrez utiliser d’autres modèles comme le modèle à facteurs hiérarchisés (voir section suivante) ou le modèle à mesures répétées (voir encore après). Conditions d’application Les conditions d’application sont les mêmes que pour l’ANOVA à deux facteurs sans interactions, sauf qu’ici, les interactions sont bien évidemment permises. Pour en savoir plus Un blog en français qui explique l’ANOVA à deux facteurs de manière plus détaillée qu’ici. Ensuite la résolution de leur exemple dans R. Enfin, des suggestions pour annoter un graphique et indiquer quelles sont les différences qui sont significatives dessus. De plus, nous avons ici un plan balancé puisque le nombre de réplicats pour chaque niveau est le même. C’est une situation optimale qu’il faut toujours chercher à atteindre pour une ANOVA, même si un nombre différent d’observations par niveau est également accepté.↩ "],
["facteurs-hierarchises.html", "11.4 Facteurs hiérarchisés", " 11.4 Facteurs hiérarchisés Nous n’avons pas toujours la possibilité de croiser les deux facteurs. Considérons le cas d’une étude d’intercalibration. Nous avons un ou plusieurs échantillons répartis entre plusieurs laboratoires, et comme les analyses dépendent éventuellement aussi du technicien qui fait la mesure, nous demandons à chaque laboratoire de répéter les mesures avec deux de leurs techniciens. Problème : ici, il s’agit bien évidemment de techniciens différents dans chaque laboratoire. Comment faire, sachant que pour le modèle croisé, il faudrait que les deux mêmes techniciens aient fait toutes les mesures dans tous les laboratoires ? La solution est le modèle à facteurs hiérarchisés qui s’écrit : \\[y_{ijk} = \\mu + \\tau1_j + \\tau2_k(\\tau1_j) + \\epsilon_i \\mathrm{\\ avec\\ } \\ \\epsilon_i \\sim N(0, \\sigma) \\] … et dans R, nous utiliserons la formule suivante : \\[y \\sim fact1 + fact2\\ \\%in\\%\\ fact1\\] Voici un exemple concret. Un gros échantillon d’oeufs déshydratés homogène est réparti entre six laboratoires différents en vue de la détermination de la teneur en matières grasses dans cet échantillon. Le but de la manoeuvre est de déterminer si les laboratoires donnent des résultats consistants. Les deux techniciens de chaque laboratoire sont labellés one et two, mais ce sont en fait à chaque fois des techniciens différents dans chaque laboratoire56. eggs &lt;- read(&quot;eggs&quot;, package = &quot;faraway&quot;) skimr::skim(eggs) # Skim summary statistics # n obs: 48 # n variables: 4 # # ── Variable type:factor ───────────────────────────────────────────────────────────────────────────────────────────────────── # variable missing complete n n_unique top_counts # Lab 0 48 48 6 I: 8, II: 8, III: 8, IV: 8 # Sample 0 48 48 2 G: 24, H: 24, NA: 0 # Technician 0 48 48 2 one: 24, two: 24, NA: 0 # ordered # FALSE # FALSE # FALSE # # ── Variable type:numeric ──────────────────────────────────────────────────────────────────────────────────────────────────── # variable missing complete n mean sd p0 p25 p50 p75 p100 hist # Fat 0 48 48 0.39 0.15 0.06 0.31 0.37 0.43 0.8 ▁▂▃▇▁▁▁▁ Commençons par corriger l’encodage erroné des techniciens qui ferait penser que seulement deux personnes ont travaillé dans l’ensemble des six laboratoires. eggs %&gt;.% mutate(., Technician = interaction(Lab, Technician)) -&gt; eggs skimr::skim(eggs) # Skim summary statistics # n obs: 48 # n variables: 4 # # ── Variable type:factor ───────────────────────────────────────────────────────────────────────────────────────────────────── # variable missing complete n n_unique # Lab 0 48 48 6 # Sample 0 48 48 2 # Technician 0 48 48 12 # top_counts ordered # I: 8, II: 8, III: 8, IV: 8 FALSE # G: 24, H: 24, NA: 0 FALSE # I.o: 4, II FALSE # # ── Variable type:numeric ──────────────────────────────────────────────────────────────────────────────────────────────────── # variable missing complete n mean sd p0 p25 p50 p75 p100 hist # Fat 0 48 48 0.39 0.15 0.06 0.31 0.37 0.43 0.8 ▁▂▃▇▁▁▁▁ Nous avons à présent douze techniciens notés I.one, I.two, II.one, II.two, … Nous pouvons visualiser ces données. Comme nous n’avons que quatre réplicats par technicien, nous nous limitons à la représentation des observations de départ et des moyennes. chart(data = eggs, Fat ~ Lab %col=% Technician) + geom_jitter(width = 0.05, alpha = 0.5) + geom_point(data = group_by(eggs, Lab, Technician) %&gt;.% summarise(., means = mean(Fat, na.rm = TRUE)), f_aes(means ~ Lab), size = 3, col = &quot;red&quot;) Figure 11.5: Mesures de fractions en matières grasses dans des oeufs dans six laboratoires, par douze techniciens différents. Les points rouges sont les moyennes par technicien. Vérifions l’homoscédasticité. Ici, il suffit de considérer la variable Technician (une fois correctement encodée !) parce que dans cette forme de modèle, le facteur qui est imbriqué (Technician) est celui à partir duquel les résidus sont calculés. Nous utiliserons un seuil \\(\\alpha\\) classique de 5% pour l’ensemble de nos tests dans cette étude. bartlett.test(data = eggs, Fat ~ Technician) # # Bartlett test of homogeneity of variances # # data: Fat by Technician # Bartlett&#39;s K-squared = 13.891, df = 11, p-value = 0.2391 Avec une valeur p de 23,9%, nous pouvons considérer qu’il y a homoscédasticité. Voilà l’ANOVA (utilisez le “snippet” two-way ANOVA (nested model) le menu contextuel hypothesis tests: means que vous obtenez en tapant .hm). anova(anova. &lt;- lm(data = eggs, Fat ~ Lab + Technician %in% Lab)) # Analysis of Variance Table # # Response: Fat # Df Sum Sq Mean Sq F value Pr(&gt;F) # Lab 5 0.44303 0.088605 9.5904 6.989e-06 *** # Lab:Technician 6 0.24747 0.041246 4.4644 0.001786 ** # Residuals 36 0.33260 0.009239 # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Nous voyons que, dans le cas présent, l’effet technicien ne peut pas être testé. Nous avons l’effet labo et les interactions entre les techniciens et les labos qui sont présentés. Les deux sont significatifs ici. Nous avons à la fois des différences significatives qui apparaissent entre labos, mais aussi, des variation d’un labo à l’autre entre techniciens (interactions). Nous devons maintenant vérifier la distribution normale des résidus dans ce modèle (Fig. 11.6). Ici rien à redire, la distribution est conforme à nos attentes. Figure 11.6: Graphique quantile-quantile des résidus pour l’ANOVA à deux facteurs hiérarchisés pour la variable Fat du jeu de données eggs. # [1] 4 25 L’effet qui nous intéresse en priorité est l’effet laboratoire. Effectuons des tests “post hoc” sur cet effet pour déterminer quel(s) laboratoire(s) diffèrent entre eux. Le code que nous utilisons habituellement ne fonctionne pas dans le cas d’un modèle hiérarchisé, mais nous pouvons utiliser la fonction TukeyHSD() à la place, en partant d’un modèle similaire créé à l’aide de la fonction aov(). aov. &lt;- aov(data = eggs, Fat ~ Lab + Technician %in% Lab) (anovaComp. &lt;- TukeyHSD(aov., &quot;Lab&quot;)) # Tukey multiple comparisons of means # 95% family-wise confidence level # # Fit: aov(formula = Fat ~ Lab + Technician %in% Lab, data = eggs) # # $Lab # diff lwr upr p adj # II-I -0.24000 -0.38459081 -0.095409195 0.0002088 # III-I -0.17250 -0.31709081 -0.027909195 0.0116356 # IV-I -0.20375 -0.34834081 -0.059159195 0.0019225 # V-I -0.22625 -0.37084081 -0.081659195 0.0004902 # VI-I -0.31250 -0.45709081 -0.167909195 0.0000021 # III-II 0.06750 -0.07709081 0.212090805 0.7240821 # IV-II 0.03625 -0.10834081 0.180840805 0.9733269 # V-II 0.01375 -0.13084081 0.158340805 0.9997181 # VI-II -0.07250 -0.21709081 0.072090805 0.6611505 # IV-III -0.03125 -0.17584081 0.113340805 0.9861403 # V-III -0.05375 -0.19834081 0.090840805 0.8705387 # VI-III -0.14000 -0.28459081 0.004590805 0.0624025 # V-IV -0.02250 -0.16709081 0.122090805 0.9969635 # VI-IV -0.10875 -0.25334081 0.035840805 0.2356038 # VI-V -0.08625 -0.23084081 0.058340805 0.4817411 plot(anovaComp.) Nous pouvons observer des différences significatives au seuil \\(\\alpha\\) de 5% entre le labo I et tous les autres labos. Les autres comparaisons n’apparaissent pas significatives. 11.4.1 Simplification du modèle Nous pourrions être tentés de simplifier notre analyse en ne testant que l’effet laboratoire. Dans ce cas, nous tomberions dans le piège de la pseudo-réplication. Nous pourrions aussi travailler sur la mesure moyenne des mesures pour chaque technicien. Du coup, nous aurions deux valeurs par laboratoire, chaque fois réalisée par un technicien différent. Nous pourrions donc considérer que les données sont indépendantes les unes des autres et nous pourrions réduite le problème à un effet unique, celui du laboratoire. Si nous n’avons plus que deux mesures par laboratoire au lieu de deux fois quatre, nous gagnons d’un autre côté puisque l’écart type de la moyenne d’un échantillon et l’écart type de la population divisé par la racine carré de n. Donc, l’écart type sur les mesures moyennes est alors deux fois plus faible, ce qui se répercutera de manière positive sur l’ANOVA. La distribution des résidus sera une distribution de Student, mais elle est symétrique et pas trop différente d’une distribution normale. Cela pourrait passer. Mais il se peut que la réduction de l’information soit telle que le test perde complètement sa puissance. Illustrons ce phénomène avec le jeu de données eggs. Nous créons le jeu de données eggs_means reprenant les moyennes des quatre mesures par techinicien dans la variable `Fat_mean. eggs %&gt;.% group_by(., Technician) %&gt;.% summarise(., Fat_mean = mean(Fat), Lab = unique(Lab)) -&gt; eggs_means skimr::skim(eggs_means) # Skim summary statistics # n obs: 12 # n variables: 3 # # ── Variable type:factor ───────────────────────────────────────────────────────────────────────────────────────────────────── # variable missing complete n n_unique # Lab 0 12 12 6 # Technician 0 12 12 12 # top_counts ordered # I: 2, II: 2, III: 2, IV: 2 FALSE # I.o: 1, II FALSE # # ── Variable type:numeric ──────────────────────────────────────────────────────────────────────────────────────────────────── # variable missing complete n mean sd p0 p25 p50 p75 p100 hist # Fat_mean 0 12 12 0.39 0.13 0.17 0.36 0.37 0.39 0.72 ▁▁▇▂▁▁▁▁ eggs_means %&gt;.% group_by(., Lab) %&gt;.% summarise(., mean = mean(Fat_mean), sd = sd(Fat_mean), count = sum(!is.na(Fat_mean))) # # A tibble: 6 x 4 # Lab mean sd count # &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; # 1 I 0.58 0.202 2 # 2 II 0.34 0.0354 2 # 3 III 0.408 0.0530 2 # 4 IV 0.376 0.00177 2 # 5 V 0.354 0.00884 2 # 6 VI 0.267 0.131 2 Représentation graphique et vérification de l’homoscédasticité. chart(eggs_means, Fat_mean ~ Lab) + geom_point() + geom_point(data = group_by(eggs_means, Lab) %&gt;.% summarise(., means = mean(Fat_mean, na.rm = TRUE)), f_aes(means ~ Lab), size = 3, col = &quot;red&quot;) bartlett.test(data = eggs_means, Fat_mean ~ Lab) # # Bartlett test of homogeneity of variances # # data: Fat_mean by Lab # Bartlett&#39;s K-squared = 10.452, df = 5, p-value = 0.0634 Analyse de variance à un facteur. anova(anova. &lt;- lm(data = eggs_means, Fat_mean ~ Lab)) # Analysis of Variance Table # # Response: Fat_mean # Df Sum Sq Mean Sq F value Pr(&gt;F) # Lab 5 0.110756 0.022151 2.1482 0.1895 # Residuals 6 0.061869 0.010311 anova. %&gt;.% broom::augment(.) %&gt;.% car::qqPlot(.$.std.resid, distribution = &quot;norm&quot;, envelope = 0.95, col = &quot;Black&quot;, xlab = &quot;Quantiles théoriques (distri. normale)&quot;, ylab = &quot;Résidus standardisés&quot;) # [1] 1 7 Nous n’avons plus d’effet significatif, malgré que le labo I obtient, en moyenne, une mesure beaucoup plus forte que les autres. En fait, en réduisant de la sorte nos données, nous avons perdu tellement d’information que le test a perdu toute puissance et n’est plus capable de détecter de manière significative des différences entre moyennes pourtant importantes. Si nous avions quatre techniciens par labo qui auraient tous dosés les échantillons en duplicats (également huit mesures par labo au total), nous n’aurions pas une perte d’information aussi forte en effectuant quatre moyennes de duplicats par labo, et l’analyse simplifiée aurait peut-être été utilisable. Il faut voir au cas par cas… Conditions d’application Les conditions d’application sont les mêmes que pour l’ANOVA à deux facteurs sans interactions, sauf qu’ici, les interactions sont bien évidemment incluses dans le modèle par construction. La variable Sample valant G ou H ne sera pas utilisée ici. En fait, au départ, les initiateurs de l’expérience ont fait croire aux laboratoires qu’il s’agissait de deux échantillons différents alors que c’est le même en réalité.↩ "],
["effet-aleatoire.html", "11.5 Effet aléatoire", " 11.5 Effet aléatoire Jusqu’à présent, nous avons considéré que nous échantillonnons toutes les modalités qui nous intéressent pour les variables facteurs explicatives. Il se peut que les modalités soient trop nombreuses et que nous ne puissions n’en étudier qu’une petite fraction. Nous avons deux possibilités. Soit nous choisissons aléatoirement quelques modalités, et nous les étudions systématiquement pour les différentes modalités de l’autre variable. Nous nous ramenons à un modèle à facteurs fixes mais nous ne pouvons donner une réponse que pour les modalités échantillonnées (restriction de la population statistique étudiée). Soit, nous échantillonnons aléatoirement dans la population un nombre restreint de modalités. Considérez un plan d’expérience relativement classique où différents items sont comparés dans une sélection au hasard de réalisations. En agronomie, il peut s’agir de comparer différentes variétés d’une céréale cultivées dans quelques fermes. Si nous ne sommes intéressés que par ces fermes-là, alors les effets sont fixes, pas de problèmes. Par contre, si nous sommes intéressés par la production de cette céréale dans une région donnée, les fermes représentent seulement un échantillon de l’ensemble des fermes qui s’y trouvent. Autrement dit, toutes les modalités possibles ne sont pas reprises dans l’expérience. Dans ce cas, nous parlerons d’un effet aléatoire pour l’effet ‘ferme’. Prenons un exemple concret. Dans une analyse réalisée en 1972 par Davies and Goldsmith, six lots différents de pénicilline provenant d’une production de ce médicament sont comparés afin d’estimer la variabilité de l’efficacité antibiotique en fonction du lot de production. La mesure consiste à incuber une bactérie Bacillus subtilis dans une série de boites de Pétri (variable plate). Dans chaque boite, un volume de solution des six lots de pénicilline (variable sample) sont placés à espacement réguler. L’effet de l’antibiotique est mesurée via le diamètre de la zone de non croissance de la bactérie autour du point d’injection (variable diameter). pen &lt;- read(&quot;Penicillin&quot;, package = &quot;lme4&quot;) pen # # A tibble: 144 x 3 # diameter plate sample # &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; # 1 27 a A # 2 23 a B # 3 26 a C # 4 23 a D # 5 23 a E # 6 21 a F # 7 27 b A # 8 23 b B # 9 26 b C # 10 23 b D # # … with 134 more rows Étant donné que chacun des six lots d’antibiotiques est testé dans chaque boite de Pétri, nous serions tentés de penser que nous avons un plan croisé classique. La fonction replications() permet de déterminer rapidement le nombre de réplicats dans un plan d’expérience. Il s’utilise avec les mêmes données et la même formule que celle qu’on utiliserait pour faire l’ANOVA. Ici, nos données sont balancées, donc idéales avec 24 réplicats pour chaque lot (sample) et 6 mesures par boite de Pétri (plate), mais nous n’avons qu’une seule mesure par lot et par plaque. Cela nous empêche d’étudier les interactions éventuelles (si les boites de Pétri sont produites de manière identiques et placées au hasard dans un incubateur fournissant un environnement homogène, il y a peu de chances qu’il y ait de telles interactions). replications(data = pen, diameter ~ sample + plate) # sample plate # 24 6 Visualisons ces données. En absence de réplicats pour chaque niveau plate par sample, le graphique suivant offre une bonne vision d’ensemble des données : chart(data = pen, diameter ~ plate | sample) + geom_point() Sur base du graphique, nous pouvons observer des différences entre lots (le ‘F’ semble moins efficace, alors que les lots ‘A’ et ‘C’ montrent le plus grand diamètre d’action). De plus, des variations d’une boite de Pétri à l’autre sont observables. Par exemple, la boite ‘g’ montre des résultats faibles partout. Si nous considérons sample et plate comme facteurs fixes, nous serions tentés d’utiliser une ANOVA à deux facteurs classique sans interactions. Ici, nous n’avons pas de test de variance qui prenne simultanément deux facteurs en compte. En absence d’interactions entre les deux facteurs, nous pouvons toujours réaliser deux tests séparés mais cela reste du domaine du “bidouillage” (nous verrons une meilleure approche via l’analyse des résidus plus loin) : bartlett.test(data = pen, diameter ~ sample) # # Bartlett test of homogeneity of variances # # data: diameter by sample # Bartlett&#39;s K-squared = 0.56002, df = 5, p-value = 0.9898 bartlett.test(data = pen, diameter ~ plate) # # Bartlett test of homogeneity of variances # # data: diameter by plate # Bartlett&#39;s K-squared = 3.4151, df = 23, p-value = 1 Ici, ni le graphique, ni les tests de Batlett ne montrent une quelconque hétérogénéité des variances. Nous pouvons poursuivre avec l’ANOVA classique… anova(anova. &lt;- lm(data = pen, diameter ~ sample + plate)) # Analysis of Variance Table # # Response: diameter # Df Sum Sq Mean Sq F value Pr(&gt;F) # sample 5 449.22 89.844 297.089 &lt; 2.2e-16 *** # plate 23 105.89 4.604 15.224 &lt; 2.2e-16 *** # Residuals 115 34.78 0.302 # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Les deux facteurs sont significatifs au seuil \\(\\alpha\\) de 5%. Pour finir, une analyse des résidus confirme que les conditions d’application de l’ANOVA sont rencontrées : anova. %&gt;.% broom::augment(.) %&gt;.% car::qqPlot(.$.std.resid, distribution = &quot;norm&quot;, envelope = 0.95, col = &quot;Black&quot;, xlab = &quot;Quantiles théoriques (distri. normale)&quot;, ylab = &quot;Résidus standardisés&quot;) # [1] 137 14 Nous pouvons ensuite vérifier visuellement l’égalité des variances (bien mieux que le bricolage avec les deux tests de Batlett) avec un graphique issu du snippet .mlplot1 ou ... &gt; models &gt; linear &gt; plot residuals versus fitted [chart, broom] qui permet de visualiser sur l’axe des ordonnées l’étalement des résidus en fonction des valeurs prédites par le modèle sur l’axe des abscisses. Cet étalement doit être relativement homogène (avec la ligne de tendance qui se rapproche de zéro) comme c’est le cas ici : #plot(anova., which = 1) anova. %&gt;.% chart(broom::augment(.), .resid ~ .fitted) + geom_point() + geom_hline(yintercept = 0) + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = &quot;Residuals&quot;) + ggtitle(&quot;Residuals vs Fitted&quot;) Au final, les effets sample et plate apparaissent tous deux significatifs. Mais en considérant l’effet plate comme fixe, nous ne pouvons considérer que les résultats pour ces boites de Pétri-là, et aucunes autres. Cependant, il est évident que les 24 boites de Pétri ont été prises au hasard dans le lot de boites disponibles et que nous souhaitons interpréter l’analyse quelles que soient les boites de Pétri utilisées. Ainsi, nous considèrerons maintenant l’effet plate comme un effet aléatoire. Cela signifie que nous considérons une réponse plus générale suivant une distribution Normale pour les boites de Pétri. Un modèle sans interactions avec un effet aléatoire s’écrit dès lors : \\[y_{ijk} = \\mu + \\tau1_j + \\tau2_k + \\epsilon_i \\mathrm{\\ avec\\ } \\tau2_k \\sim N(0, \\sigma_{\\tau2}) \\mathrm{\\ et\\ } \\epsilon_i \\sim N(0, \\sigma) \\] L’équation du modèle n’a pas changé, mais nous avons maintenant un terme aléatoire supplémentaire, \\(\\tau2_k\\) dont il faudra tenir compte dans les calculs. Les hypothèses nulle et alternative pour ce facteur s’écrivent également différemment. Nous n’indiquons plus quelles moyennes de toutes les modalités sont égales (il peut éventuellement y en avoir une infinité possibles). Sous \\(H_0\\), l’effet lié aux boites de Pétri est null. Ceci sera obtenu lorsque l’écart type de la distribution Normale associée (\\(\\sigma_{\\tau2}\\)) est lui-même nul : \\(H_0: \\sigma_{\\tau2} = 0\\) \\(H_1: \\sigma_{\\tau2} &gt; 0\\) Dans R, la fonction lm() utilisée jusqu’ici ne prend pas en compte les facteurs aléatoires. Il existe plusieurs implémentations différentes, par exemple avec aov(), nlme::lme() ou encore lme4::lmer()/lmeTest::lmer(). De plus, ces modèles sont rendus plus difficiles car les spécialistes considèrent que la valeur p n’est calculable que dans certains cas bien précis. C’est la raison pour laquelle lme4::lmer() calcule le modèle, mais ne renvoie aucune valeur p. De plus, chaque fonction utilise une formulation différente, et renvoie les résultats différemment. Il est important d’avoir cela en tête, car si vous rechercher de la documentation concernant les modèles à facteurs aléatoires dans R sur Internet, vous risquez de vous perdre dans les différentes implémentations et explications… si ce n’est à considérer que différents points de vue co-existent actuellement ! Dans la suite, nous verrons deux modèles aléatoires particuliers qui ont une configuration permettant un calcul correct des valeurs de p : le modèle en parcelles divisées (split-plot) et le modèle à mesures répétées. 11.5.1 Modèle en parcelles divisées (split-plot) Un des modèles pour lequel les valeurs p sont calculables est le modèle dit en parcelles divisées (split-plot en anglais). Cette situation apparait lorsque, parmi deux facteurs, l’un est plus difficile à diviser en sous-unités indépendantes. C’est le cas ici avec nos boites de Pétri qui forment chacune un petit microcosme unique. Il est possible de diviser la boite de Pétri en sous-régions. C’est ce qui a été fait ici pour tester les six lots, mais ces sous-régions ne sont pas indépendantes les unes des autres, elles continuent d’être liées entre elle au sein de la même boite de Pétri. Nous pouvons néanmoins considérer la boite de Pétri comme facteur aléatoire dans ce modèle split-plot. Dans ce cas, nous avons intérêt à utiliser lmeTest::lmer() qui calculera ces valeurs p là où elles sont pertinentes57. Dans la formule, un facteur aléatoire simple s’écrit (1 | facteur). Cela donne : split_plot &lt;- lmerTest::lmer(data = pen, diameter ~ sample + (1 | plate)) split_plot # Linear mixed model fit by REML [&#39;lmerModLmerTest&#39;] # Formula: diameter ~ sample + (1 | plate) # Data: pen # REML criterion at convergence: 308.2793 # Random effects: # Groups Name Std.Dev. # plate (Intercept) 0.8467 # Residual 0.5499 # Number of obs: 144, groups: plate, 24 # Fixed Effects: # (Intercept) sampleB sampleC sampleD sampleE # 25.167 -3.208 -0.250 -2.292 -2.208 # sampleF # -5.208 Attention, contrairement à lm() qui ajuste automatiquement une ANOVA sur des données qualitatives de type factor ou ordered, lmer() ne fait pas cela et considère toujours un autre modèle : une régression linéaire. Nous aborderons ce dernier modèle l’an prochain. Heureusement, la fonction anova() peut être appliquée ensuite pour obtenir la table de l’ANOVA : anova(split_plot) # Type III Analysis of Variance Table with Satterthwaite&#39;s method # Sum Sq Mean Sq NumDF DenDF F value Pr(&gt;F) # sample 449.22 89.844 5 115 297.09 &lt; 2.2e-16 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Le titre (“Type III … Sattertwaite’s method”) indique que ce n’est pas une ANOVA classique qui a été calculée. Nous n’entrerons pas dans les détails. En tous cas, vous ne voyez qu’une seule ligne dans ce tableau, relative au facteur fixe du modèle, sample. L’effet est ici significatif au seuil \\(\\alpha\\) de 5%. C’est perturbant. Nous nous attendions à avoir trois lignes comme plus haut (deux facteurs fixes avec lm()) et à pouvoir interpréter aussi le facteur plate à partir de la table de l’ANOVA. Mais rappelons-nous que plate suit maintenant une distribution Normale dont nous devons estimer l’écart type \\(\\sigma_{\\tau2}\\). Cette dernière valeur est indiquée plus haut dans l’impression de l’objet split_plot. Retournez voir : nous avons une section Random effects: avec un groupe plate et en regard, dans la colonne Std.dev., nous avons la valeur de \\(\\sigma_{\\tau2}\\) qui vaut 0,85. Comment décider si rejeter \\(H_0\\) ou non par rapport à plate ? Une façon de faire est de calculer l’intervalle de confiance à (1 - \\(\\alpha\\)) sur ce paramètre, et de vérifier s’il contient zéro (alors nous ne rejettons pas \\(H_0\\)) ou non (nous rejettons). La fonction confint() peut être utilisée pour calculer cet intervalle de condiance. Le calcul est un peu long avec la méthode par défaut, voir ?lme4::confint.merMod : confint(split_plot) # Computing profile confidence intervals ... # 2.5 % 97.5 % # .sig01 0.6243175 1.15107599 # .sigma 0.4768022 0.61436044 # (Intercept) 24.7615793 25.57175404 # sampleB -3.5153786 -2.90128807 # sampleC -0.5570453 0.05704527 # sampleD -2.5987119 -1.98462140 # sampleE -2.5153786 -1.90128807 # sampleF -5.5153786 -4.90128807 L’intervalle de confiance qui nous intéresse est le premier, nommé .sig01. Le reste est relatif à un modèle linéaire que nous étudierons l’an prochain. Ignorons-les pour l’instant. Donc, notre intervalle de confiance va de 0,62 à 1,15. Cela signifie que zéro n’est pas compris dans l’intervalle. Nous rejettons \\(H_0\\) et nous concluons que le facteur plate est significatif au seuil \\(\\alpha\\) de 5%. Le travail d’interprétation du facteur aléatoire plate est terminé, mais il nous reste à présent à réaliser un test post-hoc de comparaisons multiples sur le facteur fixe sample. Cela se réalise de la même façon qu’avec l’ANOVA classique (code issu du snippet .hmanovamult ou ... &gt; hypothesis test &gt; means &gt; anova - multiple comparisons [multcomp]) : summary(anovaComp. &lt;- confint(multcomp::glht(split_plot, linfct = multcomp::mcp(sample = &quot;Tukey&quot;)))) # Add a second factor if you want # # Simultaneous Tests for General Linear Hypotheses # # Multiple Comparisons of Means: Tukey Contrasts # # # Fit: lmerTest::lmer(formula = diameter ~ sample + (1 | plate), data = pen) # # Linear Hypotheses: # Estimate Std. Error z value Pr(&gt;|z|) # B - A == 0 -3.20833 0.15875 -20.210 &lt;1e-04 *** # C - A == 0 -0.25000 0.15875 -1.575 0.615 # D - A == 0 -2.29167 0.15875 -14.436 &lt;1e-04 *** # E - A == 0 -2.20833 0.15875 -13.911 &lt;1e-04 *** # F - A == 0 -5.20833 0.15875 -32.809 &lt;1e-04 *** # C - B == 0 2.95833 0.15875 18.635 &lt;1e-04 *** # D - B == 0 0.91667 0.15875 5.774 &lt;1e-04 *** # E - B == 0 1.00000 0.15875 6.299 &lt;1e-04 *** # F - B == 0 -2.00000 0.15875 -12.599 &lt;1e-04 *** # D - C == 0 -2.04167 0.15875 -12.861 &lt;1e-04 *** # E - C == 0 -1.95833 0.15875 -12.336 &lt;1e-04 *** # F - C == 0 -4.95833 0.15875 -31.234 &lt;1e-04 *** # E - D == 0 0.08333 0.15875 0.525 0.995 # F - D == 0 -2.91667 0.15875 -18.373 &lt;1e-04 *** # F - E == 0 -3.00000 0.15875 -18.898 &lt;1e-04 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # (Adjusted p values reported -- single-step method) .oma &lt;- par(oma = c(0, 5.1, 0, 0)); plot(anovaComp.); par(.oma); rm(.oma) Ici, tous les lots diffèrent, sauf A-C et E-D. Si la première modalité était une situation de référence, nous aurions aussi pu utiliser la comparaison multiple de Dunnet en remplaçant &quot;Tukey&quot; par &quot;Dunnet&quot; dans le code ci-dessus. Dans ce cas, toutes les comparaisons deux à deux ne seraient pas réalisées, seulement les comparaisons au témoin, donc à la modalité de référence. N’oublions par de réaliser une petite analyse des résidus de ce modèle pour vérifier qu’ils ont bien une distribution normale et que les variances sont homogènes (homoscédasticité). split_plot %&gt;.% broom::augment(.) %&gt;.% car::qqPlot(.$.resid, distribution = &quot;norm&quot;, envelope = 0.95, col = &quot;Black&quot;, xlab = &quot;Quantiles théoriques (distri. normale)&quot;, ylab = &quot;Résidus standardisés&quot;) # [1] 137 14 #plot(split_plot, which = 1) split_plot %&gt;.% chart(broom::augment(.), .resid ~ .fitted) + geom_point() + geom_hline(yintercept = 0) + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = &quot;Residuals&quot;) + ggtitle(&quot;Residuals vs Fitted&quot;) Vous pouvez constater via ces graphiques que les résidus sont les mêmes que pour le modèle fixe. Nous concluons donc, encore une fois, que la distribution des résidus est compatible avec le modèle. Maintenant, si nous considérons que sample est également un facteur aléatoire, car nous avons prélevé six lots au hasard parmi tous les lots produits par la firme pharmaceutique, alors nous devrons utiliser le modèle suivant utilisant deux facteurs aléatoires : \\[y_{ijk} = \\mu + \\tau1_j + \\tau2_k + \\epsilon_i \\mathrm{\\ avec\\ } \\tau1_j \\sim N(0, \\sigma_{\\tau1}) \\mathrm{, } \\tau2_k \\sim N(0, \\sigma_{\\tau2}) \\mathrm{\\ et\\ } \\epsilon_i \\sim N(0, \\sigma) \\] split_plot2 &lt;- lmerTest::lmer(data = pen, diameter ~ (1 | sample) + (1 | plate)) split_plot2 # Linear mixed model fit by REML [&#39;lmerModLmerTest&#39;] # Formula: diameter ~ (1 | sample) + (1 | plate) # Data: pen # REML criterion at convergence: 330.8606 # Random effects: # Groups Name Std.Dev. # plate (Intercept) 0.8467 # sample (Intercept) 1.9316 # Residual 0.5499 # Number of obs: 144, groups: plate, 24; sample, 6 # Fixed Effects: # (Intercept) # 22.97 Inutile d’utiliser anova() ici puisqu’il n’y a plus aucun facteur fixe. Nous interprétons les deux facteurs à l’aide de leur écart types respectifs calculés comme 0,85 toujours pour plate et 1,93 pour sample. Calculons les intervalles de confiance sur ces écart types. confint(split_plot2) # Computing profile confidence intervals ... # 2.5 % 97.5 % # .sig01 0.6335665 1.1821040 # .sig02 1.0957897 3.5562909 # .sigma 0.4858454 0.6294535 # (Intercept) 21.2666276 24.6778119 La première ligne .sig01 est relative à l’écart type plate avec un intervalle de confiance qui va de 0,63 à 1,18, et la seconde ligne .sig02 est relative à l’écart type de sample dont l’intervalle de confiance s’étale de 1,10 à 3,56. Dans les deux cas, zéro n’est pas compris dans l’intervalle de confiance et nous rejettons donc \\(H_0\\). A noter que si nous n’avions pas rejeté l’un des deux, nous aurions pu décider d’ajuster un modèle plus simple à un seul facteur aléatoire y ~ (1 | fact) en laisant tomber l’autre facteur dans le modèle simplifié. L’analyse des résidus (indispensable) n’est pas reproduite ici. Elle reste la même que plus haut. 11.5.2 Modèle à mesures répétées Un autre type de modèle aléatoire courant est le modèle à mesures répétées. Celui-ci se produit lorsque nous mesurons plusieurs fois les mêmes individus, par exemple, successivement dans le temps. Considérons l’expérience suivante. Dix huit volontaires (variable Subject) ont subi une privation de sommeil (restreint à 3h par 24h) pendant 9 jours (variable Days). Au jour zéro, ils ont eu un sommeil normal. Leur temps de réaction est mesuré en ms à l’aide d’un test standardisé. sleep &lt;- read(&quot;sleepstudy&quot;, package = &quot;lme4&quot;) sleep # # A tibble: 180 x 3 # Reaction Days Subject # &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; # 1 250. 0 308 # 2 259. 1 308 # 3 251. 2 308 # 4 321. 3 308 # 5 357. 4 308 # 6 415. 5 308 # 7 382. 6 308 # 8 290. 7 308 # 9 431. 8 308 # 10 466. 9 308 # # … with 170 more rows skimr::skim(sleep) # Skim summary statistics # n obs: 180 # n variables: 3 # # ── Variable type:factor ───────────────────────────────────────────────────────────────────────────────────────────────────── # variable missing complete n n_unique top_counts # Subject 0 180 180 18 308: 10, 309: 10, 310: 10, 330: 10 # ordered # FALSE # # ── Variable type:numeric ──────────────────────────────────────────────────────────────────────────────────────────────────── # variable missing complete n mean sd p0 p25 p50 p75 # Days 0 180 180 4.5 2.88 0 2 4.5 7 # Reaction 0 180 180 298.51 56.33 194.33 255.38 288.65 336.75 # p100 hist # 9 ▇▃▃▃▃▃▃▇ # 466.35 ▂▇▇▆▆▃▁▁ Un graphique pertinent ici relie les points relatifs aux tests de chaque patient afin de bien montrer le lien entre les mesures issues des mêmes sujets, soit un graphique de ce type : chart(data = sleep, Reaction ~ Days %col=% Subject) + geom_line() Nous pouvons aussi utiliser des facettes si ce graphique est trop encombré : chart(data = sleep, Reaction ~ Days | Subject) + geom_line() Comme nous pouvons nous y attendre, le temps de réaction semble augmenter en fonction de la déprivation de sommeil, mais un effet individuel est possible. Attention : résistez à la tentation de représenter ici les valeurs moyennes par jour de temps de réaction. Il est important de conserver la continuité temporelle individu par individu sur le graphique en reliant les points relatifs à chaque patient comme ci-dessus. Nous pouvons, par contre, faire la supposition que la varaition du temps de réaction soit linéaire, et le représenter graphiquement comme suit : chart(data = sleep, Reaction ~ Days | Subject) + geom_point() + stat_smooth(method = &quot;lm&quot;) # Ajuste une droite sur les données C’est ce dernier modèle que nous allons considérer. Contrairement au modèle split_plot précédent, notez que les facteurs ne sont pas croisés ici, mais les mesures répétées dans le temps sont imbriquées dans la variable Subject. Nous avons, en quelque sorte, un modèle qui est à la fois hiérarchisé et mixte, donc, contenant un facteur fixe Days et un facteur aléatoire Subject. Le modèle correspondant est : \\[y_{ijk} = \\mu + \\tau1_j + \\tau2_k(\\tau1_j) + \\epsilon_i \\mathrm{\\ avec\\ } \\tau2_k \\sim N(0, \\sigma_{\\tau2}) \\mathrm{\\ et\\ } \\epsilon_i \\sim N(0, \\sigma) \\] Avec lmerTest::lmer(), cela donne (notez la formulation particulière (fact1 | fact 2) pour marquer l’imbrication) : repeated &lt;- lmerTest::lmer(data = sleep, Reaction ~ Days + (Days | Subject)) repeated # Linear mixed model fit by REML [&#39;lmerModLmerTest&#39;] # Formula: Reaction ~ Days + (Days | Subject) # Data: sleep # REML criterion at convergence: 1743.628 # Random effects: # Groups Name Std.Dev. Corr # Subject (Intercept) 24.737 # Days 5.923 0.07 # Residual 25.592 # Number of obs: 180, groups: Subject, 18 # Fixed Effects: # (Intercept) Days # 251.41 10.47 anova(repeated) # Type III Analysis of Variance Table with Satterthwaite&#39;s method # Sum Sq Mean Sq NumDF DenDF F value Pr(&gt;F) # Days 30024 30024 1 16.995 45.843 3.273e-06 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 L’effet des jours (Days) est significatif au seuil \\(\\alpha\\) de 5%. En réalité, comme nous l’avons déjà fait remarqué, il est plus judicieux de considérer une évolution linéaire au fil des jours de la vitesse de réaction. Nous résisterons donc à l’envie d’effectuer un test post-hoc deux à deux des jours, ce qui n’a pas de sens ici. La fonction summary() nous donne l’équation de la droite Reaction = a * Days + b : summary(repeated) # Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ # lmerModLmerTest] # Formula: Reaction ~ Days + (Days | Subject) # Data: sleep # # REML criterion at convergence: 1743.6 # # Scaled residuals: # Min 1Q Median 3Q Max # -3.9536 -0.4634 0.0231 0.4633 5.1793 # # Random effects: # Groups Name Variance Std.Dev. Corr # Subject (Intercept) 611.90 24.737 # Days 35.08 5.923 0.07 # Residual 654.94 25.592 # Number of obs: 180, groups: Subject, 18 # # Fixed effects: # Estimate Std. Error df t value Pr(&gt;|t|) # (Intercept) 251.405 6.824 17.005 36.843 &lt; 2e-16 *** # Days 10.467 1.546 16.995 6.771 3.27e-06 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Correlation of Fixed Effects: # (Intr) # Days -0.138 En lisant dans le sous-tableau Fixed effects:, nous avons a = 10,5 et b = 251,4 dans la colonne Estimate en regard de Days et de (Intercept), soit ordonnée à l’origine en anglais. Cela donne l’équation suivante : Réaction = 10,5 * Days + 251,4. Le temps de réaction moyen au jour zéro est de 251 ms, et l’augmentation est de 10,5 ms par jour, toujours en moyenne (mais nous anticipons ici sur ce qu’on appelle un modèle linéaire que nous étudierons plus en détails au cours de SDD II l’an prochain). Afin de déterminer si les variations entre sujets sont significatives, nous calculons les intervalles de confiance… confint(repeated) # Computing profile confidence intervals ... # 2.5 % 97.5 % # .sig01 14.3821281 37.7160076 # .sig02 -0.4815004 0.6849866 # .sig03 3.8011763 8.7536801 # .sigma 22.8982726 28.8579976 # (Intercept) 237.6806976 265.1295138 # Days 7.3586543 13.5759173 L’intervalle .sig01 se rapporte à l’écart type pour le temps de réaction au jour zéro. Cet écart type est de 24, 7 et son intervalle de confiance s’étale de 14,4 à 37,7. Puisque zéro n’est pas compris dans l’intervalle de confiance, la variation est donc significative au seuil \\(\\alpha\\) de 5%. Nous ne discuterons pas les autres termes pour l’instants qui se rapportent au modèle linéaire. L’analyse des résidus donne ceci : repeated %&gt;.% broom::augment(.) %&gt;.% car::qqPlot(.$.resid, distribution = &quot;norm&quot;, envelope = 0.95, col = &quot;Black&quot;, xlab = &quot;Quantiles théoriques (distri. normale)&quot;, ylab = &quot;Résidus standardisés&quot;) # [1] 57 8 #plot(repeated, which = 1) repeated %&gt;.% chart(broom::augment(.), .resid ~ .fitted) + geom_point() + geom_hline(yintercept = 0) + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = &quot;Residuals&quot;) + ggtitle(&quot;Residuals vs Fitted&quot;) Nous avons quelques valeurs extrêmes qui étaient déjà repérables sur les graphiques des données brutes. Il s’agit probablement de mesures erronées, mais à part cela, la distribution des résidus est correcte. Le modèle utilisé ici considère une variation du temps de réaction identique pour tous les sujets. L’inspection visuelle des données brutes montre que ce n’est peut-être pas le cas. Un modèle plus complexe permet de varier aussi ce paramètre. Il sort du cadre de ce cours, mais vous pouvez lire le développement complet expliqué par un des auteurs de la fonction lmer() ici. Au passage, il s’agit d’un excellent exemple de comparaison de deux modèles autrement que via les valeurs p, soit une approche maintenant préconisée mais malheureusement pas encore largement intégrée dans la pratique en biologie. Conditions d’application Pour tous les modèles contenant un ou plusieurs effets aléatoires, les conditions générale de l’ANOVA restent de mise : échantillon représentatif (par exemple, aléatoire), observations indépendantes, variable réponse quantitative, n variables explicatives qualitatives à deux niveaux ou plus, distribution normale des résidus \\(\\epsilon_i\\), homoscédasticité (même variance intragroupes), De plus, nous faisons l’hypothèse supplémentaire que les variables à effet aléatoire suivent une distribution Normale, mais nous ne pouvons pas facilement tester cette hypothèse à moins d’avoir un grand nombre de niveaux testés pour la variable, ce qui est très rarement le cas. Dans tous les cas, une analyse des résidus, en particulier un graphique quantile-quantile et un graphique des résidus par rapport aux valeurs ajustées sont indispensables pour confirmer que les conditions d’application sont rencontrées. Si ce n’est pas le cas, nous pouvons tenter de transformer les données. Sinon, nous devons utiliser une version non paramétrique du modèle si elle existe (e.g., test de Friedman), mais ce n’est pas toujours possible. Pour aller plus loin On s’y perd facilement dans les différents modèles et les formulations associées pour le code R avec lm(), aov() ou lmer(). Un récapitulatif (en anglais) est présenté ici. Une version plus détailler est aussi disponible. Dans le cas où les conditions d’application de l’ANOVA à mesures répétées ne sont pas rencontrées, nous pouvons utiliser un test non paramétrique équivalent : le test de Friedman. Notez toutefois qu’il est conseillé de manière générale de ne pas utiliser les valeurs p du tout, mais de privilégier d’autres approches comme le rapport de la log-vraissemblance entre modèles, voir ici, mais cela sort du cadre de ce cours.↩ "],
["syntaxe-de-r.html", "11.6 Syntaxe de R", " 11.6 Syntaxe de R Dans cette dernière section du module 11, nous revenons sur nos outils logiciels. C’est bien de connaître les techniques de visualisation et d’analyse des données, mais c’est encore mieux de pouvoir les appliquer en pratique, … et pour cela il faut maîtriser un ou plusieurs logiciels. Nous avons utilisé principalement R, via l’interface proposée par RStudio. Nous allons maintenant approfondir quelque peu nos connaissances en R. Vous avez pu vous rendre compte que R n’est pas qu’un logiciel de science des données. Il comprend également un langage informatique du même nom58 dédié à la manipulation, la visualisation et l’analyse des données. Les langages informatiques sont, pour la plupart, conçus autour de standards bien définis. Mais R est toujours resté volontairement très libertaire à ce niveau. Comme R permet également de presque tout redéfinir dans son langage, il est apparu au cours du temps différents styles. Ces styles co-existent et co-existeront encore longtemps car ils ont tous des avantages et des inconvénients, ce qui mène à des choix personnels de chaque utilisateur dictés par l’usage qu’il fait de R, ses goûts, son caractère, et comment il a été formé (à quelle “école” il appartient). Nous n’entrerons pas dans les débats stériles autant que passionnés autour de ces questions de styles. Il est cependant utile de comprendre ces différentes approches au moins dans leurs grandes lignes. Aujourd’hui, on peut distinguer principalement trois styles dans R, résumés dans un aide-mémoire : La syntaxe de base ou syntaxe dollar est celle héritée du langage S. Elle fait la part belle aux instructions d’indiçage comme v[i] ou df[i, j], et à l’opérateur dollar $ pour extraire, par exemple la variable x du data frame df à l’aide de df$x, d’où son nom. Une instruction type en R de base sera fun(df$x, df$y). L’objet central est le data.frame qui est conçu pour contenir un tableau de données de type cas par variables. La syntaxe formule qui utilise une formule (objet ressemblant à une équation mathématique et utilisant le tilde ~ pour séparer le membre de gauche et le membre de droite). Nous avons utilisé des formules dans le cadre de graphiques réalisés à l’aide de chart() ou pour nos ANOVAs par exemple. Une instruction se présente typiquement comme fun(y ~ x, data = df), même si nous préférons dans SciViews inverser les deux arguments et écrire fun(data = df, y ~ x). La syntaxe formule est souvent associée également à un data.frame (l’argument data =), même si une liste, un tibble (voir ci-dessous) ou tout autre objet similaire peut aussi souvent être utilisé. La syntaxe “tidyverse” qui vise à produire des instruction aussi lisibles que possible (lecture proche d’un langage naturel humain). Cette syntaxe utilise quelques astuces de programmation pour rendre les instructions plus digestes et s’affranchir du df$x pour pouvoir écrire uniquement x à la place. Elle fait également la part belle au chaînage des instructions (opérateur de “pipe”) que nous avons étudié à la fin du module 5. Une instruction type sera df %&gt;% fun(x, y), même si dans SciViews nous préférons l’opérateur plus explicite %&gt;.% qui donne df %&gt;.% fun(., x, y). Tidyverse est constitué d’un ensemble de packages R qui se conforme à un style bien défini autour d’une version spéciale de data frame appelée tibble. Dans le cadre de ce cours, nous utilisons un style propre à SciViews, le style SciViews-R. Il est associé à une série de packages chargés à partir de l’instruction SciViews::R. Ce style tente de rendre l’utilisation de R plus cohérente et plus facile en reprenant le meilleur des trois styles précédents, et en y ajoutant une touche personnelle élaborée pour éliminer autant que possible les erreurs fréquentes observées lors de l’apprentissage ou de l’utilisation de R. Dans la suite de cette section, nous allons comparer quelques instructions équivalentes, mais rédigées dans différents styles pour nous familiariser avec eux et pour comprendre leurs atouts respectifs. Assurez-vous de conserver à proximité l’aide-mémoire sur les différentes syntaxes de R pour vous aider. Mais avant toute chose, un petit complément concernant l’indiçage dans R est nécessaire. 11.6.1 Indiçage dans R Après l’opérateur particulier &lt;- d’assignation (associer une valeur à un nom dans un langage informatique), les diverses variantes de l’indiçage de vecteurs, matrices et data frames sont caractéristiques de R. Elles sont puissantes, mais nécessitent de s’y attarder un peu pour bien les comprendre. Les données et les calculs dans R sont vectorisés. Cela signifie que l’élément de base est un vecteur. Un vecteur est un ensemble d’éléments rangés dans un ordre bien défini, et éventuellement nommés. La longueur du vecteur s’obtient avec length(), et il est possible de spécifier un ou plusieurs éléments du vecteur à l’aide de l’opérateur []. # Un vecteur contenant 4 valeurs numériques nommées a, b, c et d v &lt;- c(a = 2.6, b = 7.1, c = 4.9, d = 5.0) # Le second élément du vecteur v[2] # b # 7.1 # Le premier et le troisième élément v[c(1, 3)] # a c # 2.6 4.9 # Les 3 premiers éléments avec la séquence 1, 2, 3 issue de 1:3 v[1:3] # a b c # 2.6 7.1 4.9 Nous venons d’indicer notre vecteur v à l’aide de la première forme possible qui consiste à utiliser des entiers positifs pour indiquer la position des éléments que nous souhaitons retenir. La seconde forme utilise des entiers négatifs pour éliminer les vecteurs aux positions correspondantes. # Tout le vecteur v, sauf le 2ème élément v[-2] # a c d # 2.6 4.9 5.0 # Elimination du 2ème et du 3ème élément v[-(2:3)] # a d # 2.6 5.0 # Elimination du dernier élément v[-length(v)] # a b c # 2.6 7.1 4.9 La troisième forme d’indiçage nécessite que le vecteur soit nommé, comme c’est le cas pour v. Nous pouvons alors indicer en utilisant des chaînes de caractères correspondant aux noms des différent éléments. # Elément de v s&#39;appelant &#39;a&#39; v[&#39;a&#39;] # a # 2.6 # Les éléments &#39;b&#39; et &#39;d&#39; v[c(&#39;b&#39;, &#39;d&#39;)] # b d # 7.1 5.0 Enfin, la quatrième forme d’indiçage utilise un vecteur booléen (TRUE ou FALSE ; on dit logical en R) de même longueur que le vecteur, sinon les valeurs sont recyclée à concurrence de la longueur du vecteur. On ne garde que les éléments correspondant à TRUE. # Garder le premier et le quatrième élément v[c(TRUE, FALSE, FALSE, TRUE)] # a d # 2.6 5.0 # Garder le premier et le troisième (recyclage des indices une seconde fois) v[c(TRUE, FALSE)] # a c # 2.6 4.9 La dernière instruction mérite une petite explication supplémentaire. Le vecteur logical d’indiçage utilisé ne contient que deux éléments alors que le vecteur v qui est indicé en contient quatre. Les valeurs logiques sont alors recyclées comme suit. Lorsqu’on arrive la fin, on reprend au début. Donc, TRUE, on garde le premier, FALSE on élimine le second, et puis… retour au début : TRUE on garde le troisième, FALSE on jette le quatrième, et ainsi de suite si v était plus long. Donc, un indiçage avec c(TRUE, FALSE) permet de ne garder que les éléments d’ordre impair quel que soit la longueur de v, et c(FALSE, TRUE) ne gardera que les éléments d’ordre pair. La puissance de l’indiçage par valeur booléenne est immense car nous pouvons aussi utiliser une expression qui renvoie de telles valeurs. C’est le cas notamment de toutes les instruction de comparaison (opérateurs égal à ==, différent de !=, plus grand que &gt;, plus petit que &lt;, plus grand ou égal &gt;= ou plus petit ou égal &lt;=). Par exemple pour ne garder que les valeurs plus grandes que 3, on fera : # Déterminer quel élément est plus grand que 3 dans v v &gt; 3 # a b c d # FALSE TRUE TRUE TRUE # Utilisation de cette instruction comme indiçage pour filtrer les éléments de v v[v &gt; 3] # b c d # 7.1 4.9 5.0 Les instructions de comparaison peuvent être combinées entre elles à l’aide des opérateurs “et” (&amp;) ou “ou” (|). Donc, pour récupérer les éléments qui sont plus grands que 3 et plus petits ou égaux à 5, on fera : v[v &gt; 3 &amp; v &lt;= 5] # c d # 4.9 5.0 Les indiçages fonctionnent aussi pour des objets bi- or multidimensionnels. Dans ce cas, nous utiliserons deux ou plusieurs instructions d’indiçage à l’intérieur des [], séparées par un virgule. Pour un tableau à deux dimensions comme une matrice ou un data frame, le premier élément sélectionne les lignes et le second les colonnes. df &lt;- tribble( ~x, ~y, ~z, 1, 2, 3, 4, 5, 6 ) df # # A tibble: 2 x 3 # x y z # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 1 2 3 # 2 4 5 6 # Elément à la première ligne, colonnes 2 et 3 df[1, 2:3] # # A tibble: 1 x 2 # y z # &lt;dbl&gt; &lt;dbl&gt; # 1 2 3 Pour conserver toutes les lignes et/ou toutes les colonnes, il suffit de laisser la position correspondante vide. # Toute la seconde ligne df[2, ] # # A tibble: 1 x 3 # x y z # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 4 5 6 # Toute la seconde colonne df[ , 2] # # A tibble: 2 x 1 # y # &lt;dbl&gt; # 1 2 # 2 5 # Tout le tableau (pas très utile !) df[ , ] # # A tibble: 2 x 3 # x y z # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 1 2 3 # 2 4 5 6 Les autres formes d’indiçage fonctionnent aussi. # Lignes pour lesquelles x est plus grand que 3 et colonnes nommée &#39;y&#39; et &#39;z&#39; df[df$x &gt; 3, c(&#39;y&#39;, &#39;z&#39;)] # # A tibble: 1 x 2 # y z # &lt;dbl&gt; &lt;dbl&gt; # 1 5 6 Notez bien que nous n’avons pas écrit df[x &gt; 3, ] mais df[df$x &gt; 3, ]. La première forme n’aurait pas utilisé la variable x du data frame df (notée df$x), mais aurait tenté d’utiliser un vecteur x directement. Ce qui nous amène à l’extraction d’un élément d’un tableau ou d’une liste à l’aide des opérateur [[]] ou $. Pour extraire la colonne y sous forme d’un vecteur de df, nous pourrons faire : df[[2]] # [1] 2 5 df[[&#39;y&#39;]] # [1] 2 5 df$y # [1] 2 5 Pour finir, l’indiçage peut aussi être réalisé à la gauche de l’opérateur d’assignation &lt;-. Dans ce cas, la partie concernée du vecteur, de la matrice ou du data frame est remplacée par la ou les valeurs de droite. # Remplacer la troisième colonne par des nouvelles valeurs df[ , 3] &lt;- c(-10, -15) df # # A tibble: 2 x 3 # x y z # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 1 2 -10 # 2 4 5 -15 # Ceci donne le même résultat df$z &lt;- c(-10, -15) df # # A tibble: 2 x 3 # x y z # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 1 2 -10 # 2 4 5 -15 Maintenant que nous sommes familiarisés avec les différents modes d’indiçage dans R de base, nous pouvons les comparer à d’autres styles. A vous de jouer Afin d’appliquer directement les concepts vus au cours dans ce module, ouvrez RStudio dans votre SciViews Box, puis exécutez l’instruction suivante dans la fenêtre console : BioDataScience::run(&quot;11b_syntaxr&quot;) 11.6.2 Comparaison de styles Nous allons réaliser quelques opérations simples sur un jeu de données cas par variables. Nous allons créer un tableau de contingence à une entrée, sélectionner des cas et restreindre les variables du tableau, calculer une nouvelle variable, pour finir avec une représentation graphique simple. Nous ferons le même travail en utilisant les différents styles de R à des fins de comparaison. Prenons le jeu de données zooplankton qui contient 19 mesures (variables numériques) effectuées sur des images de zooplancton, ainsi que le groupe taxonomique auquel les organismes appartiennent (variable facteur class). Nous avons toujours utilisé la fonction read() pour charger ce type de données. Cette fonction fait partie de SciViews::R. SciViews::R zoo &lt;- read(&quot;zooplankton&quot;, package = &quot;data.io&quot;, lang = &quot;FR&quot;) skimr::skim(zoo) # Skim summary statistics # n obs: 1262 # n variables: 20 # # ── Variable type:factor ───────────────────────────────────────────────────────────────────────────────────────────────────── # variable missing complete n n_unique # class 0 1262 1262 17 # top_counts ordered # Cal: 288, Poe: 158, Déc: 126, Mal: 121 FALSE # # ── Variable type:numeric ──────────────────────────────────────────────────────────────────────────────────────────────────── # variable missing complete n mean sd p0 p25 p50 # area 0 1262 1262 0.72 1.74 0.06 0.23 0.35 # aspect 0 1262 1262 0.54 0.24 0.059 0.35 0.52 # circularity 0 1262 1262 0.31 0.24 0.023 0.12 0.22 # compactness 0 1262 1262 6.24 5.43 1.1 2.35 4.46 # density 0 1262 1262 0.13 0.25 0.001 0.041 0.07 # ecd 0 1262 1262 0.81 0.51 0.28 0.54 0.67 # elongation 0 1262 1262 17.44 17.17 1 5.19 11.93 # feret 0 1262 1262 1.81 1.55 0.31 0.93 1.34 # major 0 1262 1262 1.33 1.28 0.29 0.74 0.91 # max 0 1262 1262 0.7 0.22 0.068 0.54 0.68 # mean 0 1262 1262 0.21 0.12 0.016 0.13 0.2 # min 0 1262 1262 0.006 0.0044 0.004 0.004 0.004 # minor 0 1262 1262 0.56 0.37 0.18 0.36 0.48 # mode 0 1262 1262 0.073 0.2 0.004 0.012 0.02 # perimeter 0 1262 1262 6.45 6.31 0.95 2.82 4.33 # range 0 1262 1262 0.69 0.22 0.064 0.53 0.68 # size 0 1262 1262 0.94 0.73 0.28 0.59 0.71 # std_dev 0 1262 1262 0.17 0.073 0.013 0.12 0.17 # transparency 0 1262 1262 0.089 0.11 0 0.011 0.052 # p75 p100 hist # 0.61 41.27 ▇▁▁▁▁▁▁▁ # 0.74 1 ▃▅▇▇▆▆▆▅ # 0.43 0.91 ▇▆▃▃▂▁▂▂ # 8.37 43.41 ▇▃▁▁▁▁▁▁ # 0.15 5.74 ▇▁▁▁▁▁▁▁ # 0.88 7.25 ▇▁▁▁▁▁▁▁ # 24.25 134.36 ▇▃▁▁▁▁▁▁ # 2.05 10.59 ▇▃▁▁▁▁▁▁ # 1.38 10.01 ▇▂▁▁▁▁▁▁ # 0.89 1.02 ▁▁▃▅▆▆▃▇ # 0.27 0.78 ▅▇▇▂▁▁▁▁ # 0.008 0.032 ▇▂▁▁▁▁▁▁ # 0.63 5.91 ▇▁▁▁▁▁▁▁ # 0.032 1.02 ▇▁▁▁▁▁▁▁ # 7.81 89.26 ▇▁▁▁▁▁▁▁ # 0.88 1.02 ▁▁▃▆▆▆▃▇ # 1.02 7.4 ▇▁▁▁▁▁▁▁ # 0.22 0.42 ▂▅▇▇▅▂▁▁ # 0.12 0.54 ▇▃▂▁▁▁▁▁ Voici comment se distribuent les organismes en fonction de class, en utilisant la syntaxe de base : table(zoo$class) # # Annélide Appendiculaire Calanoïde Chaetognathe # 50 36 288 51 # Cirripède Cladocère Cnidaire Cyclopoïde # 22 50 22 50 # Décapode Oeuf_allongé Oeuf_rond Poisson # 126 50 49 50 # Gastéropode Harpacticoïde Malacostracé Poecilostomatoïde # 50 39 121 158 # Protiste # 50 La fonction table() crée un objet du même nom qui se comporte de manière différente d’un data frame. Ainsi, l’impression de l’objet se présente comme un ensemble de valeurs nommées par le niveau dénombré pour un tableau de contingence à une seule entrée. Si vous voulez réaliser la même opération à l’aide d’une formule, vous pouvez utiliser la fonction tally() depuis le package mosaic. Vous pouvez charger ce package à l’aide de l’instruction library(mosaic), et ensuite utiliser tally() comme si de rien était, ou alors vous spécifiez complètement la fonction à l’aide de mosaic::tally() (littéralement, “la fonction tally du package mosaic”). Cette dernière approche est favorisée par SciViews-R car elle permet de comprendre immédiatement d’où vient la fonction et d’éviter aussi d’appeler malencontreusement une fonction du même nom qui serait définie dans un autre package. #mosaic::tally(data = zoo, ~ class) Tidyverse favorise l’assemblage d’un petit nombre de fonction (des “verbes”) pour obtenir les mêmes résultats que des fonctions plus spécialisées dans les autres styles. Ainsi, ses instructions seront souvent plus verbeuses (inconvénient), mais aussi beaucoup plus lisibles et compréhensibles par un humain (immense avantage). La réalisation d’un tableau de contingence consiste en fait à regrouper les données en fonction de class, et ensuite à compter (contingenter) les observations dans chaque classe. Nous pouvons écrire une instruction qui réalise exactement ce traitement de manière explicite (sachant que la fonction n() sert à dénombrer) : zoo %&gt;% group_by(class) %&gt;% summarise(n()) # # A tibble: 17 x 2 # class `n()` # &lt;fct&gt; &lt;int&gt; # 1 Annélide 50 # 2 Appendiculaire 36 # 3 Calanoïde 288 # 4 Chaetognathe 51 # 5 Cirripède 22 # 6 Cladocère 50 # 7 Cnidaire 22 # 8 Cyclopoïde 50 # 9 Décapode 126 # 10 Oeuf_allongé 50 # 11 Oeuf_rond 49 # 12 Poisson 50 # 13 Gastéropode 50 # 14 Harpacticoïde 39 # 15 Malacostracé 121 # 16 Poecilostomatoïde 158 # 17 Protiste 50 Nous obtenons un objet tibble, et non pas un objet spécifique au traitement réalisé. C’est dans la philosophie de tidyverse que d’utiliser et réutiliser autant que possible un tibble qui permet de contenir des données “bien rangées” (ou “tidy data” en anglais, d’où le nom de ce style, tidyverse pour “univers bien rangé”). Comme contingenter des observations est une opération fréquente, il existe exceptionnellement une fonction dédiée qui fait le travail en une seule opération : count(). count(zoo, class) # # A tibble: 17 x 2 # class n # &lt;fct&gt; &lt;int&gt; # 1 Annélide 50 # 2 Appendiculaire 36 # 3 Calanoïde 288 # 4 Chaetognathe 51 # 5 Cirripède 22 # 6 Cladocère 50 # 7 Cnidaire 22 # 8 Cyclopoïde 50 # 9 Décapode 126 # 10 Oeuf_allongé 50 # 11 Oeuf_rond 49 # 12 Poisson 50 # 13 Gastéropode 50 # 14 Harpacticoïde 39 # 15 Malacostracé 121 # 16 Poecilostomatoïde 158 # 17 Protiste 50 Le résultat est le même. Comparons maintenant la fonction table() de base et count() de tidyverse du point de vue des arguments. table() prend un vecteur comme argument. A nous de l’extraire du data frame à l’aide de zoo$class, ce qui donne table(zoo$class). Par contre, count() comme toute fonction tidyverse qui se respecte, prend comme premier argument un tibble ou un data.frame, bref un tableau cas par variables. C’est ensuite au niveau du second argument que l’on spécifie la variable que nous souhaitons utiliser à partir de ce tableau. Ici, plus besoin d’indiquer que c’est une variable qui vient du tableau zoo, car count() le sait déjà. Bien que ce ne soit pas évident au premier cou d’œil, count() ne respecte pas la syntaxe de base de R et évalue class de manière particulière59. Au final, l’appel à table() nécessite de comprendre ce que fait l’opérateur $. Au contraire, l’instruction tidyverse count(zoo, class) se lit et se comprend très bien presque comme si c’était écrit en anglais. Vous lisez en effet “compte dans zoo la classe” (avantage), mais le coût en est une évaluation non standard de ses arguments (inconvénient qui ne peut pas apparaître à ce stade mais que vous constaterez plus tard quand vous ferez des choses plus évoluées avec ces instructions). Le style de SciViews-R accepte à la fois la syntaxe de base et celle de tidyverse, avec une préférence pour cette dernière lorsque la lisibilité des instructions est primordiale. De plus, le style formule est également abondamment utilisé dès qu’il s’agit de réaliser un graphique ou un modèle statistique (les deux étant d’ailleurs souvent associés). Bien. Admettons maintenant que nous voulons représenter la forme (ratio d’aspect, rapport largeur / longueur) des œufs en fonction de leur taille sur un graphique (en utilisant les variables aspect et area) présents dans notre échantillon de zooplancton. Deux niveaux de la variable class les contiennent : Oeuf_allongé et Oeuf_rond. Nous voulons donc filtrer les données du tableau zoo pour ne garder que ces deux catégories, et éventuellement, nous voulons aussi restreindre le tableau aux trois variables aspect, area et class puisque nous n’avons pas besoin des autres variables. En R de base cela peut se faire en une seule étape à l’aide de l’opérateur d’indiçage [] comme nous avons vu plus haut. zoo2 &lt;- zoo[zoo$class == &quot;Oeuf_allongé&quot; | zoo$class == &quot;Oeuf_rond&quot;, c(&quot;aspect&quot;, &quot;area&quot;, &quot;class&quot;)] zoo2 # # A tibble: 99 x 3 # aspect area class # &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; # 1 0.965 0.385 Oeuf_rond # 2 0.976 6.04 Oeuf_rond # 3 0.964 0.271 Oeuf_rond # 4 0.911 0.259 Oeuf_rond # 5 0.950 0.322 Oeuf_rond # 6 0.959 0.335 Oeuf_rond # 7 0.976 0.343 Oeuf_rond # 8 0.975 1.11 Oeuf_rond # 9 0.449 0.495 Oeuf_allongé # 10 0.987 2.02 Oeuf_rond # # … with 89 more rows En tidyverse, les deux opérations (filtrage des lignes et sélection des variables en colonnes) restent deux opération successives distinctes dans le code. Notez au passage que nous repassons à l’opérateur de chaînage %&gt;.% de SciViews-R que nous avons l’habitude d’utiliser à la place de l’opérateur correspondant de tidyverse %&gt;%. zoo %&gt;.% filter(., class == &quot;Oeuf_allongé&quot; | class == &quot;Oeuf_rond&quot;) %&gt;.% select(., aspect, area, class) -&gt; zoo2 zoo2 # # A tibble: 99 x 3 # aspect area class # &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; # 1 0.965 0.385 Oeuf_rond # 2 0.976 6.04 Oeuf_rond # 3 0.964 0.271 Oeuf_rond # 4 0.911 0.259 Oeuf_rond # 5 0.950 0.322 Oeuf_rond # 6 0.959 0.335 Oeuf_rond # 7 0.976 0.343 Oeuf_rond # 8 0.975 1.11 Oeuf_rond # 9 0.449 0.495 Oeuf_allongé # 10 0.987 2.02 Oeuf_rond # # … with 89 more rows Le résultat est le même, mais la syntaxe est très différente. Notez que les variables dans la syntaxe de base sont complètement qualifiées (zoo$class), ce qui nécessite de répéter plusieurs fois le nom du jeu de données zoo (inconvénient) mais lève toute ambiguïté (avantage). La version de tidyverse est plus “propre” (avantage), mais cela implique d’utiliser une évaluation non standard de class qui n’est pas une variable existante dans l’environnement où le code est évalué (inconvénient). La sélection des variables est également différente. Dans R de base, des chaînes de caractères doivent être compilées dans un vecteur d’indiçage à l’aide de c(), alors que select() de tidyverse permet de spécifier simplement les noms des variables sans autres fioritures (mais cela doit être évalué de manière non standard, encore une fois). Pour calculer une nouvelle variable, par exemple le logarithme en base 10 de l’aire dans log_area, nous ferons comme ceci en R de base : zoo2$log_area &lt;- log10(zoo2$area) head(zoo2) # # A tibble: 6 x 4 # aspect area class log_area # &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; # 1 0.965 0.385 Oeuf_rond -0.414 # 2 0.976 6.04 Oeuf_rond 0.781 # 3 0.964 0.271 Oeuf_rond -0.568 # 4 0.911 0.259 Oeuf_rond -0.587 # 5 0.950 0.322 Oeuf_rond -0.492 # 6 0.959 0.335 Oeuf_rond -0.475 Avec tidyverse, nous savons déjà que mutate() est le verbe à employer pour cette opération. zoo2 &lt;- mutate(zoo2, log_area = log10(area)) head(zoo2) # # A tibble: 6 x 4 # aspect area class log_area # &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; # 1 0.965 0.385 Oeuf_rond -0.414 # 2 0.976 6.04 Oeuf_rond 0.781 # 3 0.964 0.271 Oeuf_rond -0.568 # 4 0.911 0.259 Oeuf_rond -0.587 # 5 0.950 0.322 Oeuf_rond -0.492 # 6 0.959 0.335 Oeuf_rond -0.475 La syntaxe de tidyverse se lit mieux et est plus propre (avantage), mais elle nécessite pour y arriver une évaluation non standard de area, ce qui est un inconvénient par rapport à la snytaxe R de base. Pour finir, revenons sur les différents moteurs graphiques pour faire un nuage de points en utilisant différents styles. Une petite précaution supplémentaire est nécessaire. Pour class, nous devons préalablement laisser tomber les niveaux non utilisés à l’aide de droplevels(). # Tous les niveaux sont toujours là levels(zoo2$class) # [1] &quot;Annélide&quot; &quot;Appendiculaire&quot; &quot;Calanoïde&quot; # [4] &quot;Chaetognathe&quot; &quot;Cirripède&quot; &quot;Cladocère&quot; # [7] &quot;Cnidaire&quot; &quot;Cyclopoïde&quot; &quot;Décapode&quot; # [10] &quot;Oeuf_allongé&quot; &quot;Oeuf_rond&quot; &quot;Poisson&quot; # [13] &quot;Gastéropode&quot; &quot;Harpacticoïde&quot; &quot;Malacostracé&quot; # [16] &quot;Poecilostomatoïde&quot; &quot;Protiste&quot; # Ne retenir que les niveaux relatifs aux oeufs zoo2$class &lt;- droplevels(zoo2$class) # C&#39;est mieux levels(zoo2$class) # [1] &quot;Oeuf_allongé&quot; &quot;Oeuf_rond&quot; Voici un graphe de base… avec également la syntaxe de base : plot(zoo2$log_area, zoo2$aspect, col = zoo2$class) legend(&quot;bottomright&quot;, legend = c(&quot;Oeuf allongé&quot;, &quot;Oeuf rond&quot;), col = 1:2, pch = 1) Le même graphique, mais en utilisant l’interface formule alternative avec plot() : plot(data = zoo2, aspect ~ log_area, col = class) legend(&quot;bottomright&quot;, legend = c(&quot;Oeuf allongé&quot;, &quot;Oeuf rond&quot;), col = 1:2, pch = 1) L’interface formule est également employée avec le moteur lattice via la fonction xyplot(). Ici, nous utilisons la version chart() en appelant chart$xyplot(). chart$xyplot(data = zoo2, aspect ~ log_area, groups = zoo2$class, auto.key = TRUE) Dans tidyverse, c’est le moteur graphique ggplot2 qui est utilisé, avec sa syntaxe propre : ggplot(data = zoo2, aes(x = log_area, y = aspect, col = class)) + geom_point() Dans SciViews-R, chart() utilise aussi par défaut le moteur graphique ggplot2, mais il est plus flexible et permet soit d’utiliser aes() comme ggplot(), soit une interface formule élargie (c’est-à-dire qu’il est possible d’y inclure d’autres “aesthetics” à l’aide des opérateurs %aes=%) : # chart() et aes() chart(data = zoo2, aes(x = log_area, y = aspect, col = class)) + geom_point() # chart() avec une formule élargie chart(data = zoo2, aspect ~ log_area %col=% class) + geom_point() Il y aurait encore beaucoup à dire sur les différents styles de syntaxe dans R, mais nous venons de discuter les éléments essentiels. SciViews-R propose d’utiliser un ensemble cohérent d’instructions qui est soigneusement choisi pour rendre l’utilisation de R plus facile (sur base de nos observations des difficultés et erreurs d’apprentissage principales). Il se base sur tidyverse avec une pincée de R de base et une bonne dose de formules là où elles se montrent utiles. Des fonctions et des opérateurs originaux sont ajoutés dans le but d’homogénéiser et/ou clarifier la syntaxe. De votre côté, vous êtes libre d’utiliser le style que vous préférez. si la curiosité vous pousse à essayer autre chose et à adopter un autre style que SciViews-R, nous en serons ravis. R est ouvert, il offre beaucoup et c’est à vous maintenant de créer votre propre boite à outils taillée réellement à votre mesure ! Pour finir, il parait que les belges sont forts en auto-dérision. La caricature suivante devrait vous faire sourire : Prolifération de standards par xkcd. Pour en savoir plus Swirl vous permet d’apprendre la syntaxe de base de R de manière conviviale et interactive. Le site R maintient une page de documents et tutoriaux ici (descendez jusqu’à la section concernant les documents en français si vous préférez travailler dans cette langue). Enfin, les manuels de R sont un peu techniques, mais ils décrivent dans le détail la syntaxe de base. Mosaic est une initiative américaine qui vise en partie un objectif assez similaire à celui de SciViews-R : homogénéiser l’interface de R et en faciliter l’apprentissage. A student’s guide to R est un ouvrage en ligne qui vous apprendra à utiliser R selon le style mosaic qui fait la part belle à l’interface formule. La littérature concernant le tidyverse est abondante. Commencez par le site web qui pointe également vers R for Data Science que nous conseillons comme première source pour apprendre R à la sauce tidyverse (une version en français au format papier est également disponible). Voyez ensuite la page “learn the tidyverse” pour divers ouvrages et autres matériels d’apprentissage. Historiquement, ce langage s’appelait S à l’origine dans les années 1970 alors qu’il a été inventé par John Chambers et ses collègues aux laboratoires Bell. R est une implémentation open source de S écrite dans les années 1990 par Ross Ihaka et Robert Gentleman. Ensuite R a pris de l’importance à tel point qu’on parle maintenant du langage R (sachant bien qu’il s’agit d’un dialecte du langage S).↩ Cette évaluation particulière s’appelle le “tidyeval”. Son explication est hors de propos dans cette introduction à la science des données mais si vous êtes curieux, vous pouvez toujours lire ceci.↩ "],
["correlation.html", "Module 12 Corrélation", " Module 12 Corrélation Objectifs Comprendre les différents niveaux d’association de deux variables numériques. Connaître et savoir utiliser les coefficients de corrélation de Pearson, de Spearman, et de Kendall. Pouvoir manipuler des matrices de corrélation et des corrélogrammes. Utiliser également la matrice de nuages de points comme représentation graphique complémentaire. Maîtriser le test de corrélation. Développer son esprit critique face à des analyses statistiques et des graphiques. Être capable de discerner les erreurs principales qui se rencontrent dans les graphiques (et ne pas les faire soi-même). Prérequis Ce module présente des descripteurs statistiques de l’association de deux variables, ainsi qu’un test d’hypothèse associé. Pour comprendre cette matière, il faut comprendre la logique derrière un test d’hypothèse vue au module 8. La seconde partie reviendra sur les représentations graphiques vues aux modules 2, 3 et 4 qui doivent être bien maîtrisé avant d’aborder cette section. A vous de jouer En lien avec ce module vous avez une série d’exercices à réaliser. Vous avez à : Réalisez une séance d’exercice sur la corrélation Afin d’appliquer directement les concepts vu au cours dans ce module, ouvrez RStudio dans votre SciViews Box, puis exécutez l’instruction suivante dans la fenêtre console : BioDataScience::run(&quot;12a_correlation&quot;) compléter des fichiers RMD au sein du projet ci-dessous : Un projet couvre toute la matière du module 12. Il y a 3 exercices pour les 3 parties de ce module. Pour l’année académique 2019-2020, l’URL à utiliser pour accéder à votre tâche si vous suivez le cours de Sciences des données I à Mons est le suivants : https://classroom.github.com/a/sjzY0x-h. Lisez attentivement le README. Ce projet doit être terminé à la fin du module. Pour les autres utilisateurs de ce livre, veuillez faire un “fork” du dépôt sdd1_module12. Si vous souhaitez accéder à une version précédente de l’exercice, sélectionner la branche correspondante à l’année que vous recherchez. "],
["association-de-deux-variables.html", "12.1 Association de deux variables", " 12.1 Association de deux variables Nous pouvons décrire l’étendue d’une variable numérique à l’aide de la variance qui, pour rappel est la somme des écarts à la moyenne divisée par le nombre de degrés de liberté (n dans le cas d’une population et n - 1 dans le cas d’un échantillon). \\[var_X = S^2_X = \\frac{\\sum_{i = 1}^n (x_i - \\bar{X})^2}{n-1}\\] L’écart type, noté \\(\\sigma\\) dans le cas d’une population et \\(S\\) dans le cas d’un échantillon est la racine carrée de la variance. C’est une autre mesure de la dispersion d’une variable numérique. Donc : \\[S_X = \\sqrt{S^2_X}\\] Plus la variance est élevée, plus les observations sont dispersées autour de la moyenne. Lorsque nous avons affaire à deux variables numériques, une représentation de l’une par rapport à l’autre se fait naturellement à l’aide d’un graphique en nuage de points. Voici trois situations fictives différentes (Y1, Y2 et Y3 en fonction de X) : Nous pouvons observer que la forme du nuage de points diffère entre ces trois situations. Le graphique A est allongé le long d’une oblique proche de la première bissectrice. Cela signifie que, lorsque des valeurs de X sont faibles, les valeurs de Y1 sont faibles aussi. Lorsque les valeurs de X sont élevées, celles de Y1 tendent à l’être également. Nous avons plutôt une proportionnalité entre les valeurs observées pour X et pour Y1. Dans le graphique C de droite, c’est l’inverse. Nous avons une proportionnalité inverse entre X et Y3. Dans le graphique B du centre, le nuage de point ne s’étire pas dans une direction oblique particulière. Nous dirons ici qu’il n’y a pas d’association entre X et Y2. Ce type d’association entre deux variables numérique est un élément important dans notre analyse car un nuage de points qui s’allonge le long d’une direction oblique sur le graphique est signe d’un mécanisme sous-jacent responsable de cette association (mais attention à ne pas conclure directement à un mécanisme de cause à effet direct, voir plus loin). Il serait donc souhaitable de pouvoir quantifier le degré d’une telle association. Nous pouvons définir la covariance comme étant une mesure de la variance dans le même sens pour toutes les paires de valeurs observées entre nos deux vairables numériques. Elle se définit de manière très similaire à la variance mais fait naturellement intervenir simultanément les observations de X et Y, ainsi que leurs moyennes respectives : \\[cov_{X,Y} = \\frac{\\sum_{i = 1}^n (x_i - \\bar{X}) \\cdot (y_i - \\bar{Y})}{n-1}\\] Voyons ce que cela donne dans notre exemple fictif contenu dans un data frame nommé `df : cov(df$X, df$Y1) # [1] 0.9336631 cov(df$X, df$Y2) # [1] 0.004064163 cov(df$X, df$Y3) # [1] -0.9230542 La covariance (fonction cov() dans R) fait effectivement le boulot de quantifier l’association entre les deux variables X et Y. Nous avons une valeur positive entre X et Y1, faible et se rapprochant de zéro entre X et Y2, et négative pour le cas X et Y3 d’une proportionnalité inverse. Cette mesure n’est cependant pas normée, c’est-à-dire qu’elle peut varier vers des valeurs très grandes ou très petites en fonction des données. Donc pour une même forme de nuage de points, la valeur dépendra, par exemple, des unités de mesure choisies. Si je transforme les données de mon jeu fictif df en les multipliant par dix pour simuler un changement d’unité dans df2, j’obtiens : df2 &lt;- df * 10 cov(df2$X, df2$Y1) # [1] 93.36631 cov(df2$X, df2$Y2) # [1] 0.4064163 cov(df2$X, df2$Y3) # [1] -92.30542 C’est embêtant, puisque la forme du nuage de points n’a, lui, pas changé du tout. Le coefficient de corrélation de Pearson, noté r, est une autre mesure qui est normée de telle façon qu’il valle +1 pour une proportionnalité directe parfaite (les points sont strictement alignés le long d’une droite) et -1 lorsque la proportionnalité est inverse parfaite (et toujours 0 en cas de non-association). \\[cor_{X,Y} = r_{X,Y} = \\frac{cov_{X,Y}}{\\sqrt{S^2_X \\cdot S^2_Y}} = \\frac{cov_{X,Y}}{\\sqrt{S^2_X} \\cdot \\sqrt{S^2_Y}} = \\frac{cov_{X,Y}}{S_X \\cdot S_Y}\\] C’est grâce à la division par le produit des écarts types de X et Y que nous arrivons à normer correctement le coefficient. Celui-ci peut se calculer à l’aide de la fonction cor() dans R. Cela donne : cor(df$X, df$Y1) # [1] 0.9781819 cor(df2$X, df2$Y1) # [1] 0.9781819 cor(df$X, df$Y2) # [1] 0.003939552 cor(df2$X, df2$Y2) # [1] 0.003939552 cor(df$X, df$Y3) # [1] -0.979511 cor(df2$X, df2$Y3) # [1] -0.979511 Cette fois-ci, nous obtenons la même valeur pour r que le calcul se fasse à partir de df ou de df2. De plus les valeurs absolues très proches de 1 (0,978 dans le cas A et 0,923 dans le cas C) suggèrent que la proportionnalité est très forte. C’est effectivement ce que nous observons également sur les graphiques. Faites attention à deux points importants. Le coefficient de corrélation de Pearson mesure une association linéaire entre deux variables numériques. La figure suivante montre quelques nuages de points et les valeurs de r associées. Exemples de nuages de points et leur coefficients de corrélation de Pearson associés, issu de https://commons.wikimedia.org/w/index.php?curid=15165296. L’existence d’une corrélation n’implique pas forcément que la variation d’une des deux variables est le résultat de la variation de l’autre (cause à effet). Il se peut, par exemple qu’il y ait une troisième variable non prise en compte qui soit à l’origine de la variation, directement ou indirectement des deux autres. Une variable particulièrement pernicieuse de ce point de vue est le temps. A peu près tout ce qu’on étudie en biologie est variable dans le temps. Et donc, bien souvent, il existe des corrélations entre des variables qui n’ont rien à voir l’une avec l’autre lorsqu’elles sont toutes deux mesurées à différents moments, ce qu’on appelle des séries temporelles ou chronologiques. La vidéo suivante apporte d’autres éclaircissements sur ce sujet, sur base d’un exemple tiré de la littérature scientifique. L’association entre deux variables numériques peut ainsi s’envisager selon trois niveaux impliquant des hypothèses de plus en plus fortes quant aux mécanismes responsables de cette association : La corrélation. Ici les deux variables numériques sont sur le même pied d’égalité. Nous nous bornons à observer l’association sans élaborer plus d’explication sur son existence. C’est le coefficient de corrélation qui la quantifie. La relation. Ici, nous modélisons l’association, par exemple par une droite, dite droite de régression. Nous verrons cet outil très important des statistiques et de la science des données dans la partie modélisation du cours de Science des Données Biologiques II. Dans ce cas, nous considérons qu’un mécanisme sous-jacent est responsable de la forme du nuage de points, et nous considérons qu’une fonction mathématique peut être utilisée pour prédire les valeurs d’une variable connaissant celles de l’autre. La causalité. En plus de la relation, nous considérons que c’est la variation d’une des deux variables qui est à l’origine, directement ou indirectement de la variation de l’autre. Une relation de cause à effet ne peut être démontrée de manière sûre que par l’expérience, comme expliqué dans la vidéo plus haut. 12.1.1 Matrice de corrélation Dans un cas multivarié (plus de deux variables), nous pouvons toujours étudier les associations entre variables numériques à l’aide de r (ou à l’aide de la covariance) à conditions de calculer ces descripteurs statistiques pour tous les couples de variables considérées deux à deux. Pour N variables, nous rassemblerons tous ces calculs dans une matrice carrée N par N qui croise tous les cas deux à deux possibles dans un même tableau. Prenons un exemple à trois variables. Le jeu de données trees rassemble la mesure du diamètre, de la hauteur et du volume de bois de cerisiers noirs. La matrice de corrélation peut se calculer à l’aide de cor() dans R, ou mieux, à l’aide de correlation() dans SciViews::R. Dans ce dernier cas, la fonction summary() peut être appliquée dessus pour obtenir une vision synthétique de la matrice de corrélation. trees &lt;- read(&quot;trees&quot;, package = &quot;datasets&quot;) head(trees) # # A tibble: 6 x 3 # diameter height volume # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 0.211 21.3 0.292 # 2 0.218 19.8 0.292 # 3 0.224 19.2 0.289 # 4 0.267 21.9 0.464 # 5 0.272 24.7 0.532 # 6 0.274 25.3 0.558 trees_cor &lt;- correlation(trees) trees_cor # Matrix of Pearson&#39;s product-moment correlation: # (calculation uses everything) # diameter height volume # diameter 1.000 0.519 0.967 # height 0.519 1.000 0.597 # volume 0.967 0.597 1.000 Vous noterez que : Les éléments sur la diagonale de la matrice de corrélation valent toujours 1. En effet, il s’agit de la corrélation d’une variable en fonction d’elle-même, or \\[r_{X,X} = 1\\] - Le triangle inférieur et le triangle supérieur (de part et d’autre de la diagonale) sont identiques, ou si vous préférez, sont comme le reflet dans un miroir l’un de l’autre. C’est parce que le coefficient de corrélation de X et Y est toujours le même que celui de Y et X. \\[r_{X,Y} = r_{Y,X}\\] Pour ces raisons seul le triangle inférieur (ou supérieur) est informatif. Le reste (diagonale et autre triangle) sont triviaux et répétitifs. Vous trouverez parfois une représentation de la matrice de confusion sous la forme uniquement du triangle inférieur. La fonction summary() effectue une telle représentation, et simplifie encore la représentation pour aider à trouver les corrélations importantes dans un gros tableau. summary(trees_cor) # Matrix of Pearson&#39;s product-moment correlation: # (calculation uses everything) # d h v # diameter 1 # height . 1 # volume B . 1 # attr(,&quot;legend&quot;) # [1] 0 &#39; &#39; 0.3 &#39;.&#39; 0.6 &#39;,&#39; 0.8 &#39;+&#39; 0.9 &#39;*&#39; 0.95 &#39;B&#39; 1 Voyez la page d’aide de la fonction correlation() pour plus d’information via ?correlation. Vous pouvez passer un data frame à la fonction, ou encore, l’appeler via correlation(data = df, ~ var1 + var2 + var3) en utilisant une formule pour spécifier les variables du tableau à étudier. L’argument use = permet de spécifier quoi faire en cas de valeurs manquantes. Si vous indiquez &quot;complete.obs&quot;, toute ligne du tableau contenant au moins une valeur manquante sera élimniée avant le calcul. Avec &quot;pairwise.complete.obs&quot; les éliminations de valeurs manquantes se font pour chaque paire de variables individuellement des autres. 12.1.2 Corrélogramme Il existe aussi des représentation graphiques spécialisées, appelées corrélogrammes pour visualiser les coefficents de corrélations dans un cas multivarié. La fonction plot() appliquée à notre objet Correlation en offre une version simple. plot(trees_cor) La matrice est représentée par des ellipses de plus en plus allongées au fur et à mesure que r se rapproche de 1. Une couleur bleue est utilisée pour les corrélations positives et une couleur rouge pour les corrélations négatives (mais vous pouvez aussi choisir d’autres couleurs). Ici, toutes les corrélations sot positives. Sur le jeu de données zooplancton, nous pouvons réaliser un corrélogramme plus intéressant qui illustre mieux la diversité de cette représentation graphique. Considérons, à titre d’exemple, les variables contigües size jusqu’à density (que l’on peu indiquer par size:density dans la fonction select()) : zoo &lt;- read(&quot;zooplankton&quot;, package = &quot;data.io&quot;) zoo %&gt;.% select(., size:density) %&gt;.% correlation(.) -&gt; zoo_cor plot(zoo_cor) Vous noterez que les variables elongation et compactness sont redondantes (r = 1). De plus, les données le long de la diagonale et sur le triangle supérieur n’apportent rien. Nous pouvons aussi bien décider de ne représenter que le triangle inférieur sur notre corrélogramme. plot(zoo_cor, type = &quot;lower&quot;) 12.1.3 Importance des graphiques Faites bien attention avec le coefficient de corrélation, la matrice de corrélation et le corrélogramme, car des formes de nuages de points complexes peuvent se solder par des valeurs peu indicatives ! Un jeu de données artificiel appelé “quartet d’Anscombe” montre très bien comment des données très différentes peuvent avoir même moyenne, même variance et même coefficient de corrélation. Ce n’est qu’avec un graphique en nuage de points (ou matrice de nuages de points, voir plus loin) qu’il est possible de détecter le problème. anscombe &lt;- read(&quot;anscombe&quot;, package = &quot;datasets&quot;) head(anscombe) # # A tibble: 6 x 8 # x1 x2 x3 x4 y1 y2 y3 y4 # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 10 10 10 8 8.04 9.14 7.46 6.58 # 2 8 8 8 8 6.95 8.14 6.77 5.76 # 3 13 13 13 8 7.58 8.74 12.7 7.71 # 4 9 9 9 8 8.81 8.77 7.11 8.84 # 5 11 11 11 8 8.33 9.26 7.81 8.47 # 6 14 14 14 8 9.96 8.1 8.84 7.04 Séparons les quatre variables X d’un côté et les quatre variables Y de l’autre. ans_x &lt;- anscombe[, 1:4] ans_y &lt;- anscombe[, 5:8] Que valent les moyennes60, les variances et les coefficients de corrélation ? # Moyennes des X lapply(ans_x, mean) # $x1 # [1] 9 # # $x2 # [1] 9 # # $x3 # [1] 9 # # $x4 # [1] 9 # Variance des X lapply(ans_x, var) # $x1 # [1] 11 # # $x2 # [1] 11 # # $x3 # [1] 11 # # $x4 # [1] 11 # Moyenne des Y lapply(ans_y, mean) # $y1 # [1] 7.500909 # # $y2 # [1] 7.500909 # # $y3 # [1] 7.5 # # $y4 # [1] 7.500909 # Variance des Y lapply(ans_y, var) # $y1 # [1] 4.127269 # # $y2 # [1] 4.127629 # # $y3 # [1] 4.12262 # # $y4 # [1] 4.123249 C’est les mêmes valeurs pour les 4 séries. Que donnent les coefficients de corrélation61 ? diag(cor(ans_x, ans_y)) # [1] 0.8164205 0.8162365 0.8162867 0.8165214 Nous avons quatre fois la même valeur… et pourtant : pl &lt;- list( chart(data = anscombe, y1 ~ x1) + geom_point(), chart(data = anscombe, y2 ~ x2) + geom_point(), chart(data = anscombe, y3 ~ x3) + geom_point(), chart(data = anscombe, y4 ~ x4) + geom_point() ) combine_charts(pl) Nous voyons que ces trois paires de variables n’ont rien à voir l’une avec l’autre ! Il est même possible d’aller encore plus loin, voir le datasaurus dozen, ou encore ici, ou en français ici. 12.1.4 Matrice de nuages de points La matrice de nuages de points part du même principe que la matrice de corrélation ou que le corrélogramme : représenter plusieurs variables deux à deux selon une grille N par N pour N variables numériques. Ici, il s’agit de représenter des nuages de points deux à deux. Nous venons de voir pourquoi c’est important de le faire en complément des autres outils dans le cadre de notre exploration de l’association entre ces variables. Dans la SciViews Box, un snippet est disponible dans le menu charts: multivariate à partir de .cm, entrée de menu multivariate X-Y scatterplot. GGally::ggscatmat(as.data.frame(trees), 1:3) Comme dans le cas de la matrice de corrélation, les graphiques en nuage de points sur la diagonale ne seraient pas très utiles puisqu’ils représenteraient une variable par rapport à elle-même. Ils sont donc remplacés par des graphes de densité montrant la répartition des données pour chanque variable considérée individuellement. Sur le triangle supérieur, ce sont les coefficients de corrélation de Pearson qui sont indiqués, et sur le triangle inférieur, les différentes possibilités de nuages de points deux à deux. La variable sur laxe des abscisses se lit dans la colonne au dessus et la variable représentée sur l’axe des ordonnées se lit dans la ligne à droite. Par exemple, le graphique en bas à gauche correspond au diamètre en X et au volume en Y. Cette représentation graphique est donc complémentaire au corrélogramme. 12.1.5 Matrice de variances-covariances De même que nous pouvons calculer une matrice de corrélations, nous pouvons calculer une matrice de covariances, mais sachant que \\(cov_{X,X} = var_X\\), nous avons également les variances le long de la diagonale. Pour cette raison, nous appelons ce tableau, une matrice de variances-covariances. Par exemple, pour le jeu de données trees, cela donne : cov(trees) # diameter height volume # diameter 0.006341258 0.08050548 0.03583784 # height 0.080505484 3.80012903 0.54143731 # volume 0.035837844 0.54143731 0.21659744 Comme ces descripteurs statistiques ne sont pas normés, ils sont plus difficiles à interpréter. Nous préférons donc la matrice de corrélations pour étudier l’association entre plusieurs variables numériques. Néanmoins, la matrice de variances-covariances interviendra plus tard dans d’autres traitements statistiques et il est utilie de la connaitre (par exemple, dans le cadre de l’ACP que nous étudierons en Science des Données Biologiques II). 12.1.6 Corrélations de Spearman et Kendall Le coefficient de corrélation de Pearson représente une corrélation linéaire. Cependant, il se peut que vous soyez intéressé par un corrélation non linéaire, un nuage de points qui s’allonge le long d’une courbe. Dans ce cas, vous pouvez utiliser soit le coefficient \\(\\rho\\) de Spearman, soit le \\(\\tau\\) de Kendall. Le \\(\\rho\\) de Spearman est le même calcul que le coefficient de Pearson, mais appliqué sur les données préalablement transformées en rangs. Sa valeur vaudra +1 ou -1 lorsque les points s’alignent parfaitement le long de n’importe quelle fonction monotone croissante ou décroissante. Le \\(\\tau\\) de Kendall utilise un calcul selon la même logique que les tests de Wilcoxon ou de Kruskal-Wallis. Nous allons ici compter le nombre de paires concordantes \\(n_c\\) définies par \\(x_i &lt; x_j\\ \\mathrm{et}\\ y_i &lt; y_j\\), ou \\(x_i &gt; x_j\\ \\mathrm{et}\\ y_i &gt; y_j\\). Nous compterons aussi le nombre de paires discordantes \\(n_d\\) telles que \\(x_i &lt; x_j\\ \\mathrm{et}\\ y_i &gt; y_j\\), ou \\(x_i &gt; x_j\\ \\mathrm{et}\\ y_i &lt; y_j\\). Enfin, si \\(x_i = x_j\\ \\mathrm{et}\\ y_i = y_j\\), la paire n’est pas comptabilisée. Nous avons alors pour un échantillon de \\(n\\) observations des variables numériques X et Y : \\[\\tau_{X,Y} = \\frac{n_c - n_d}{\\frac{1}{2} \\cdot n \\cdot (n - 1)}\\] En pratique dans R, les fonctions cor() et correlation() peuvent être utilisées, mais en spécifiant method = &quot;spearman&quot; ou method = &quot;kendall&quot;. Pour trees, cela donne : correlation(trees, method = &quot;spearman&quot;) # Matrix of Spearman&#39;s rank correlation rho: # (calculation uses everything) # diameter height volume # diameter 1.000 0.441 0.955 # height 0.441 1.000 0.579 # volume 0.955 0.579 1.000 correlation(trees, method = &quot;kendall&quot;) # Matrix of Kendall&#39;s rank correlation tau: # (calculation uses everything) # diameter height volume # diameter 1.000 0.317 0.830 # height 0.317 1.000 0.450 # volume 0.830 0.450 1.000 … à comparer avec la matrice de corrélation de Pearson : correlation(trees) # Equivalent à method = &quot;pearson&quot; # Matrix of Pearson&#39;s product-moment correlation: # (calculation uses everything) # diameter height volume # diameter 1.000 0.519 0.967 # height 0.519 1.000 0.597 # volume 0.967 0.597 1.000 Les valeurs obtenues diffèrent, mais les tendances restent similaires ici. Les differences sont d’autant plus importantes que le nuage de points est curvilinéaire. A vous de bien choisir votre coefficient en fonction de ce que vous recherchez, une association linéaire (Pearson) ou pas (Spearman, le plus utilisé, ou Kendall éventuellement). 12.1.7 Test de corrélation C’est bien beau de pouvoir quantifier une corrélation, mais à partir de quand est-elle significative ? En d’autres termes, nous voudrions déterminer si l’allongement du nuage de points peut être fortuit (par le biais de l’échantillonnage aléatoire) ou non. Il existe un test d’hypothèse qui répond à cette question, avec une version pour chacun des trois coefficients de corrélation, r de Pearson, \\(\\rho\\) de Spearman, et \\(\\tau\\) de Kendall. Pour r de Pearson, nous aurons les hypothèses suivantes : \\(H_0\\ :\\ r = 0\\) \\(H_1\\ :\\ r \\neq 0\\) Il existe aussi des variantes unilatérales à gauche (\\(H_1\\ :\\ r &lt; 0\\)) ou à droite (\\(H_1\\ :\\ r &gt; 0\\)) dans le cas où nous aurions des indications que l’association ne peut qu’être de type proportionnalité inverse ou directe, respectivement. Prenons l’exemple de r pour les deux variables les plus corrélées dans trees : diameter et volume. Notez la forme particulière de la formule à utiliser. Comme les deux variables sont sur le même pied d’égalité, il n’y a pas de raison d’en placer une à gauche du signe ~ dans la formule. On l’écrit alors ~ var1 + var2. Choisissons \\(\\alpha\\) à 5% avant d’effectuer le test, ensuite, considérons un test unilatéral à droite avec alternative = &quot;greater&quot; puisque le volume de bois ne peut qu’augmenter avec le diamètre de l’arbre (relation inverse pas crédible). cor.test(data = trees, ~ diameter + volume, alternative = &quot;greater&quot;) # # Pearson&#39;s product-moment correlation # # data: diameter and volume # t = 20.44, df = 29, p-value &lt; 2.2e-16 # alternative hypothesis: true correlation is greater than 0 # 95 percent confidence interval: # 0.9394172 1.0000000 # sample estimates: # cor # 0.9670023 Comme la valeur P est inférieure à \\(\\alpha\\), nous pouvons rejeter \\(H_0\\) et conclure que le coefficient de corrélation entre le diamètre et le volume est significativement positive au seuil \\(\\alpha\\) de 5% (r = 0.967, ddl = 29, valeur P &lt; 0.001). On s’en doutait avec un coefficient aussi proche de 1. Mais qu’en est-il pour l’association entre le diamètre et la hauteur ? cor.test(data = trees, ~ diameter + height, alternative = &quot;greater&quot;) # # Pearson&#39;s product-moment correlation # # data: diameter and height # t = 3.2664, df = 29, p-value = 0.0014 # alternative hypothesis: true correlation is greater than 0 # 95 percent confidence interval: # 0.2576453 1.0000000 # sample estimates: # cor # 0.5186072 Elle est, elle aussi significative au seuil \\(\\alpha\\) de 5%, mais de manière moins nette, puisque la valeur P vaut un peu plus de 0,1%. Que donnerait un test de Spearman sur ces mêmes variables ? cor.test(data = trees, ~ diameter + height, alternative = &quot;greater&quot;, method = &quot;spearman&quot;) # Warning in cor.test.default(x = structure(c(0.211, 0.218, 0.224, 0.267, : # Cannot compute exact p-value with ties # # Spearman&#39;s rank correlation rho # # data: diameter and height # S = 2773.4, p-value = 0.006528 # alternative hypothesis: true rho is greater than 0 # sample estimates: # rho # 0.4408387 Le test nous averti qu’en présence d’ex aequos, le calcul n’est qu’approchant. Ici aussi nous rejetons \\(H_0\\). Enfin, pour comparaison (en pratique, on ne fait pas systématiquement tous les tests, mais on choisi celui qui est le plus adéquat), que donnerait un test de Kendall ? cor.test(data = trees, ~ diameter + height, alternative = &quot;greater&quot;, method = &quot;kendall&quot;) # Warning in cor.test.default(x = structure(c(0.211, 0.218, 0.224, 0.267, : # Cannot compute exact p-value with ties # # Kendall&#39;s rank correlation tau # # data: diameter and height # z = 2.4575, p-value = 0.006995 # alternative hypothesis: true tau is greater than 0 # sample estimates: # tau # 0.3168641 Même remarque concernant les ex aequos et valeur P très similaire ici à celle du test de Spearman. A vous de jouer Vous avez a présent toutes les connaissances théoriques afin de : réaliser le learnR spécifique au module 12 Afin d’appliquer directement les concepts vu au cours dans ce module, ouvrez RStudio dans votre SciViews Box, puis exécutez l’instruction suivante dans la fenêtre console : BioDataScience::run(&quot;12a_correlation&quot;) appliquer vos connaissances dans la première partie du projet spécifique du module La première partie porte sur la corrélation. Le lien pour débuter ce projet ce trouve en début de module 12 http://biodatascience-course.sciviews.org/sdd-umons/correlation.html appliquer vos connaissances dans le projet de biométrie humaine Retournez dans votre projet de biométrie humaine et réalisez au moins un test de corrélation. La fonction lapply() distribue la fonction donnée en argument sur tous les éléments du tableau, et calcule ici la moyenne successivement pour les 4 variables.↩ Ici, nous croisons les X et les Y, et extrayons la diagonale de ce tableau qui correspond aux coefficients entre X1 et Y1, entre X2 et Y2, entre X3 et Y3 et entre X4 et Y4, respectivement.↩ "],
["communication.html", "12.2 Communication", " 12.2 Communication Savoir communiquer ses résultats est vital en science des données. Ce n’est pas si facile car il faut pouvoir simplifier les analyses et utiliser au mieux les visuels (c’est-à-dire, les graphiques) pour raconter une histoire qui soit à la fois captivante et compréhensible. Communiquer le fruit de ses recherches de la meilleure façon qui soit pour que les non-initiés puissent le comprendre fait partie du bagage indispensable du scientifique des données. Hans Rosling est sans nuls doute très doué pour communiquer des résultats statistiques. La vidéo suivante est un peu longue (20min) et en anglais, mais elle en vaut vraiment la peine62. De plus, il explique à quel point il est important de partager et de rassembler les données dans des grandes bases de données, et ensuite d’en tirer des études utiles pour l’humanité. C’est l’avenir des sciences des données, y compris en biologie, qu’il est en train de prédire là. 12.2.1 Présentations R Markdown Parmi tous les formats R Markdown, il en existe plusieurs adaptés aux présentations de type “PowerPoint”. Le format R Presentation est très simple et parfaitement intégré dans RStudio. Divers formats compilés depuis R Markdown s’affichent dans un explorateur Web (ioslides ou Slidy, ou encore mdshower ou xaringan). Enfin, le format Beamer permet de créer des présentations en PDF. Le format R Presentation (menu File -&gt; New file -&gt; R Presentation dans RStudio) est très simple pour créer des diapositives de présentation directement depuis Markdown sous forme HTML, et pouvant s’exécuter dans RStudio ou dans un explorateur Web. Ce type de présentation est très bien intégré dans RStudio. Outre l’édition au format R Markdown la présentation elle-même apparait dans un onglet spécial Presentation en haut à droite qui n’interfère pas avec les autres onglets. Par contre, les possibilités de personnalisation sont plutôt limitées. Deux formats basés sur le HTML et JavaScript (comprenez, des formats qui s’exécutent dans un explorateur Web) sont aussi proposés par défaut via les types de documents R Markdown (menu File -&gt; New File -&gt; R Markdown, puis Presentation). Ls deux formats par défaut sont ioslides et Slidy. L’apparence et les fonctionnalités des deux systèmes diffèrent quelque peu. Le mieux est de tester les deux et de choisir celui qu’on préfère. La présentation apparait dans une fenêtre séparée. Les possibilités de personnalisation sont plus poussées, mais elles se font à l’aide de feuilles de styles au format CSS. Cela impose de comprendre leur logique. Une troisième option est de compiler un document PDF de présentation en passant par le package Beamer sous LaTeX. L’avantage est que c’est lisible partout. L’inconvénient : seul du contenu statique est accepté (pas de gifs animés, pas de vidéos ou difficilement directement dans la présentation). Sinon, les possibilités de personnalisation sont immenses63. Les trois formats ioslides, Slidy et Beamer sont parfaitement intégrés à RStudio, avec un menu contextuel dans le bouton Knit qui permet de passer facilement de l’un à l’autre pour autant que vous n’utilisez pas des balises spécifiques à l’un de ces trois formats. Avec le format Beamer, vous voyez le PDF résultant dans une fenêtre séparée. Cette fenêtre ne permet pas de lancer la présentation. Il vous faut l’ouvrir dans un lecteur PDF séparé qui offre cette fonctionnalité (Acrobat Reader, Mac Preview, SumatraPDF sous Windows, eVince sous Linux, …) pour visionner votre présentation confortablement. Enfin, quelques packages R additionnels proposent d’autres formats de présentation. Dans la SciViews Box, vous avez le package mdshower qui propose le moteur de présentation Shower, et xaringan qui propose un moteur particulièrement sophistiqué et flexible. Ces outils sont plus spécialisés, mais aussi plus puissants pour créer des présentations au format HTML à visionner dans un explorateur Web. Quel type de présentation R Markdown choisir au final ? Toute cette panoplie d’options ne facilite pas notre choix. En fait, c’est plus une question de goût personnel. Essayez les différentes options par vous-même. Le choix principal est au final entre un format HTML ou PDF. Le format PDF est, par définition, plus portable. Cependant, il ne permet que du contenu statique. Si vous avez des gifs animés, des graphiques interactifs, ou des vidéos, alors orientez-vous plutôt vers un moteur HTML/Javascript. Dans la prochaine version de la SciViews Box, il sera également possible de générer ses présentations directement au format PowerPoint. A vous de jouer appliquer vos connaissances dans la seconde partie du projet spécifique du module La seconde partie porte sur la réalisation d’une présentation. Le lien pour débuter ce projet ce trouve en début de module 12 http://biodatascience-course.sciviews.org/sdd-umons/correlation.html Vous pouvez activer les sous-titres en anglais via la barre de boutons en bas de la vidéo.↩ Toutes nos présentations dans le cadre du cours sont au format R Markdown/Beamer avec un template UMONS/SDD fortement personnalisé. Dans le cadre de vos exercices, vous aurez accès à un dépôt GotHub Classroom qui vous prose un template similaire.↩ "],
["critique-statistique.html", "12.3 Critique statistique", " 12.3 Critique statistique Les statistiques ont mauvaise presse auprès de certaines personnes qui pensent qu’on peut leurs faire dire tout et son contraire. Cela a même donné lieu à des ouvrages comme “Attention, statistiques ! Comment en déjouer les pièges” par Joseph Klatzmann ou “How to lie with statistics” par Darrell Huff. Derrière des titres provocateurs, ces ouvrages présentent, en fait, de manière sérieuse les pièges principaux et les moyens de les déjouer. Car, en réalité, ce n’est pas l’usage des statistiques qui est en cause ici, mais son mauvais usage. Voir aussi “Statistical reasoning for everyday life”, par Bennett, Briggs &amp; Triola. Dans la littérature scientifique et tout autour de nous, nous pouvons trouver des exemples de mauvais usages des statistiques (application erronée de méthodes statistiques). Quelque fois, il s’agit de triche manifeste, mais la plupart du temps c’est par ignorance. Développer un esprit critique statistique est important pour pouvoir démasquer ces diverses situations et ne pas tomber soi-même dans les pièges les plus grossiers. Voici quelques conseils qui vous aideront à développer votre esprit critique statistique. La formulation statistique est-elle en adéquation avec la question biologique posée ? Y-a-t-il des biais dans les techniques d’échantillonnage et/ou de mesure ? Les graphiques sont-ils adéquats par rapport à ce qui doit être montré ? Les axes sont-ils placés correctement, et sont-ils bien libellés ? Le graphique respecte-t-il les conventions ? Les unités sont-elles correctes ? Les calculs sont-ils corrects ? Les variables sont-elles du type correct pour l’analyse (qualitative ordonnée ou non, ou alors, quantitative discrète ou continue) ? Les conditions d’application des tests statistiques sont-elles respectées ? La taille de l’échantillon est-elle suffisante ? N’y a-t-il pas pseudo-réplication (plusieurs mesures issues d’un même individu considérées comme des observations indépendantes) ? Les grandeurs observées sont-elles plausibles ? Vous pouvez vous rapporter à des éléments connus et comparer. Par exemple, si l’on vous dit qu’une souris adulte pèse 1g, est-ce plausible ou non ? Faite une recherche sur le Web, ou un raisonnement du genre : une souris est constituée principalement d’eau. Un gramme d’eau occupe un volume de 1 cm3. Le volume de la souris adulte est-il supérieur, égal ou inférieur à un cube de 1 cm de côté ? Les mêmes données ne peuvent être utilisées deux fois. Si elles sont utilisés pour découvrir un effet, et en même temps pour le vérifier, c’est incorrect. Une corrélation ou un effet fortuit n’est-il retiré d’une grande quantité de tests non significatifs ? Soyez attentifs aux tests multiples réalisés sans ajustement du seuil \\(\\alpha\\). Les conclusions sont-elles en adéquation avec ce qui est observé dans les données ? Les conclusions répondent-elles à la ou les questions posées initialement ? Pour terminer ce module, nous vous proposons quelques situations (soit des problèmes, soit des graphiques) qui ont toutes en commun d’être erronées. A vous de trouver ce qui ne va pas. Pour ne pas fausser la donne, les réponses ne sont pas fournies dans ce documents, mais seront discutées en classes tous ensembles. Graphe en rubans Que pensez-vous du graphique suivant ? Longévité Un chercheur compile les statistiques de longévité de diverses professions. Pour ce faire, il encode les données des certificats de décès (nom, âge au moment du décès et profession). Il calcule ensuite l’âge moyen de décès par profession. Il constate que la valeur minimale est observée chez les étudiants, avec une valeur moyenne de seulement 20,7 ans (Wainer, Palmer &amp; Bradlow, A selection of selection anomalies, Chance, vol. 11, n°2). La « profession » d’étudiant est-elle réellement plus dangereuse que celle de policier, chauffeur de taxi, ou cascadeur ? Expliquez… Corrélations Vous en pensez quoi ? Prison Qu’est-ce qui ne va pas dans la figure suivante ? Etendue des axes Comparez de manière critique les deux graphiques précédents. Aidez-vous des schémas ci-dessous pour étayer votre explication. Travaux d’artistes ? Que pensez-vous des trois figures suivantes ? Chauve-souris Un biologiste étudie une chauve-souris insectivore naine. Il trouve dans la littérature que la biomasse totale de cette chauve-souris varie de 0,23 à 1,95 kg/ha dans les forêts recensées. Afin de calculer l’abondance de ces populations de chauve-souris, il détermine le poids moyen d’un individu comme étant (moyenne ± écart type) 55 ± 13 mg (n = 45). Il utilise ces données pour comparer les populations de chauve-souris aux autre animaux présents dans cette forêt. Il en conclu que la population de chauve-souris dans ces forêts est très nettement supérieure à celle des oiseaux et équivalente à celle des insectes. Ce résultat est inattendu et permet de considérer cette chauve-souris comme espèce clé dans la chaîne trophique, alors que son effet a toujours été négligé auparavant, tant elle est discrète et passe inaperçu la plupart du temps. Vous travaillez aussi sur les chaînes trophiques de ces mêmes forêts. Comment réagissez-vous à la lecture de ce rapport ? Que faites-vous ensuite ? Patinage Que se passe-t-il si tout le mode respecte cette consigne (considérant qu’il est impossible que tous les patineurs aient exactement la même vitesse) ? Pseudo-perspective Que pensez-vous de ces graphiques ? Aidez-vous du schéma suivant pour expliquer ce qui ne va pas… Homme moyen Le magazine “Men’s Health” a publié des statistiques qui décrivent l’“homme moyen”. Celui-ci a 34,4 ans, pèse 79,4kg, mesure 177,8cm, dors 6,9 heures chaque nuit, bois 3,3 tasses de café par jour et consomme 1,2 boisson alcoolique quotidiennement. Sachant que toutes les distributions sont unimodales, donc que les valeurs moyennes correspondent toutes à des observations effectivement mesurées en grand nombres (identiques ou très proches) sur des hommes réels, ce portrait robot de l’“homme moyen” décrit-il effectivement un grand nombre d’individus réellement existants ? Justifiez. Qu’en serait-il de l’“homme médian” ? Public ou privé ? Observez bien le grahique ci-dessus… Ensuite, regardez celui ci-dessous qui est réalisé à l’aide des mêmes données. Commentez… Camemberts, tartes et cie Que pensez-vous de ces graphiques ? Espérance de vie L’espérance de vie est une donnée statistique qui permet de connaître la durée de vie moyenne qu’on peut espérer atteindre à un moment donné pour une nation donnée. Cette statistique est calculée et publiée par de nombreux organismes, incluant l’OMS. Les statistiques indiquent que l’espérance de vie des hommes dans nos pays est de 75,5 ans, et des femmes de 83,5 ans. Calculez le temps que vous pouvez espérer encore vivre en fonction de votre âge. Que pensez-vous de ce calcul ? Femmes au travail Considérez les deux graphiques suivants qui sont sensés représenter la même information (les mêmes données sont utilisées). Comparez-les de manière critique. Moules Un scientifique mesure la stabilité de la membrane lysosomale (indice de stress des cellules utilisé en écotoxicologie : on sait que les polluants étudiés tendent à déstabiliser la membrane des lysosomes) chez la moule Mytilus edulis en Mer du Nord. Deux régions sont comparées : la pleine mer (A), et l’embouchure de l’Escault dans sa partie considérée comme la plus polluée (B). Cinq moules sont prélevées aléatoirement sur les deux sites, et dix mesures sont réalisées sur chaque individu. Le scientifique conclu à une stabilité lysosomale significative plus faible au seuil alpha de 5% dans le site B (test de Student non apparié et unilatéral à gauche, t = -6,5, ddl = 49, valeur P &lt; 0.001). Que pensez-vous de cette étude ? République bananière ? Que pensez-vous du graphique suivant ? Euro manquant Trois clients dans un restaurant payent leur repas : 30€ (10€ par personne). Le serveur se rend compte qu’en fait leur repas n’a coûté que 25€ en tout. Comme il ne pourra diviser les 5€ à rendre en trois facilement, il décide de garder 2€ dans sa poche et rend 1€ à chaque client. Donc, chaque client a payé 10 – 1 = 9€, soit un total de 27€. Avec les 2€ que le serveur a gardé dans sa poche, cela fait 29€. Alors, où est passé l’euro manquant par rapport aux 30€ payés initialement ? Réfléchissez et dénoncer l’erreur de raisonnement dans le récit précédent. Stylos et vers verts ? Que pensez-vous de ces graphiques ? Vous pouvez vous aidez des schémas suivants pour étayer votre réponse. Insecticides Un chercheur dans une industrie chimique s’intéresse à l’effet d’un nouvel insecticide à effet progressif. Il teste son produit sur des drosophiles et observe une mortalité de 10% par jour, et ce, quel que soit le moment où il effectue les mesures après avoir mis les mouches en contact avec l’insecticide. Il en conclu qu’il faut 10 jours pour tuer toutes les mouches. Ce résultat est meilleur que le produit du concurrent, car ce dernier tue 80% des mouches sur la même durée de 10 jours. Que pensez-vous de la façon dont cette expérience a été menée et de ses conclusion ? Virus zika Ca ne s’invente pas ! Lotto Par le plus grand des hasards, le numéro 8 est sorti 6 fois en 7 tirages successifs du lotto. Sachant qu’une vérification de ce que ce numéro n’a pas plus de chances que les autres d’être tiré au sort, vous ne manquerez pas de constater en bon statisticien(ne) que le numéro 8 est très nettement sur-représenté dans les tirages. La prochaine fois que vous remplirez votre grille de lotto, jouerez-vous le numéro 8 ? Pourquoi ? Vous est-il arrivé de jouer la suite 1, 2, 3, 4, 5, 6, 7, 8 au lotto (ou rempliriez-vous une grille avec ces nombres si vous deviez y jouer) ? Pourquoi ? A vous de jouer appliquer vos connaissances dans la dernière partie du projet spécifique du module La dernière partie porte sur la critique statistique. Le lien pour débuter ce projet ce trouve en début de module 12 http://biodatascience-course.sciviews.org/sdd-umons/correlation.html "],
["svbox.html", "A Installation de la SciViews Box", " A Installation de la SciViews Box La SciViews Box est une machine virtuelle (c’est-à-dire, l’équivalent d’un ordinateur complet, mais “dématérialisé” et utilisable à l’intérieur de n’importe quel autre ordinateur physique). Elle est spécialement configurée pour analyser des données et rédiger des documents scientifiques de manière professionnelle. Dans notre cas, le logiciel de gestion de la machine virtuelle, l’hyperviseur, est VirtualBox. C’est un logiciel gratuit qui existe pour Windows, MacOS et la plupart des systèmes Linux. L’avantage d’utiliser une machine virtuelle dans le contexte qui nous concerne ici est double : Elle est complètement pré-configurée et pré-testée. Comme tout le monde utilise la même machine virtuelle, les résultats obtenus chez l’un sont parfaitement reproductibles chez d’autres. L’installation est simple, mais il y a quand même quelques pièges. Suivez le guide… "],
["prerequis-11.html", "A.1 Prérequis", " A.1 Prérequis Avant d’installer la SciViews Box, vérifiez que votre ordinateur répond aux conditions requises et qu’il est correctement configuré. A.1.1 Ordinateur La SciViews Box, et la Science des Données en général, nécessitent un ordinateur ayant une puissance de calcul suffisante. Les tablettes et autres chromebooks sont donc exclus (sauf à être utilisés comme simples browsers web avec les calculs déportés sur un serveur, voir par exemple Chromebook Data Science). Si l’utilisation d’un serveur est une bonne idée pour l’apprentissage, ce n’est pas une solution sur le long terme pour tout le monde. En effet, vous êtes et restez dépendant du serveur que l’on a bien voulu configurer et partager avec vous (sera-t-il encore disponible après votre cours, par exemple ?). La solution proposée avec la SciViews Box vous rend complètement autonome dès le départ. Le choix d’un ordinateur ayant une capacité de calcul suffisante n’est pas aisé et les disparités en matière de performances sont énormes, voir NovaBench CPU score. La configuration de référence est la suivante, avec un score global Novabench d’environ 1000 : Processeur : à 2 ou 4 coeurs / 4 threads d’une vitesse de calcul suffisante (score CPU Novabench d’environ 500). Mémoire vive : 8Go avec un score RAM Novabench d’environ 200. Disque dur : disque rapide SSD de 256Go (score disque Novabench d’environ 75 avec vitesse d’écriture &gt;= 300Mo/s et vitesse de lecture &gt;= 400Mo/s). Affichage : 1920x1080 ou mieux. La plupart des cartes graphiques ou des coprocesseurs graphiques intégrés conviennent (pas besoin d’une bête de course si vos calculs ne nécessitent pas des instructions GPU, en tous cas). Comme base, nous considèrerons un score GPU NovaBench d’environ 200 qui correspond au processeur graphique intégré Intel HD 620. Réseau : Wifi à la norme 802.11ac. Connectique : USB 3.0 ou C pour ajouter des périphériques, HDMI ou DisplayPort pour connecter un écran externe, et une prise casque pour visionner des vidéos sans déranger les voisins. Portabilité et autonomie : 13 pouces avec un poids &lt; 1,5kg pour un ordinateur à enmener partout, sinon 15 pouces et un poids &lt;= 2kg. Autonomie d’au moins 5 à 6h. Système d’exploitation : récent et si possible 64-bit. Windows 7 ou plus convient (mais pensez à mettre-à-jour vers Windows 10), MacOS 10.10 Yosemite ou plus, ou un Linux tel Debian 8 (Jessie) ou 9 (Stretch), Ubuntu 16.04 Xenial ou supérieur, … Une configuration “standard” avec au moins 20Go de libre pour la SciViews Box (nécessairement sur le disque C: sous Windows), et une configuration non “bidouillée” (répertoire utilisateur et programmes standards, entre autres). Pour une configuration de base, vous pouvez aller jusqu’à diviser les scores Novabench et les valeurs (nombre de coeurs CPU, taille de la mémoire vive et du disque) par deux et accepterez un disque dur classique à la place d’un disque SSD plus rapide. Pour une configuration performante, multipliez les scores Novabench par deux, et ajoutez-y éventuellement une carte graphique Nvidia performante pour des calculs GPU et un second disque de 1To ou plus pour stocker des gros jeux de données. Un “laptop” (ordinateur portable) est mieux, mais si vous n’avez pas besoin d’une solution nomade, un “desktop” convient aussi et est plus modulable. Pour tester votre système, nous vous proposons donc d’utiliser le logiciel gratuit pour un usage personnel Novabench. Vous obtiendrez un rapport, voir ci-dessous un résultat pour un PC en configuration de référence64 (Asus Zenbook UX330U) et un Mac en configuration optimale MacBook Pro 15 pouces mid-2015. Vous pourrez comparer à la configuration de référence : A.1.2 Activation de la virtualisation La virtualisation fait appel à un jeu d’instructions disponible sur pratiquement tous les processeurs modernes (Intel VT-x ou AMD-v). Malheureusement, elle est désactivée par défaut sur quasi tous les PC (mais les Macs sont, eux, configurés correctement en sortie d’usine). Tant que ces instructions de virtualisation ne seront pas activées, le programme d’installation de la SciViews Box va bloquer avec le message suivant: Même si vous arriviez à l’installer quand même, vous ne pourriez pas la démarrer, et verriez juste un message de ce type (issu d’une version antérieure de la SciViews Box): Pour activer ce jeu d’instructions, il faut aller dans le BIOS ou l’**UEFI*, c’est-à-dire, le petit programme qui définit comme se fait le démarrage de votre ordinateur. Il n’y a malheureusement pas de recette unique car chaque constructeur a sa propre façon de faire. De plus, l’endroit où il faut aller dans les menus de configuration du BIOS/UEFI diffère aussi d’un ordinateur à l’autre. Cependant, la procédure générale est la suivante: Redémarrer l’ordinateur, Au tout début du démarrage, il faut appuyer sur une touche ou une combinaison de touches (par exemple, DEL, F2, …). Restez à l’affût d’un message furtif qui l’indique à l’écran, Une fois entré dans le BIOS, repérez l’entrée correspondant au jeu d’instructions de virtualisation. Vous aurez plus de chances en regardant dans le menu relatif au processeur, ou dans les options avancées. Recherchez une entrée de type “Virtualisation”, “Intel Virtual Technology”, ou “Instructions AMD-v”. Activez cette option (cela n’aura aucun effet sur les logiciels que vous avez installés jusqu’ici et qui n’utilisent pas cette fonction), Sortez du BIOS/UEFI en sauvegardant les modifications (suivez les instructions à l’écran), Redémarrez l’ordinateur. Si vous n’arrivez pas à entrer dans le BIOS/UEFI, ou à trouver l’entrée correspondante dans celui-ci, rechercher “BIOS Virtualization” ou “UEFI Virtualization” accompagné de la marque et du modèle de votre ordinateur dans votre moteur de recherche internet favori. Vous y trouverez certainement des instructions plus précises relatives à votre ordinateur. Ce site liste quelques uns de raccourcis claviers à utiliser en fonction de la marque des ordinateurs pour entrer dans le BIOS et explique la procédure pour entrer dans l’UEFI depuis Windows 10. Si votre ordinateur est conforme aux spécifications ci-dessus, et si la virtualisation est activée, vous êtes maintenant prêt à installer votre SciViews Box! Dans ce cas, passez directement à la section A.2. Si vous n’avez pas encore d’ordinateur et souhaitez en acheter un, voyez quelques conseils utiles ci-dessous pour faire le bon choix. A.1.3 Conseils pour acheter un PC Pour les étudiants de l’UMONS et ceux de Charleroi, vous ne devez pas posséder votre propre ordinateur pour suivre le cours de science des données biologiques. Des ordinateurs en configuration de référence (voir ordinateur) vous sont accessibles au cours et aux travaux pratiques, et en dehors des heures (demander un accès au secrétariat des sciences à la salle “Escher” pour l’UMONS et voyez avec le secrétariat interfacultaire à Charleroi). Toutefois, si vous pouvez acquérir un ordinateur personnel, cela vous apportera un confort et une flexibilité indéniable, à condition de bien le choisir ! Si vous suivez les directives ci-dessus, vous ne pourrez pas vous tromper. Mais comme vous n’aurez probablement pas la possibilité de tester les ordinateurs avec Novabench avant l’achat, voici quelques exemples de configurations types et leur coûts approximatifs. Attention : cette analyse est réalisée en octobre 2018. Le matériel informatique et les prix changent constamment, et les informations sont rapidement obsolètes. Les lignes directrices devraient cependant rester valables un certain temps : recherchez les modèles plus récents qui succèdent à ceux proposés ici dans le catalogue des constructeurs. L’élément le plus important étant la vitesse de calcul du processeur, obtenez la référence du processeur de l’ordinateur que vous convoitez et recherchez-le ici. Comparer alors son score à notre configuration de référence, et faites-en de même pour les autres caractéristiques (mémoire vive, type et taille du disque dur, etc.) Prenez aussi comme référence les ordinateurs de votre salle de travaux pratiques. Par exemple à l’UMONS, dans la salle “Pentagone”, les ordinateurs sont équipés d’Intel Core i5-6400T @ 2.20Ghz. Ce sont des processeurs 4 coeurs/4 threads avec un score CPU Novabench de 511. Ils possèdent également 8Go de mémoire vive, un disque dur SDD avec 190Go dédiés à Windows 10 64-bit, et un processeur graphique intégré qui affiche 1920x1080 pixels à l’écran. Aux salles “Escher” et “Turing”, ce sont des processeurs Intel Core i5-4590 @ 3.30Ghz, également 4 coeurs/4 threads affichant un score CPU Novabench de 515. Le reste de leurs configurations est similaire à celle des ordinateurs “Pentagone”. Vous pouvez également vous baser sur d’autres tests, les CPUMarks et comparer le processeur de l’ordinateur que vous voulez acheter avec les scores de vos machines des salles de travaux pratiques à partir de cette page. Lors du choix de votre ordinateur, il faut tout d’abord vous demander si vous voulez un ordinateur pour apprendre à traiter des données sur des petits tableaux, et que vous prévoyez de le changer dans 1 ou 2 ans (dans ce cas, une configuration de base convient), ou si vous voulez investir sur du plus long terme. Visez alors plus haut. Naturellement, le prix sera un critère fondamental, également65. Justification des besoins : Processeur : l’élément le plus important. Un processeur puissant et multitâche est indispensable. Il vous faut au moins 2 coeurs et 4 threads (selon les modèles, chaque coeur peut gérer une seule tâche -ou “thread” en anglais- ou deux). Un processeur 4 coeurs/4 threads est encore mieux, et à partir de 4 coeurs/8 threads, c’est parfait. Pour la vitesse de calcul, comme indiqué plus haut, un score CPU Novabench de 500 ou mieux, ou un CPUMark de 5000 ou mieux doit être visé pour une configuration de référence ou performante. Avec un score moitié moindre, c’est encore un processeur utilisable, mais ne descendez pas en dessous pour une configuration de base. Mémoire vive : il vous faut suffisamment de mémoire pour la partager entre la machine hôte et la machine virtuelle, et garder assez de resources pour ouvrir des tableaux (moyennement) volumineux. Donc, visez 8Go de mémoire vive si possible. Pas moins de 4Go, et plus vous en avez, mieux c’est. Il existe des configurations laptops à 16Go voire plus. C’est utile mais non nécessaire dans le cadre des cours de science des données biologiques ! Disque dur : ici, vous devrez peut-être faire un choix entre espace de stockage et vitesse du disque. En effet, les disques mécaniques classiques font maintenant facilement 1To, ce qui est confortable. Par contre, ils sont plus lents que les disques SSD qui sont à privilégier. Mais ces derniers sont de capacité moindre (dans des gammes de prix raisonnables), généralement 128Go ou 256Go. Des configurations plus haut de gamme combinent deux disques : un SSD rapide pour le système et un disque de 1To classique pour les données. C’est l’idéal. Si vous investissez dans un ordinateur ayant un disque dur SSD rapide mais pas assez gros pour contenir vos nombreux fichiers, photos, vidéos, morceaux de musique, etc., vous pourrez toujours compléter votre configuration avec un disque dur de 1To externe USB 3.0 pour une cinquantaine d’euros. Pensez aussi à investir dans une clé USB de 8 ou 16Gb pour transférer vos données. Faites attention de bien choisir un modèle USB 3.0 reconnaissable à son connecteur bleu, infiniment plus rapide qu’un modèle USB 2.0. Il vous en coûtera une dizaine d’euros. Carte graphique et écran : la qualité de la carte graphique est moins importante ici. La plupart des configurations actuelles conviennent. Voyez plutôt la taille (et donc, le poids) qui est un critère important pour un ordinateur portable. Voulez-vous un PC de 13 ou 14 pouces plus compact et transportable, ou un 15 à 17 pouces plus confortable, mais plus lourd ? Pour la résolution d’écran, ne descendez pas en dessous de 1400x900 pixels pour un travail confortable (RStudio affiche plusieurs fenêtres côte-à-côte), et vérifiez visuellement si la qualité de l’écran vous convient. Wifi et accessoires: une bonne connexion Wifi sera nécessaire pour vous connecter à Internet. La norme WiFi 802.11ac est idéale. Enfin, vérifiez les connexions proposées : USB rapide (3.0, 3.1 ou C), Thunderbold, DisplayPort, HDMI, etc. pour connecter des périphériques et des écrans externes, lecteur de cartes éventuel, etc. Actuellement l’USB C tend à se généraliser comme connecteur universel. D’autres critères comme la qualité de construction, la robustesse, la qualité du clavier et du trackpad éventuel, l’autonomie pour un portable, … sont importants. Pensez à consulter les tests détaillés effectués par des pros avant de vous décider, par exemple, les numériques en français, PCMag ou techradar en anglais. Voici quelques configurations types qui conviennent, volontairement choisies chez différents constructeurs pour ne privilégier personne. Dans le cadre de vos études, vous allez certainement vouloir emporter votre ordinateur avec vous partout. Nous vous présentons donc des ordinateurs portables de moins de 2kg, plus adaptés à cet usage. A.1.3.1 Configurations de base Evitez autant que possible de descendre en dessous de celles-ci. Dites-vous bien que ces machines sont estampillées “bureautique”, et sont trop juste pour analyser des gros jeux de données, mais elles peuvent convenir parfaitement dans le cadre du cours de science des données biologiques. Si vous possédez déjà un PC, faites un bilan avec Novabench et décidez par vous-même si vous pouvez ou non l’utiliser de manière confortable, éventuellement en installant la SciViews Box 2018 et en testant ainsi directement. Les options existent aussi pour “booster” un ordinateur un peu juste : ajout de mémoire vive et/ou remplacement du disque dur par un disque SSD rapide. Modèle Processeur [c/t] (nova/cpu) Mémoire Disque Graphique Ecran Poids Prix Lenovo IdeaPad 320S-14IKB Core i3-7100U [2/4] (406/3798) 8Go SSD 128Go Intel HD620 14’’ (1920x1080) 1.7kg 500€ Acer Swift 3 Core i3-8130U [2/4] (579/5061) 4Go SSD 256Go Intel HD620 14’’ (1920x1080) 1.45kg 600€ MacBook Air Core i5-5350U [2/4] (363/3358) 8Go SSD 128Go Intel HD6000 13.3’’ (1440x900) 1.35kg 1000€ Avec un budget de 500€-600€, des concessions sont nécessaires. A titre d’exemple, nous reprenons deux configurations sous Windows ici. Le Lenovo choisi a 8Go de mémoire vive, mais un disque SDD de faible capacité (128Go) et un processeur un peu moins puissant. L’Acer a un plus gros disque et un meilleur processeur (toujours i3, cependant), mais n’a que 4Go de mémoire vive. Toutefois, un seul disque dur de seulement 128Go, c’est quand même fort juste sous Windows 10 qui est déjà très gourmand en espace disque à la base. Donc votre préférence ira si possible plutôt vers une configuration du type Acer Swift 3 ci-dessus66. Des versions avec processeur Core i5 et 8Go de mémoire existent. Elles sont parfaites, … mais le prix les alignent presque avec nos configurations de référence ci-dessous. Vous verrez aussi dans les tests que ces machines ne sont pas irréprochables, mais il faut mettre les “défauts” relevés en regard du prix très contenu, et relativiser. Pour la science des données, nous privilégierons des ordinateurs aux processeurs plus rapides, quitte à être un peu moins bien cotés dans les tests sur la qualité de l’écran (comme l’Acer, par exemple). Du côté Mac portables, nous avons le MacBook 12’’ et le MacBook Air, présenté ici. Ils sont beaucoup plus chers, mais ce sont des ordinateurs durables et bien finis qui se revendent très bien. Dans les deux cas, le processeur (même si Core i5, ou i7) est fort juste et est plus à l’aise en bureautique. Pour analyser des petits jeux de données, ça fonctionne quand même bien. Ici aussi, des concessions sont nécessaires sur la capacité du disque dur pour tirer le prix à … 1000€ tout de même ! Ce MacBook Air existait depuis 2015 quasiment inchangé, mais a été changé dans la gamme du constructeur plus tôt dans l’année. Nous l’avons laissé dans la liste car vous pouvez encore en trouver en occasion à prix équivalent aux deux autres machines, mais faites attention à éviter les modèles anciens à processeurs lents qui se vendent encore, et qui ne sont pas assez puissants ! Un modèle avec un disque de 256Go existe aussi. Si votrte budget le permet, envisagez tout de même le nouveau modèle ! A.1.3.2 Configurations de référence Avec un budget un peu plus élevé, vous êtes nettement plus confortable : processeur assez rapide et 8Go de mémoire vive et disque SSD de 256Go. Ces laptops sont parfaits pour le cours de science des données biologiques et pour bien d’autres tâches dans le cadre de vos études. Modèle Processeur [c/t] (nova/cpu) Mémoire Disque Graphique Ecran Poids Prix HP Pavilion X360 Core i5-8250U [2/4] (801/7667) 8Go SSD 256Go GeForce MX130 14’’ (1920x1080) 1.6kg 900€ Acer Swift 5 Core i5-8250U [2/4] (801/7667) 8Go SSD 256Go Intel HD620 14’’ (1920x1080) 0.97kg 900€ MacBook Pro 13’’ Core i5-8259U [2/4] (???/10938) 8Go SSD 256Go Intel Iris+640 13’’ (2560x1600) 1.4kg 1600€ Le HP Pavilion est un portable à écran tactile représentatif de ce créneau (pour les modèles les plus puissants de la gamme en tous cas). Vous combinez un bon processeur, 8Go RAM, un disque dur rapide de 256Go, une carte graphique accélérée et un écran correct pour l’usage prévu pour un poids raisonnable. L’Acer Swift 5 est repris ici pour son poids plume et ses résultats excellents aux tests, mais sa carte graphique est en retrait par rapport au HP (élément secondaire pour la science des données). Un autre très bon exemple de machine portable qui convient parfaitement pour la science des données. Vivement conseillé, donc. Nous avons aussi inclu le premier MacBook Pro en version disque de 256Go à titre de comparaison (testé en version 2017) : il est plus cher mais à ce prix, vous avez tout de même un écran incomparablement meilleur, un processeur très rapide et une valeur à la revente bien plus haute. Chez Apple, restez dans la gamme MacBook Pro en laptops. Les processeurs des autres modèles les font tous entrer dans la catégorie de base. Attention aussi au prix des adaptateurs supplémentaires souvent indispensables pour les produits Apple ! N’oubliez pas de demander votre remise “éducation”, sur présentation de votre carte d’étudiant (le tarif indiqué tient compte de cette remise). A.1.3.3 Configurations performantes Un budget plus large permet d’acquérir un laptop de course qui sera utile pendant des années, et même pour un travail lourd plus tard… Dans ces configurations, pas de concessions. On veut un processeur i7 à 4 ou 6 coeurs ou équivalent, 16Go de RAM, un disque SSD d’au moins 512Go, ou mieux deux disques, une carte graphique rapide (sauf sur les ultra-portables) et un excellent écran. Modèle Processeur [c/t] (nova/cpu) Mémoire Disque Graphique Ecran Poids Prix MSI GF63 Core i7-8750H [6/12] (1400/12548) 16Go SSD 256Go + HDD 1To GeForce GTX1050Ti 15.6’’ (1920x1080) 1.86kg 1300€ Dell XPS 13 9370 Core i7-8550U [4/8] (837/8327) 16Go SSD 512Go Intel HD620 13.3’’ (1920x1080) 1.1kg 1400€ Asus ZenBook Flip Core i7-8550U [4/8] (837/8327) 16Go SSD 512Go Intel HD400 13.3’’ (1920x1080) 1.1kg 1600€ MacBook Pro 15’’ Core i7-8750H [6/12] (1400/12548) 16Go SSD 512Go Radeon Pro 555X 15.4’’ (2880x1800) 1.83kg 2700€ Le MSI est un PC dit “gamer”. Tous les laptops dans cette catégorie sont très rapides… et conviennent parfaitement bien pour la science des données, y compris pour les calculs GPU. Chez MSI, un modèle comme le GS65 Stealth obtient le label de meilleur “gamer laptop 2018” (chez techradar) et des modèles similaires sont très bien placés ailleurs. Mais nous préférons le GF63 moins cher car équipé d’une carte graphique un cran en dessous, et du coup, mieux positionné en rapport qualité/prix pour les sciences des données. Il est aussi nettement moins lourd. Chez Asus (ROG) et Lenovo entre autres, des machines quasi-équivalentes existent aussi. Attention : d’autres configurations de “laptops gamers” sont lourdes et elles chauffent beaucoup. Dans la catégorie ultraportable, on trouve aussi diverses machines plus puissantes. Le Dell XPS 13 de 13’’ est l’un des mieux classés systématiquement dans les tests un peu partout et représentatif de ce type d’ordinateurs ultra-compacts, légers, mais très performants. Poids plume oblige, on a un processeur moins puissant que sur un PC gamer (vérifiez qu’il soit suffisamment performant sur le modèle choisi, en effet, il existe plusieurs processeurs i5 et même i7 trop lents) et une carte graphique plus basique (pas grave). Choisissez un modèle avec 16Go de RAM et au moins 256Go de disque dur. Le Huawei Matebook X Pro est une splendide machine ultraportable avec un magnifique écran de 3000x2000 pixels (excusez du peu !), mais il n’est pas repris ici, car difficile à trouver en Belgique. Un test complet est aussi disponible. Ici, vous choisirez la version i5 avec processeur Core i5-8550U, 8Go de RAM et un disque SSD de 256Go pour une configuration de référence à 1500€, mais pourrez opter pour le i7 avec un Core i7-8250U, 16Go de RAM et un disque SDD de 512Go pour une configuration optimale pour environ 2000€. C’est l’un des meilleurs ordinateurs que l’on pouvait trouver fin 2018 ! Toujours dans les ultraportables, vous trouverez aussi les convertibles. Ceux équipés d’un écran tactile et qui peuvent se “retourner” pour s’utiliser comme une tablette haut de gamme. Les ordinateurs de type Microsoft Surface en sont les représentants emblématiques, mais les tests nous conduisent aussi vers l’Asus Zenbook Flip comme l’un des meilleurs (et assurément, un excellent rapport qualité/prix). Ici, on trouve des versions en 13’’ et en 15’’ mais toujours très portables et puissantes. De très bonnes machines pour analyser ses données ! A titre de comparaison, l’équivalent chez Apple est également présenté (le MacBook Pro 15’’, avec option disque de 512Go et remise “éducation”). Ce dernier est à nouveau beaucoup plus cher. Mais son écran est incomparable, sa finition est impeccable, et il tourne sous MacOS naturellement pour les afficionados ! Les modèles 2019 corrigent des défauts de clavier et écrans des années précédentes et proposent aussi des version i7 à 6 coeurs, et même i9 à 8 coeurs. Voici le test du modèle 2017. C’est une excellente machine. Malheureusement, la tendance est au minimalisme pour la connectique : du USB-C et c’est tout. Cela oblige à acheter et à transporter des connecteurs supplémentaires. Les anciennes générations de MacBook Pro, “pré touch bar” se trouvent encore dans le marché de l’occasion à des prix proches de PC équivalents sous Windows. Ils sont un peu moins performants que les nouveaux, mais restent excellents, … et possèdent beaucoup plus de connecteurs intégrés (de bonnes affaires, donc). Voilà ! En espérant que ceci pourra vous aider au mieux dans le choix de votre outil informatique. Les ordinateurs cités ici sont déjà très anciens (ils le sont comme point de comparaison à l’époque où l’étude des minima matériels requis pour la SciViews Box ont été explorés en détails) ; Vous trouverez facilement des machines plus récentes au moins aussi performantes dans le commerce aujourd’hui.↩ Renseignez-vous au niveau des services étudiants et à l’AGE : des aides existent pour les étudiants boursiers qui souhaitent acquérir un ordinateur dans le cadre de leurs études.↩ Ce ne sont que des exemples. Recherchez des configurations équivalentes chez d’autres constructeurs aussi !↩ "],
["install.html", "A.2 Installation", " A.2 Installation Vous allez devoir d’abord installer VirtualBox, un logiciel gratuit et libre qui se chargera de gérer votre machine virtuelle. Ensuite, vous installerez la SciViews Box en elle-même. A.2.1 VirtualBox Récupérez l’installateur correspondant à votre système ici. L’installation avec tous les paramètres par défaut convient. Il se peut que vous voyiez un message vous indiquant que VirtualBox doit réinitialiser le réseau ou une autre ressource. Vérifiez que tous les documents en cours éventuels sont sauvegardés, et ensuite, vous pourrez continuer l’installation sans risques. De même, sous Windows, l’installateur de VirtualBox vous préviendra peut-être qu’il doit installer l’un ou l’autre périphérique. Vous pouvez également continuer sans craintes (précaution prise par Microsoft, mais ces périphériques fonctionnent bien). Cliquez donc sur “Installer” ici sans quoi ViurtualBox ne fonctionnera pas A.2.2 SciViews Box La procédure d’installation de la SciViews Box diffère selon le système d’exploitation. Reportez-vous à la sous-section correspondante pour Windows, MacOS ou Linux. A.2.2.1 Installation sous Windows Chargez l’installateur ici ou, pour les étudiants de l’UMONS et de Charleroi, récupérez-le depuis le disque StudentTemp de la salle informatique (sous-répertoire SDD\\Software\\SciViews Box 2019). Pensez aussi à placer le fichier svbox2019.vdi.xz dans le même répertoire que l’installateur svbox2019_win_setup.exe. Sinon vous devrez le télécharger lors de l’installation (il pèse tout de même 4,3Gb)! Lancez l’installation. Vous verrez l’écran suivant (probablement en version française sur votre ordinateur). Vous pouvez cliquer ‘Yes’/‘Oui’. Il s’agit seulement d’une précaution de Microsoft lorsqu’il ne connait pas l’éditeur du programme à installer, comme c’est le cas ici. Si le fichier svbox2019.vdi.xz n’est pas présent dans le même répertoire que le programme d’installation, il est à présent téléchargé (cliquez sur “Details” pour suivre l’opération): Une fois le téléchargement terminé, l’installation se poursuit. Vous verrez ensuite qu’il y a encore une opération obligatoire à lancer: la décompression du disque virtuel de la SciViews Box (svbox2019.vdi) via ‘7z’. En cliquant ‘Finish’, cette décompression démarre toute seule. N’interrompez surtout pas la décompression du disque virtuel! Sinon, votre SciViews Box ne pourra pas démarrer et vous devrez tout recommencer à zéro en désinstallant et réinstallant complètement l’application. Losque tout est installé, vous avez une nouvelle icône sur votre bureau. Poursuivez à la section suivante pour démarrer et paramétrer votre SciViews Box. En option, vous pouvez épingler le nouveau programme dans la barre des tâches. Il sera plus facilement accessible (voir ci-dessous). A.2.2.2 Installation sous MacOS Chargez l’installateur ici ou, pour les étudiants de l’UMONS et de Charleroi, récupérez-le depuis le disque StudentTemp de la salle informatique (sous-répertoire SDD/Software/SciViews Box 2019). Si vous le pouvez, placez le fichier svbox2019.vdi.xz dans le dossier de téléchargements (Téléchargements ou Downloads selon la version de votre MacOS), sinon ce fichier sera téléchargé au même emplacement (il pèse 4,3Gb)! Double-cliquez sur svbox2019_macos_setup.dmg. Suivez simplement les instructions. Déplacez à la souris ‘SciViews Box 2019’ vers le dossier ‘Applications’ dans la fenêtre de l’installeur (cette partie de l’installation est très rapide, donc, vous n’aurez peut-être pas l’impression que quelque chose se passe), Ensuite, toujours dans cette fenêtre, double-cliquez sur le dossier ‘Applications’ et recherchez l’entrée ‘SciViews Box 2019’. Double-cliquez dessus, Si vous avez chargé l’installateur depuis Internet, il se peut que votre Mac indique un message et vous empêche de l’ouvrir. Dans ce cas, il faut cliquer avec le bouton droit de la souris et selectionner “Ouvrir” dans le menu contextuel tout en maintenant la touche ALT ou ‘Option’ enfoncée, et ensuite cliquer “Ouvrir” dans la boite qui s’affiche. Laissez l’installation se terminer. Cela peut prendre plusieurs minutes. En option, vous pouvez aussi accrocher le programme de manière permanente dans le “Dock” pour le lancer facilement depuis cet endroit. Cliquez bouton droit et dans le menu “Options”, sélectionnez l’entrée “Garder dans le Dock”. A.2.2.3 Installation sous Linux Il est parfaitement possible d’installer la SciViews Box sous Linux. Cependant, un programme d’installation simplifié n’a pas encore été développé pour ce système. Voyez au cas par cas avec vos enseignants pour qu’ils vous expliquent comment installer la SciViews Box manuellement sous Linux. A.2.2.4 Migration et désinstallation Le disque dur virtuel de la SciViews Box est un fichier volumineux de plus de 15Go. L’installeur fait en sorte qu’il soit partagé entre plusieurs utilisateurs de l’ordinateur, et qu’il reste inchangé au cours de son utilisation. Ainsi, VirtualBox enregistrera dans vos dossiers personnels un fichier qui stocke les différences par rapport à l’état de départ de la Box. Il est donc possible de désinstaller partiellement la SciViews Box 2019 sans rien perdre. Pour cela, il suffit de désinstaller l’application (sous Windows, allez dans le panneau de configuration -&gt; Applications -&gt; SciViews Box 2019 -&gt; Désinstaller ; sous MacOS déplacez l’application SciViews box 2019 depuis le dossier Applications vers la corbeille et faites de même pour le fichier ‘/Users/Shared/SciViews/svbox2019.vdi’). Vous récupèrerez immédiatement près de 16Go d’espace disque. VirtualBox ne pourra plus démarrer la Box, naturellement, mais conservera vos données. Si besoin, vous pourrez réinstaller simplement l’application SciViews Box 2019 pour retrouver votre Box en l’état. Une désinstallation complète nécessite d’aller d’abord supprimer la machine virtuelle dans VirtualBox (clic bouton droit et sélection de Supprimer...) pour tous les utilisateurs qui ont créé une Box avant de désinstaller l’application principale comme ci-dessus. Si vous avez des projets créés avec des SciViews Box antérieures, deux solutions existent : Gardez-les tel quels. Faites éventuellement une désinstallation partielle de la Box. Vous pourrez toujours revenir plus tard sur ces projets après réinstallation. Migrez-les vers la nouvelle SciViews Box. Copiez vos projets depuis le répertoire shared de l’ancienne Box vers celui de la nouvelle. Dans ce cas, vous devrez vérifier que votre code fonctionne toujours sous la nouvelle Box, et l’adapter éventuellement. A.2.3 Github Desktop Dans ce cours, nous utilisons Git et Github pour gérer les différentes versions de vos projets et les partager avec vos binômes et vos enseignants. Github Desktop facilite grandement la gestion de vos projets sous Git. Ce programme gratuit est directement intégré dans la SciViews Box à partir de sa version 20189, mais elle est aussi très facile à installer en natif (optionel) : son téléchargement et le lancement de son installeur ne pose pas de problèmes particuliers. A présent, tous les ligiciels requis sont installés… Il ne reste plus que quelques petites opérations de configuration à réaliser. Voyez ceci à la section suivante. "],
["configuration.html", "A.3 Configuration", " A.3 Configuration Même si la SciViews Box est pré-configurée, vous allez avoir quelques manipulations simples à réaliser pour être complètement opérationnel. Ces étapes sont détaillées ci-dessous. Nous en profiterons par la même occasion par nous familiariser avec quelques uns des outils logiciels que vous utiliserez plus tard, à commencer par le lanceur rapide SciViews Box. A.3.1 Lanceur SciViews Box L’application que vous venez d’installer est un lanceur rapide qui facilite le démarrage, la fermeture et la gestion de votre machine virtuelle SciViews Box. Démarrez cette application et vous verrez la fenêtre suivante : Dans ce premier cours, vous n’utiliserez pas toutes ses fonctionnalités. Ainsi, vous n’aurez pas besoin de Jupyter(lab), Adminer ou SSH, mais repérez les autres outils. Le message en rouge n’apparait pas systématiquement. Il signale des éléments importants. Ici, il indique que la configuration de la SciViews Box doit encore être faite, et pour cela, vous devez (1) la démarrer à l’aide du gros bouton en haut à gauche, (2) vous logger (mot de passe = sv), et (3) répondre Yes lorsqu’une boite de dialogue vous propose de configurer la SciViews Box comme ci-dessous : Cette dernière étape est importante ! Ne cliquez pas No ici, sous peine de ne pas avoir une machine virtuelle configurée comme celle de vos collègues ! Le mot de passe vous sera redemandé, et ensuite, la configuration se poursuivra. Elle pourra prendre plusieurs minutes. Soyez patient. Vous pourrez ouvrir la fenêtre où s’opère le travail pour en suivre la progression, si vous le souhaitez. Pour ce faire, lancer une fenêtre de terminal (Win + T, Command + T, ou Control + T selon votre clavier et configuration). Ensuite entrez xfce4-terminal --drop-down suivi de la touche entrée. Sinon attendez simplement la fenêtre du configurateur de la SciViews Box qui va apparaître à la fin. A.3.2 Configurateur de la Box Prenez le temps de parcourir les différents éléments dans cette fenêtre67. La partie à gauche en haut concerne la configuration du clavier. En effet, la machine virtuelle utilisera votre clavier physique, mais elle n’a aucun moyen de déterminer de quel modèle il s’agit. Vous allez donc l’indiquer maintenant. Utilisez la zone de texte intitulée Test area (type here) pour vérifier que la machine virtuelle interprète correctement les touches de votre clavier. Pour le changer, cliquez sur le bouton Change keyboard layout. La boite de dialogue de sélection du clavier apparait. Elle propose des configurations différentes sous forme de représentations graphiques, avec les touches caractéristiques surlignées en jaune. Vous pouvez entrer les premières lettres du type de clavier pour aller directement à la configuration correspondante dans la liste (ex.: entrez be pour un clavier belge). Si votre clavier ne se trouve pas dans les templates les plus courants, configurez-le à l’aide du bouton Other keyboard.... Fermez cette fenêtre pour retourner au configurateur lorsque vous aurez fini. Attention que pour un clavier de Mac, vous devez utiliser la configuration intitulée fr(mac), comme dans la copie d’écran ci-dessus. Sur un PC, la configuration sera simplement be. Enfin, toujours concernant le clavier, la case à cocher Exchange left CTRL / CMD (Mac shortcuts) permet d’utiliser les raccourcis Mac (comme Command-c pour copier et Command-v pour coller à la place de Ctrl-c ou Ctrl-v sur un PC). Cette option n’est utile qu’aux possesseurs d’un Mac qui veulent avoir des raccourcis plus homogènes entre leur système MacOS hôte et la machine virtuelle68. Notez que VirtualBox réserve une touche clavier à son propre usage. Par défaut, c’est la touche Command ou Win de droite sur les anciennes version, mais il se peut que ce soit la touche équivalente de gauche. Dans ce cas, vous aurez un conflit avec l’option Exchange left CTRL / CMD keys Vous devez redéfinir la touche hôte de VirtualBox. Allez dans son menu Input -&gt; Keyboard -&gt; Keyboard Settings... Redéfinissez la touche utilisée par l’hôte comme étant la touche commande de droite. Revenons à notre configurateur à l’intérieur de la SciViews Box. Juste en dessous des paramètres de clavier, vous voyez la configuration du fuseau horaire. Ici aussi, votre machine virtuelle n’a pas l’information de votre système hôte, et peut donc ne pas afficher l’heure correctement. Vous avez la possibilité de corriger cela en cliquant sur le bouton Change time zone. Vous devez débloquer la boite de dialogue (bouton Unlock en bas, puis entrer le mot de passe pour pouvoir effectuer des changements). Les trois boutons suivants à gauche en bas servent à choisir le style des fenêtres, le set d’icônes et l’image d’arrière plan de votre SciViews Box. C’est ici que vous pourrez la paramétrer au mieux pour qu’elle vous plaise visuellement. A noter que, si vous double-cliquez sur les entrées dans les boites de dialogue de configuration, vous allez pouvoir prévisualiser l’effet en live. Utile pour apprécier le rendu avant de faire son choix ! Enfin dans la partie gauche, le dernier item permet de modifier l’apparence et le comportement du dock de la SciViews Box (qui s’appelle “plank”). Vous ne devriez pas le changer pour l’instant, mais vous pouvez quand même explorer les différentes options offertes ici. La zone en bas à droite permet de modifier le mot de passe. Pour rappel, il s’agit d’un mot de passe simple et peu sécurisé par défaut : sv. En fait, vous n’avez pas réellement besoin d’un mot de passe à l’intérieur de votre SciViews Box telle qu’elle est configurée car vous ne pouvez y accéder qu’en local à partir de l’ordinateur hôte. Par contre, il est possible d’ouvrir l’accès. A ce moment-là, il serait utile, et même indispensable, de modifier le mot de passe. Dans le cadre de votre utilisation de la SciViews Box pour ce cours, que ce soit sur les machines de la salle de T.P., ou sur votre ordinateur personnel, ne changez pas le mot de passe ! Votre machine virtuelle est déjà protégée par votre système hôte puisque seul un accès local est autorisé. La zone en haut à droite permet de configurer votre compte Git. Comme vous allez utiliser Git et Github de manière intensive tout au long de ce cours, veuillez configurer cette partie du système correctement d’amblée! Les trois boutons du bas proposent de s’enregistrer sur trois systèmes distants d’hébergement de dépôts Git (si vous ne savez pas ce que c’est, imaginez juste que c’est là que vous allez pouvoir entreposer de manière sûre tous vos projets !): Github, Gitlab ou Bitbucket. Tous trois ont des avantages et des inconvénients, et ils proposent tous des utilisations gratuites dans certains cas. Durant nos cours de Science des Données à l’UMONS et ceux de Bioinformatique et Science des Données sur le site de Charleroi, nous utiliserons Github. Cette utilisation sera gratuite pour vous, et vous allez déjà pouvoir commencer à construire votre identité professionnelle sur le Net par son intermédiaire. Donc, enregistrez-vous de manière sérieuse. Choisissez un login représentatif de vos nom et prénom, pas un truc louffoque ou rigolo sur le moment, mais que vous regretterez plus tard sachant que votre login ne pourra pas être changé plus tard ! Utilisez simplement une contraction facile à retenir de vos prénom et nom, telle que ce login n’existe pas encore sur Github (il faut parfois essayer plusieurs combinaisons différentes pour les noms courants). Vous allez donc vous créer un compte sur Github en cliquant sur le bouton correspondant, et en indiquant un login et un mot de passe. Nous vous demandons également d’utiliser expressément et uniquement votre adresse email UMONS ici : prénom.nom@student.umons.ac.be. En effet, ce sera, pour nous, notre seul moyen de vous identifier sans erreur sur Github lorsque nous interviendrons pour vous conseiller et/ou pour corriger vos travaux. Une fois enregistré sur le site de Github, reportez votre login et votre adresse email dans le configurateur de la SciViews Box, pour que Git puisse vous identifier correctement en local69. Après avoir effectué tout ceci,vous pourrez cliquer sur le bouton OK de la fenêtre du configurateur SciViews Box. La machine virtuelle devra redémarrer pour appliquer toutes les modifications de manière durable. Cliquez donc également OK dans la boite de dialogue qui apparait ensuite (sinon, elle redémarrera toute seule après 30 sec) : A.3.3 Installation des tutoriels Nous allons maintenant installer les tutoriels liés à ces cours de science des données biologiques. Vous allez apprendre par la même occasion comment ajouter des applications dans votre SciViews Box. Cela se fait en trois étapes: Télécharger l’installeur de l’application. Vous le trouverez à l’adresse http://go.sciviews.org/BioDataScience2. Assurez-vous de bien le charger dans le répertoire Downloads ou Téléchargements par défaut sur votre ordinateur70. Rentrez dans le lanceur rapide de la SciViews Box. Il repère les autoinstalleurs et les déplace dans le dossier partagé pour les rendre utilisables par la SciViews Box. Vous devez voir un message indiquant la disponibilité d’autoinstalleur(s). Vos fichiers téléchargés ont également disparus du répertoire de téléchargement à ce stade, Rentrez dans la SciViews Box depuis le lanceur. Si la Box était déjà active ou si elle est réveillée du mode veille, vous allez devoir vous délogger et relogger pour que l’installation démarre, … sinon, vous allez voir directement le message suivant qui propose d’installer ces apps (cliquez sur Yes, bien sûr, pour l’installer). Si le message n’apparait pas, voici comment se délogger (log out). Ensuite, entrez votre mot de passe comme d’habitude pour vous relogger… A ce moment, le message doit apparaitre, et l’installation doit se faire après avoir cliqué sur le bouton Yes. Bravo! Vous avez terminé l’installation et la configuration de votre SciViews Box. Cependant, nous allons encore effectuer une petite opération qui vous facilitera la vie, et nous vous expliquerons par la même occasion comment accéder aux fichiers respectifs de la machine virtuelle et du système hôte dans la section suivante. A.3.4 Accès aux fichiers Le disque physique de votre ordinateur hôte, et le disque virtuel de la SciViews Box sont deux choses différentes. Cela signifie que vous avez, en réalité deux ordinateurs et deux disques indépendants. Donc, vous n’accédez pas aux fichiers d’une machine à partir de l’autre71. Ce n’est pas pratique, et ce n’est pas vrai pour un dossier particuler nommé shared. Ce dossier shared est synchronisé en temps réel entre les deux systèmes. C’est donc l’endroit idéal pour échanger des données et pour faire collaborer vos deux machines. Inutile de préciser, donc, que nous travaillerons essentiellement à l’intérieur de ce dossier. Un sous-dossier, nommé projects sera utilisé pour héberger toutes nos analyses. Il est donc primordial d’y accéder facilement à la fois depuis l’ordinateur hôte et depuis la SciViews Box. Vous allez donc apprendre à retrouver ce dossier projects facilement. Sur votre ordinateur hôte, ce dossier est un peu difficile à trouver en naviguant dans l’explorateur de fichiers (ou le Finder sur le Mac). Pour cette raison, le lanceur rapide propose un bouton pour y accéder plus facilement. Une fois dans le dossier shared, nous vous conseillons d’épingler le sous-dossier projects dans les raccourcis rapides de votre explorateur de fichiers. Voici comment faire sous Windows et sous MacOS. Sous Windows, cliquez bouton droit sur projects, et sélectionnez “épingler dans Accès rapide”. Sous MacOS, vous glissez-déposez projects dans la barre latérale du Finder. Dans la SciViews Box, ce dossier est accessible depuis deux endroits: /media/sf_shared et ~/shared (~ représente le répertoire de l’utilisateur, c’est-à-dire /home/sv). Ici aussi vous pouvez épingler votre dossier projects pour en facilter l’accès: Les deux moyens d’accéder au dossier projects dans la SciViews Box (à partir de sf_shared ou de sv, puis `shared) et comment l’épingler sur le côté. A retenir: le dossier shared et ses sous-dossiers comme projects sont considérés un peu comme des dossiers réseau par la SciViews Box. Cela implique que certaines fonctions du système de fichiers n’y sont pas accessibles. Parmi celles-ci, la poubelle. Donc, vous ne pourrez qu’effacer complètement des items en cliquant bouton droit et sélectionnant ‘Delete’ dans le menu contextuel dans le gestionnaire de fichiers. Si vous essayer de placer des fichiers ou dossiers depuis shared dans la poubelle de la SciViews Box, cela vous sera refusé (voir copie d’écran ci-dessous). Par contre, cela fonctionne très bien depuis l’ordinateur hôte (Windows, MacOS, …). Un tout dernier point concernant les ordinateurs de la salle de T.P. de l’UMONS ainsi que pour ceux du laboratoire informatique du site de Charleroi. Pour des questions de performance, la machine virtuelle SciViews Box, et le dossier shared ne sont pas sur votre compte, mais directement sur le disque de l’ordinateur. Cela signifie qu’ils ne sont pas transportables vers un autre ordinateur. Vous pouvez créer une copie de shared dans mes documents ou sur une clé USB pour les transporter vers un autre ordinateur… mais nous verrons que cela n’est pas nécessaire pour tout ce que vous stockerez sur Github. En effet, vous avez accès à ces contenus depuis n’importe où via n’importe quelle connexion internet. Si jamais vous voulez retourner plus tard au configurateur de la SciViews Box, vous n’aurez qu’à cliquer sur son icône tout en haut à droite dans la barre supérieure.↩ Le Mac définit ses raccourcis claviers différemment du PC. Outre l’inversion de l’utilisation des touches Ctrl et Command, le Mac possède deux touches Alt, une à gauche et une à droite. Le PC a, par contre, deux touches correspondantes, mais celle de droite est nommée Alt Gr. Ces touches jouent des rôles différents: raccourcis claviers pour Alt et accès aux touches de niveau 3 et 4 pour Alt Gr. Pour les utilisateurs Mac, notez que vos deux touches Alt ont des rôles différents dans la SciViews Box comme pour un clavier PC.↩ A la première utilisation de Git à l’intérieur du logiciel RStudio, votre login et votre mot de passe vous seront également redemandés. De même, vous devrez également fournir ces informations dans Github Desktop et la première fois que vous naviguerez vers https://github.com depuis le navigateur Web de votre PC hôte. Mais ensuite, vous accèderez immédiatement au service.↩ Il pourra ainsi être replacé au bon endroit et exécuté dans la Box↩ Le presse-papier est synchronisé entre les deux machines pour le texte qui y est copié.↩ "],
["svbox-use.html", "A.4 Utilisation", " A.4 Utilisation Une fois votre machine virtuelle configurée, vous vous trouvez confronté à cet écran qui montre le fond d’écran et un ensemble d’items par dessus. Nous l’appellerons le bureau de la SciViews Box. Cette machine virtuelle utilise le système d’exploitation Linux. Vous pouvez accèder au application présentes sur cette machine à partir du menu Applications en haut à gauche du bureau. Ce dernier offre un menu déroulant avec l’ensemble des applications disponibles. Ces applications sont rangées en dossier tel que Favorites, Recently Used , All, … Le “dock” en bas du bureau permet de lancer des applications rapidement et d’accéder aux fenêtres des applications en cours d’exécution tel que RStudio, Jupyter, Spyder,… Pour accèder à vos dossiers et fichiers, il suffit de cliquer sur l’icône en forme de dossier avec une image de petite maison que l’on retrouve également dans le dock. "],
["prise.html", "B Prise en main", " B Prise en main Cette annexe comprend une description détaillée des différents outils utilisés dans la cadre de cette formation. Passez à la section suivante pour découvrir les outils. (ex: B.1 RStudio). "],
["rs.html", "B.1 RStudio", " B.1 RStudio Sélectionnez l’icône RStudio dans le dock (cercle bleu avec un R blanc au centre). Un login vers RStudio apparaît. Il faut y entrer les informations suivantes : Username : sv Password : sv Cochez éventuellement Stay signed in pour éviter de devoir réentrer ces informations continuellement : RStudio s’ouvre. C’est votre interface de travail à partir de laquelle vous allez piloter R. La fenêtre principale comporte différents éléments : Une barre de menu et une barre d’outils générale en haut Un panneau à gauche intitulé Console où vous pouvez entrer des instructions dans R pour manipuler vos données Un panneau à droite en haut qui comprend plusieurs onglets, dont Environment qui vous indique les différents items (on parle d’objets) chargés en mémoire dans R (mais pour l’instant, il n’y a encore rien). Un panneau en bas à droite comportant lui aussi plusieurs onglets. Vous devriez voir le contenu de Files au démarrage, un explorateur de fichiers simplifié relatif au contexte de travail actuel dans RStudio. Pour l’instant, aucun document de travail n’est encore ouvert. Pour en créer un, ou ouvrir un document existant, vous utilisez le menu Files, ou encore, le premier bouton de la barre d’outils générale : Le menu Session permet d’interagir directement avec R qui est lancé automatiquement en arrière plan dès que RStudio est ouvert. Par exemple, il est possible de relancer R à partir d’une entrée dans ce menu : Le menu Help propose différentes possibilités pour accéder à la documentation de R ou de RStudio. Les aide-mémoires (“cheatsheets” en anglais) sont très pratiques lors de l’apprentissage. Nous conseillons de les imprimer et de les consulter régulièrement. Le dernier bouton de la barre d’outils générale, intitulé Project permet d’ouvrir, fermer, et gérer les projets RStudio. Vous avez maintenant repéré les éléments fondamentaux de l’interface de RStudio. A ce stade vous pouvez vous familiariser avec l’aide-mémoire relatif à l’IDE RStudio. Vous verrez qu’il y a beaucoup de fonctionnalités accessibles à partir de la fenêtre principale de RStudio. Ne vous laissez pas intimider : vous les apprendrez progressivement au fur et à mesure de l’utilisation du logiciel. B.1.1 Projet dans RStudio Un projet sert, dans RStudio, à organiser son travail. Un projet va regrouper l’ensemble des jeux de données, des rapports, des présentations, des scripts d’une analyse généralement en relation avec une ou plusieurs expériences ou observations réalisés sur le terrain ou en laboratoire. Voici à quoi ressemble l’interface de RStudio lorsque vous ouvrez un projet : Notez que le nom du projet est mentionné en haut à droite. Notez également, que le répertoire de base de votre projet est le répertoire actif dans l’onglet Console (~/shared/projects/mon_premier_projet/ dans l’exemple), et que l’onglet Files affiche son contenu. Un fichier mon_premier_projet.Rproj y est placé automatiquement par RStudio. Ce fichier contient les paramètres de configuration propres à ce projet72. C’est aussi une excellente façon de repérer qu’un répertoire est la base d’un projet RStudio, en repérant ce fameux fichier .Rproj. B.1.1.1 Création d’un projet Créez votre premier projet en suivant les quatre étapes suivantes : Étape 1. Dans RStudio, Sélectionnez le bouton tout à droite dans la barre d’outils générale de RStudio qui ouvre un menu contextuel relatif aux projets. Sélectionnez y l’entrée New Project.... Étape 2. Une boite de dialogue s’ouvre. Sélectionnez New Directory pour créer votre projet dans un nouveau dossier. Il est également possible d’employer un dossier existant comme point de départ Existing Directory). Étape 3. Sélectionnez New Project tout en haut dans l’écran suivant qui vous propose également des projets particuliers (que nous n’utiliserons pas pour l’instant). Étape 4. Ensuite, RStudio vous demander quelques informations pour préconfigurer votre projet. Nommez le projet : Directory name. Indiquez ici project_test Indiquez où vous voulez le placer : Create project as subdirectory of. Sélectionnez le sous-dossier projects dans le dossier shared partagé entre la SciViews Box et la machine hôte. Sélectionnez Create a git repository Désélectionnez Use packrat with this project(il est important de ne pas sélectionner packrat, sous peine de dupliquer de nombreux packages R dans votre projet) Vous utilisez aussi le menu spécial projet pour créer un nouveau projet (New Project...), ouvrir un projet existant (Open Project...) ou encore fermer un projet (Close Project). Vous remarquez également que les derniers projets employés sont placés sous les trois options citées ci-dessus afin d’y accéder plus rapidement. Un projet ne doit bien sûr être créé qu’une seule fois ! Une fois les étapes ci-dessus effectuées, vous retournez simplement à votre projet en ouvrant le menu contextuel projets et en sélectionnant votre projet dans la liste. S’il n’y apparait pas, choisissez Open Project... et sélectionnez le fichier .Rproj relatif à votre projet. Ne créez bien évidemment jamais de projet à l’intérieur des dossiers d’un autre projet, surtout si vous utilisez Git. Sinon, RStudio va s’emméler les pinceaux ! B.1.1.2 Organisation d’un projet Le répertoire projects contient maintenant un projet RStudio intitulé project_test. Depuis la SciViews Box, il se situe dans : /home /sv /shared /projects /project_test # Le répertoire de base du projet project_test.Rproj # Fichier de configuration du projet créé par RStudio .gitignore # Fichier relatif à la gestion de version Vous devez maintenant structurer votre projet afin d’avoir différents sous-répertoires pour organiser au mieux le travail. Ceci concerne à la fois les données et les rapports d’analyse en lien avec ce projet. Cliquez sur le bouton New Folder dans la barre d’outils de l’onglet Files et appelez ce nouveau dossier data. Ajoutez également les dossiers analysis et R. Vous pouvez faire cela depuis RStudio, mais aussi depuis le système hôte si c’est plus confortable pour vous. /home /sv /shared /projects /project_test # Le répertoire de base du projet analysis # Le dossier qui comprend toutes les analyses (rapport, présentation,...) data # Le dossier qui comprend toutes les données project_test.Rproj # Fichier de configuration du projet créé par RStudio .gitignore # Fichier relatif à la gestion de version R # Le dossier qui comprend tous les scripts d&#39;analyse Vous obtenez donc un projet configuré de la manière suivante : L’organisation cohérente d’un projet est indispensable pour le bon fonctionnement et la clarté de vos analyses de données. B.1.1.3 Chemins relatifs dans un projet L’utilisation d’un projet permet de structurer de manière cohérente son travail. Vous allez maintenant devoir rendre votre projet portable. Un projet RStudio pourra être qualifié de portable s’il est possible de déplacer son répertoire de base et tout ce qu’il contient (ou le renommer) sans que les analyses qu’il contient n’en soient affectées. Ceci est utile pour copier, par exemple, le projet d’un PC à un autre, ou si vous décidez de restructurer vos fichiers sur le disque dur. La première règle est de placer tous les fichiers nécessaires dans le dossier du projet ou dans un sous-dossier. C’est ce que nous venons de faire plus haut. La seconde règle est de référencer les différents fichiers au sein du projet avec des chemins relatifs. Nous allons maintenant apprendre à faire cela. /home /sv /shared /projects /project_test # Le répertoire de base du projet analysis # Le dossier qui comprend toutes les analyses (rapport, présentation,...) rapport_test.rmd # Rapport d&#39;analyse data # Le dossier qui comprend toutes les données dataset.csv # jeu de données exemple project_test.Rproj # Fichier de configuration du projet créé par RStudio .gitignore # Fichier relatif à la gestion de version R # Le dossier qui comprend tous les scripts d&#39;analyse Les différents systèmes d’exploitations (Windows, MacOS, Linux) utilisent des conventions différentes pour les chemins d’accès aux fichiers. Dans notre cas, la machine virtuelle utilise un système d’exploitation Linux. La barre oblique (/ dite “slash” en anglais) sépare les différents dossiers imbriqués sous Linux et sous MacOS. Le système d’exploitation Windows utilise pour sa part, la barre oblique inversée (\\, dite “backslash” en anglais, mais dans R et RStudio, vous pourrez également utiliser le slash /, ce que nous vous conseillons de faire toujours pour un maximum de compatibilité entre systèmes). Par exemple, votre fichier dataset.csv se référence comme suit dans la SciViews Box, donc sous Linux : /home/sv/shared/projects/project_test/data/dataset.csv Ce chemin d’accès est le plus détaillé. Il est dit chemin d’accès absolu au fichier. Vous noterez qu’il est totalement dépendant de la structure actuelle des dossiers sur le disque. Si vous renommez project_test ou si vous le déplacez ailleurs, la référence au fichier sera cassée ! Ainsi, si vous partagez votre projet avec un collaborateur qui le place ailleurs sur son disque dur, le chemin d’accès devra être adapté sans quoi l’analyse ne pourra plus s’exécuter correctement. Décodons ce chemin d’accès : /, racine du système /home/sv/, notre dossier personnel comme utilisateur sv /home/sv/shared/, le dossier partagé entre la SciViews Box et notre PC hôte /home/sv/shared/projects/project_test/, le dossier de base de notre projet /home/sv/shared/projects/project_test/data/, le répertoire qui contient le fichier dataset.csv. Le répertoire utilisateur /home/&lt;user&gt; est différent sous MacOS (il s’appelle /Users/&lt;user&gt;) et sous Windows (il se nomme généralement C:\\Users\\&lt;user&gt;). Comme c’est un répertoire clé, et qu’il est impossible d’écrire un chemin absolu qui soit le même partout, il existe un raccourcis : le “tilde” (~) qui signifie “mon répertoire utilisateur”. Ainsi, vous pouvez aussi accéder à votre jeu de données dataset.csv comme ceci : ~/shared/projects/project_test/data/datasets.csv Ce chemin d’accès est déjà plus “portable” d’un système à l’autre et d’un utilisateur à l’autre. Il est donc à préférer. Notez que sous R, vous devez doubler les backslashs sous Windows (~\\\\Documents\\\\...). Ce n’est ni très esthétique, ni compatible avec les deux autres systèmes. Heureusement, R comprend aussi le slash comme séparateur sous Windows, de sorte que la même syntaxe peut être utilisée partout ! Nous vous conseillons donc d’utiliser aussi systématiquement les slashs sous Windows dans R ou RStudio. Si cette façon d’écrire le chemin d’accès est compatible entre les trois systèmes d’exploitation, elle ne permet toujours pas de déplacer ou de renommer notre projet. L’utilisation d’un chemin relatif permet de définir la position d’un fichier par rapport à un autre dossier qui est dit le répertoire actif. A titre d’exemple, nous voulons faire référence au jeu de données dataset.csv depuis notre rapport rapport_test.Rmd. Demandez-vous d’abord quel est le répertoire actif. Pour un fichier R Markdown ou R Notebook, c’est facile, c’est le dossier qui contient ce fichier. Dans la console R, cela peut varier selon le contexte. Si vous avez ouvert un projet, c’est le répertoire de base du projet par défaut, mais cela peut être modifié. Le répertoire actif pour R est toujours indiqué en gris à côté de l’onglet Console dans RStudio. Vous pouvez aussi interroger R à l’aide de l’instruction getwd(): getwd() Vous pouvez réaliser cela dans un chunk R dans votre document R Notebook par exemple : Une fois que vous connaissez le répertoire actif, vous naviguez à partir de celui-ci. Il existe une convention pour reculer d’un dossier dans la hiérarchie : pour cela vous indiquez .. à la place d’un nom de dossier. Voici ce que cela donne : ../data/dataset.csv Comment lit-on ceci? Tout d’abord, notez (c’est très important) que le chemin d’accès ne commence pas par / (Linux ou MacOS), ou C:/ (ou toute autre lettre, sous Windows). C’est le signe que l’on ne part pas de la racine du système de fichier, mais du répertoire actif. Ensuite, les différents éléments se décryptent comme suit : ~/shared/projects/project_test/analysis, répertoire actif au départ pour le document R Notebook .., retour en arrière d’un niveau. On est donc dans ~/shared/projects/project_test /data, naviguer dans le sous-dossier data. On est donc maintenant dans ~/shared/projects/project_test/data. C’est le répertoire qui contient le fichier qui nous intéresse /datasets.csv, le nom du fichier référencé. A noter que si le fichier se trouve déjà dans le répertoire actif, le chemin relatif se résume au nom du fichier directement ! Nulle part dans ce chemin relatif n’apparaît le nom du répertoire de projet, ni d’aucun autre répertoire parent. Ainsi, il est possible de renommer ou déplacer le projet sans casser la référence relative à n’importe quel fichier à l’intérieur de ce projet. Donc, en utilisant uniquement des références relatives, le projet reste parfaitement portable. B.1.2 Scripts R dans RStudio Un script R est une suite d’instructions qui peuvent être interprétées pour effectuer nos analyses. Ce script est stocké dans un fichier dont l’extension est .R, et que l’on placera de préférence dans le sous-dossier R de notre projet. Un script R s’ouvre dans la fenêtre d’édition de RStudio. Les parties de texte précédées d’un dièse (#) sont des commentaires. Ils ne sont jamais exécutés, mais ils permettent de structurer et d’expliquer le contenu du document (ou bien d’empêcher temporairement l’exécution d’instructions). Afin de bien documenter vos scripts, Commencez-les toujours par quelques lignes de commentaires qui contiennent un titre, le nom du ou des auteurs, la date, un copyright éventuel, … L’utilisation de sections comme à la ligne 6 ci-dessus est vivement conseillée. Ces sections sont créée à l’aide de l’entrée de menu Code -&gt; Insert Section... dans RStudio. Elles sont reprises dans le bas de la fenêtre édition pour une navigation rapide dans le script. B.1.2.1 Création d’un script R Vous avez à votre disposition plusieurs méthodes pour ouvrir un nouveau script R dans RStudio, dont deux vous sont montrées dans l’animation ci-dessous. B.1.2.2 Utilisation d’un script R Un script R est un document natif de R. Ce dernier va interpréter les intructions qui compose le script et qui ne sont pas précédées d’un dièse (cliquez sur Run dans la barre d’outils de la fenêtre d’édition, ou utilisez le raccourci clavier Ctrl+Enter ou Cmd+Enter sur MacOS pour exécuter des instructions). Un script R doit être organisé de manière cohérente afin d’être exécutable de haut en bas. Dans l’exemple ci-dessus, on commence par : Étape 1. Importer les principaux outils avec l’instruction SciViews::R. Étape 2. Utiliser l’instruction urchin &lt;- read(&quot;urchin_bio&quot;, package = &quot;data.io&quot;) pour importer le jeu de données urchin_bio provenant du package data.io et l’assigner à urchin. On retrouve à présent urchin dans l’environnement global (Global environment dans l’onglet Environnement dans le fenêtre en haut à droite) de RStudio. Étape 3. .?urchin et View(urchin) donnent des renseignements sur le jeu de données en renvoyant vers la page d’aide du jeu de données et en ouvrant ce jeu de données dans une fenêtre de visualisation. Étape 4. Réaliser des graphiques avec la fonction chart(). Notez que les instructions exécutées dans le script sont envoyées dans la fenêtre Console en bas à gauche. B.1.3 R Markdown/R Notebook Un document R Markdown est un fichier dont l’extension est .Rmd. Il combine à la fois des instructions R (pour les analyses) et le langage Markdown (pour le texte). Le R Markdown ne vous permet pas de visualiser directement le résultat final d’un rapport d’analyse73 Tout comme dans un script R, les intructions doivent être également exécutées lors de la réalisation du rapport. Une forme spéciale de document R Markdown est le R Notebook. Ce dernier est un peu un intermédiaire entre un script R et un R Markdown. Il se présente de manière très similaire à ce dernier, mais vous pouvez également exécuter le code qu’il contient ligne par ligne comme dans un script. Un document R Markdown / R Notebook se structure de la manière suivante : Un préambule Des zones d’édition Le language employé est le Markdown Des zones de code R Ces zones de codes sont appelées des chunks Le préambule est nécessairement situé au tout début du document et est balisé à l’aide de trois tirets --- sans rien d’autre sur une ligne au début et à la fin. Le préambule comporte un ensemble d’entrées de type nom: valeur qui configurent le document ou la façon dont il sera compilé en rapport final. Nous pouvons y indiquer le titre principal, le ou les auteurs, la date, … Le reste du document R Markdown est subdivisé en zones successives et contrastés sur des fonds de couleurs différentes dans RStudio. On y retrouve : Les zones de texte des parties Markdown où vous pouvez écrire votre prose. Les chunks contennant du code, des instructions qui seront interprétées pour réaliser un calcul, un graphique, un tableau, etc. Le résultat de ce traitement sera placé à cet endroit dans le rapport final. Ces chunks sont balisés en entrée par trois apostrophes inverses suivies d’accolades contenant des instructions relatives au programme à utiliser, par exemple, ```{r} pour des chunks faisant appel au logiciel R, et sont terminés par trois apostrophes inverses (```). Dans les zones Markdown, vous pouvez ajouter des balises qui permettront de formater votre texte dans la version finale de votre rapport. Par exemple, un ou plusieurs dièses (plus communément connu par sont appellation en anglais : “hastag”) en début de ligne suivi d’un espace indique que la suite correspond à un titre. Titre de niveau 1 avec un seul dièse, de niveau 2 avec deux dièses, et ainsi de suite jusqu’à 6 niveaux possibles. Dans la capture d’écran ci-dessous, nous avons remplacé tout le contenu par défaut d’un R Notebook (à part le préambule) par une série de titres de niveau 1 correspondant à la structure générale d’un rapport scientifique : Introduction Objectif Matériel et méthodes Résultats Discussion Conclusions B.1.3.1 Création d’un R Markdown/Notebook Vous avez à votre disposition deux méthodes pour ouvrir un nouveau R Notebook dans RStudio. Voyez l’animation ci-dessous. B.1.3.2 Utilisation d’un R Markdown/Notebook Afin de visualiser les résultats des chunks dans votre rapport final, vous devez veiller à exécuter chaque chunks dans l’ordre dans un R Notebook. Ceci n’est pas nécessaire dans un R Markdown, mais dans ce cas, tous les chunks sont systématiquement recompilés à chaque génération de rapport, ce qui peut être pénible si les calculs sont longs. Pour exécuter un chunk, vous pouvez : cliquer sur le bouton “play”, sous forme d’une flèche verte pointant vers la droite, situé en haut à droite du chunk cliquer sur Run et sélectionner Run Current Chunk dans le menu déroulant qui apparait Employer le raccourci clavier Ctrl+Shift+Enter Le bouton Run propose plusieurs actions intéressantes : Exécuter la/les ligne(s) d’instruction sélectionnée(s) : Run Selected Line(s) Exécuter le chunk en entier : Run Current Chunk Exécuter tous les chunk précédents : Run All Chunk Above Redémarer la console R et exécuter tous les chunks: Restart R and Run All Chunks. Cette action est particulière intéressante pour s’assurer que le document est réellement reproductible ! … Aprés la phase d’édition du texte (et des intructions dans les chunks pour un document R Notebook), vous pouvez visualiser votre rapport final en cliquant sur le bouton Preview (Notebook) ou Knit (Markdown). Le rapport est rapidement généré avec un rendu simple et professionnel. Par défaut, ce rapport présente le texte que vous avez écrit, avec les résultats que vous avez choisi de générer via R, mais également les instructions que vous avez employées pour obtenir ces résultats. Ceci permet de mieux comprendre, directement dans le rapport, comment tout cela a été calculé. Il est possible de cacher le code (dans un document généré depuis un Notebook R), ou d’indiquer une directive de compilation dans les chunks pour éviter que le code ne s’imprime dans le rapport final. Voyez les options en cliquant sur le petit engrenage à côté de la flèche verte en haut à droite du chunk. Consultez l’aide-mémoire de R Markdown accessible à partir du menu RStudio Help -&gt; Cheatsheets -&gt; R Markdown Reference Guide, voir chunk options p.2-3 pour plus d’informations sur les nombreuses options disponibles. Par exemple, en ajoutant la directive echo=FALSE dans la balise d’entrée d’un chunk (```{r, echo=FALSE}), on empèche d’imprimer le code de ce chunk dans le rapport. Notez que sur la droite du bouton Preview ou Knit, vous avez un autre bouton représenté par un petit engrenage. Il donne accès à un menu déroulant qui vous donne la possibilité de modifier la façon de générer vos rapports. L’entrée tout en bas Output Options... permet de paramétrer la présentation du rapport. Si vous cliquez sur la petite flèche noire pointant vers le bas juste après Preview ou Knit, vous avez un autre menu déroulant qui donne accès aux différents formats possibles : HTML, PDF, Word, etc. Essayez les différentes options pour visualiser comment votre rapport se présente dans les différents formats. N’éditer jamais à la main un fichier .Rproj. Laisser RStudio s’en occuper tout seul.↩ Les systèmes d’édition professionnels dissocient en effet le fond de la forme : vous rédiger d’abord le contenu, et ensuite, vous indiquer le style à lui appliquer.↩ "],
["github-annexe.html", "B.2 GitHub", " B.2 GitHub Un réseau social a été conçu autour de Git pour sauvegarder vos projets sur le “cloud”, les partager et collaborer avec d’autres personnes. Ce système se nomme GitHub (tout comme Facebook ou LinkedIn). GitHub rassemble donc “Git”, la gestion de version et “Hub” relatif au réseau. D’autres réseaux équivalents existent comme Gitlab ou Bitbucket. B.2.1 Votre activité et profil Pour vous montrer différentes sections sur GitHub, nous utiliserons le compte de GuyliannEngels. Une fois connecter sur Github, nous nous trouvons sur une page qui nous montre notre activité sur ce réseau. A droite de la page, nous pouvons observer les derniers dépôts (On parle de Repositories dans GitHub) sur lequels vous avez travaillé et au centre votre activité récente. Nous pouvons également visiter notre profil. A droite, il y a une photo et une petite présentation vous concernant tout comme vous le retrouvez sur tout réseau social, au centre vos projets récents (Popular repositories) et vos contributions générale sur ce réseau social. Les contributions sur le réseau est très important. Il indique de manière globale votre travail ou plutot votre apport sur vos différents projets. Dans notre exemple, nous pouvons observer 983 contributions sur l’année écoulée. B.2.2 Vos projets Sur GitHub, vous pouvez héberger vos projets (qui se nomment sur GitHub repositories). Notre exemple se base sur le projet sdd-umons, que vous pouvez librement consulter. Il s’agit en effet d’un dépôt public. Vous avez la possibilité d’avoir des projets publics ou privés. Les projets publics sont visibles par tous. La collaboration est le point central de GitHub. Un dépôt public peut être vu par tous. Il peuvent y apporter des modifications et puis vous soumettre les modifications. Nous reviendrons plus tard sur ces apports par la communauté. Pour des projets plus sensibles, vous avez la possiblité d’avoir des projets privés. Pour collaborer sur des projets privées vous serez amené à prendre un compte payant. Dans le cadre de notre cours, nous resterons toujours sur un compte gratuit. Un dépôt sur GitHub est proposé de la manière suivante. Vous pouvez observer le nom du dépot et la personne ou l’organisation qui travaille sur ce projet (BioDataScience-Course/sdd-umons). Dans notre cas, tous les projets relatifs au cours de sciences des données biologiques sont hébergés sur BioDataScience-Course(Il en sera de même pour tous les travaux que vous réaliserez dans le cadre des cours). Sous le nom du dépôt, vous pouvez observer, le dépots sur lequel ce projet s’inspire (phgrosjean/bookdown-test). Vous pouvez observer une première barre d’outils comprenant les sections Code, Issues, Pull requests, Projects, Security, Insights et Settings (toutes les sections ne seront pas détaillées dans cet ouvrage). B.2.2.1 Code Dans cette section, vous pouvez observer une nouvelle barre d’outils qui comprend les sections suivantes : commits, branches, releases, environment, contributors et View license. Nous pouvons observer pas moins de 270 commits (Rappelez vous qu’un état d’avancement d’un projet est un commit). B.2.2.2 Issues Cette section est prévue afin de discuter, de réfléchir et de collaborer sur un projet commun. B.2.2.3 Insights La section Insights nous renseigne sur l’activité de notre projet. On peut y voir par exemple les contributeurs (Contributors) du projet. Nous avons donc 4 personnes qui ont contribué sur cet ouvrage. Les informations fournies dans cette section sont employées dans le cadre des corrections des travaux de groupes. B.2.3 Débuter un dépôt Lorsque nous souhaitons débuter un nouveau projet qui sera déposé sur Github, nous devons l’initialiser sur GitHub. Pour créer un nouveau dépôt (Create a new repository), nous devons fournir les informations suivantes : Repository template Nous devons décider d’utiliser ou non un template existant parmi la liste des templates que nous avons. Owner Nous devons decider du responsable du dépôt soit une organisation ou un responsable. Repository name Nous devons définir un nom pertinent pour notre projet. Description Nous pouvons proposer une courte description de notre dépôt. Public ou Private Nous devons décider si notre projet est public ou privé. README Nous pouvons éditer un fichier de présentation qui se nomme le README. Ce dernier est un fichier qui va présenter succinctement notre projet. On peut l’éditer depuis GitHub directement. .gitignore Il est intéressant de configurer le dépôt avec un fichier .gitignore orienté sur l’utilisation de R. GitHub peut en effet héberger des projets avec des languages très variés. license Nous pouvons adjoindre à notre projet une licence. Il en existe plusieurs afin d’expliquer précisement ce que l’on a le droit de faire ou non avec votre dépôt. Le site https://choosealicense.com peut vous aider à définir votre licence. Une fois votre dépôt configuré, il ne vous reste plus qu’à le cloner comme expliqué dans la section B.2.4. B.2.4 Cloner un dépot existant Lorsque nous souhaitons travailler sur un de nos projets, il faut commencer par le cloner pour avoir une copie en local de ce dernier. B.2.4.1 Via RStudio Pour commencer, vous devez copier le lien menant à votre dépôt sur GitHub. Il vous suffit de cliquer sur Clone or downloadet de copier l’url proposée (vous avez d’ailleurs un bouton à cet effet). Ensuite, vous devez vous rendre dans RStudio et sélectionner l’onglet Project, suivi de New Project... (Si les projets restent encore un peu flous pour vous, rendez vous dans la section @ref(#rs-projet)). Une nouvelle fenêtre s’ouvre . Vous devez sélectionner Version Control, puis Git. Pour finir, vous devez renseigner l’url précédemment copiée depuis GitHub, chosir un nom à votre dépôt (Laissez le nom par défaut est une bonne pratique), choisir un dossier pour cloner votre dépot (le dossier projects, du dossier shared est dédié à cela) et créer une copie en local de votre projet en cliquant sur Create Project. Vous êtes enfin prêt à éditer votre projet. N’oubliez pas de réaliser des Commit, des Pull et des Push. B.2.4.2 Via GitHub Desktop Github Desktop est un programme mis au point afin de faciliter l’utilisation de GitHub. Cloner un dépôt avec GitHub desktop est d’une simplicité déconcertante. Depuis Github, sélectionnez Clone or download puis Open in Desktop et enfin acceptez l’ouverture de Github deskop. Dans GitHub Desktop, une fenêtre de configuration s’ouvre, il suffit de sélectionner le dossier où l’on souhaite placer le dépôt et cloner le dépôt. Cette fonction n&#39;est disponible que sur la version Mac et Windows de GitHub Desktop B.2.5 Déposer un projet débuté Nous avons débuté un projet sur RStudio configuré avec le gestionnaire de version comme présenté dans l’annexe B.1.1.1. Cependant, après avoir progressé dans ce projet (et réalisé plusieurs Commit), vous souhaitez le partager sur GitHub. Rassurez-vous, il ne faut pas tout recommencer. Il aurait cependant été plus simple de réflechir dès le début du projet à cette éventuallité, néanmoins voici une solution à ce problème. Une bonne pratique avant de vous lancer dans un nouveau projet et de se poser et de réfléchir aux objectifs du projets et aux moyens à mettre en oeuvre pour atteindre ces objectifs. B.2.5.1 Via RStudio Nous partons d’un projet RStudio d’exemple qui se nomme repos-example. Comme vous pouvez le voir, ce projet comprend 3 Commit mais nous ne pouvons ni faire des Pull et des Push. C’est tout à fait normal vu que nous ne sommes pas lié avec GitHub. Pour déposer un projet RStudio existant sur GitHub, vous devez débuter par créer un nouveau dépôt dans Github qui ressemble très fortement à l’annexe B.2.3. Avec une particularité que vous ne devez pas configuré le README, le .gitignore et la license. Vous pouvez vous appercevoir que le dépot est vide et qu’il vous est proposé différentes solutions dont …or push an existing repository from the command line . Il s’agit donc de mettre en ligne (Push) un projet exsitant. Dans votre projet RStudio, sélectionnez l’onglet Tools puis Shell.... Un onglet terminal vient de s’ouvrir à côte de l’onglet de la console. Il vous suffit ensuite d’y copier les 3 instructions proposer sur GitHub et de taper sur la touche enter et le tour est joué Afin de vérifier que votre projet RStudio est correctement mis en ligne sur GitHub, vous pouvez recharger votre page sur GitHub. B.2.5.2 Via GitHub Desktop Nous partons d’un projet RStudio d’exemple qui se nomme repos-example2 similaire au projet présenté précédement. Dans GitHub Desktop, vous devez sélectionner File, puis Add Local Repository.... Vous devez commencer par sélectionner le dossier de votre projet, puis de publier votre dépôt (publish repository). C’est à nouveau un programme très simple d’utilisation. B.2.6 Copier un dépôt Lorsque que vous souhaitez apporter votre aide sur un projet qui n’est pas le votre. Vous devez réaliser un Fork de ce travail puis un Clone. N’oubliez pas que la base de GitHub est de faciliter la collaboration. Afin de soumettre vos modifications à un projet. Il faut réaliser un Pull requests. Cette étape permet de proposer vos modifications au responsable du projet. Ce dernier pourra accepter ou refuser vos modifications. Il s’agit souvent d’un espace de discussions entre le responsable et le collaborateur volontaire. "],
["github-classroom.html", "B.3 GitHub Classroom", " B.3 GitHub Classroom GitHub Classroom est une extension de GitHub qui facilite le travail avec GitHub dans le contexte d’exercices à réaliser dans le cadre d’un cours. Vous serrez amené à cloner et modifier des dépôts issus de GitHub Classroom pour réaliser vos exercices. Nous avons fait le choix de configurer vos dépôts afin qu’il soit privé. En effet, vos travaux ne sont visibles que par vous et vos professeurs. Vous rencontrerez tout au long de cet ouvrage des fenêtres d’exercice comme montré ci-dessous. Il vous suffit de cliquer sur le lien mis à votre disposition. Ce lien vous redirige vers le site GitHub Classroom. Il vous suffit à nouveau uniquement d’accepter l’assignation (Accept assigment). Votre travail individuel ou par groupe se configure avant de vous proposer de retourner sur GitHub afin de voir votre dépôt en utilisant le lien proposé. Depuis GitHub, vous devez cloner votre dépôt pour y avoir accès en local comme expliqué dans la section B.2.4. Vous êtes enfin prêt à éditer votre projet. N’oubliez pas de réaliser des Commit, des Pull et des Push. "],
["learnr.html", "C Tutoriels “learnr”", " C Tutoriels “learnr” En complément de ce syllabus, vous allez utiliser également des tutoriels interactifs construits avec learnr. Si ce n’est déjà fait, commencez par installer ces tutoriels dans votre SciViews Box (voir A.3.3). Entrez l’instruction ci-dessous dans la fenêtre Console de RStudio suivie de la touche Entrée : BioDataScience::run() La liste des tutoriels vous est proposée (notez que ces tutoriels comme les autres outils pédagogiques sont encore en cours de développement, mais plusieurs vous seront disponibles): 1: 02a_base 2: 02b_decouverte 3: 02c_nuage_de_points 4: 02d_test ... Entrez le numéro correspondant au tutoriel que vous voulez exécuter et un document interactif apparait. Par exemple, en entrant 1 suivi de la touche Entrée, vous êtes redirigé vers le tutoriel concernant les bases de R et intitulé 02a_base. La première chose à vérifier à l’ouverture du tutoriel interactif est le nom d’utilisateur (équivalent à votre username dans Github) et votre adresse email (adresse email associée à votre compte Github ou de votre compte d’étudiant). En effet, votre progression sera enregistrée, mais cela ne peut se faire que si vous renseignez ces données correctement avant de travailler dans le tutoriel “learnr”. Le learnr est un outil pédagogique mis au point afin de proposer des tutoriaux interactifs comprennant des illustrations, des questions à choix multiples, des exercices R, … Les learnr qui vous seront proposés tout au long de votre formation seront composés de la manière suivante : Objectif Introduction Une série d’exercices Conclusion Vous retrouvez d’ailleurs cette structure en haut à gauche de ce dernier. Chaque page du tutoriel est importante et nécessite votre attention. Objectifs Cette section détaillera l’ensemble des notions que vous allez apprendre à maitriser durant ce tutoriel. Dans le cadre de ce premier tutoriel, l’objectif est de découvrir les bases du language R. Introduction Cette section vous replacera dans le contexte du tutoriel interactif avec un rappel succinct des notions théoriques indispensables afin de répondre à la série d’exercices. Cette section ne remplace pas les autres matériels pédagogiques qui vous sont proposés. Vous devez donc travailler dans l’ordre proposé dans le présent manuel au sein de chaque module pour vous préparer correctement au learnr de test en fin de section. Une serie d’exercices Cette section peut être de longueur très variable en fonction de la difficulté et des notions à appréhender. Des zones de codes R vous sont proposées dans les exercices. Elles vous permettent d’expérimenter directement des instructions dans R depuis le document learnr. Pour exécuter ces instructions, il faut cliquer sur Run Code. Vous pouvez le faire autant de fois que vous le voulez. Modifiez le code, cliquez Run Code, analysez le résultat, modifiez votre code, recliquez Run Code, etc… jusqu’à ce que vous soyez satisfait du résultat. Finissez l’exercice et soumettez votre réponse en cliquant sur Submit Answer. Des boutons Hint, lorsqu’ils sont présents, vous proposent des aides si vous êtes bloqués. Les boutons Solution… montrent ce qu’il fallait entrer. N’allez pas voir directement la solution. Essayez d’abord par vous même ! Conclusion Cette section termine ce tutoriel et propose de laisser des commentaires avec l’utilisation de dièse #. Fermez le tutoriel lorsqu’il est terminé. En retournant dans RStudio, vous devez déconnecter votre process R qui est toujours occupé à servir la page learnr du tutoriel. Pour cela, placez le curseur dans la fenêtre Console et cliquez sur la touche Esc. A ce moment, vous récupérez la main dans R et pouvez à nouveau travailler normalement dans RStudio. "],
["redaction-scientifique.html", "D Rédaction scientifique", " D Rédaction scientifique La rédaction de textes scientifiques doit respecter un certain caneva et différentes règles qui sont résumés dans cette annexe. Pour en savoir plus Recherche documentaire et aide à la création (ReDAC). L’Université de Mons met à disposition de ses étudiants un cours en ligne qui rassemble un maximum de renseignements sur la rédaction de rapports scientifiques. "],
["organisation.html", "D.1 Organisation", " D.1 Organisation Un rapport scientifique respecte généralement le schéma suivant : Table des matières (facultatif) Introduction But Matériel et méthodes Résultats Discussion Conclusion Bibliographie Annexe(s) (si nécessaire) Pour des travaux de plus grande ampleur comme les travaux de fin d’études, le schéma ci-dessus est adapté, et éventuellement divisé en chapitres, en y ajoutant généralement une partie remerciement en début de manuscrit, ainsi qu’une liste des figures, des tables, des abbréviations utilisées, voire un index en fin d’ouvrage. "],
["contenu.html", "D.2 Contenu", " D.2 Contenu Le rapport sert à restituer de façon synthétique les résultats d’une étude scientifique, et les interprétations. Le tout est remis dans le contexte de la bibliographie existante en la synthétisant dans l’introduction et en comparant les résultats avec d’autres études connexes dans la discussion. Il faut garder à l’esprit qu’un lecteur doit comprendre l’intégralité du rapport avec un minimum de connaissances a priori sur l’étude réalisée, mais avec des connaissances générales dans la spécialité. Donc, un rapport sur un sujet biologique est adressé à un lecteur biologiste pour lequel il ne faut pas rappeler les concepts de base dans sa discipline. Par contre, il faut expliquer avec suffisamment de détails comment l’étude a été réalisée dans la section “matériel et méthodes”. En général, les phrases sont simples, directes, courtes et précises (veuillez à utiliser le vocabulaire adéquat et précis). Les explications sont, autant que possible, linéaires. Evitez les renvois dans différentes autres parties du rapport, si ce n’est pour rappeler un élément évoqué plus haut, ou pour se référer à une figure ou une table. A ce sujet, les figures (dont les images, photos, schémas et graphiques) sont numérotées (Figure 1, Figure 2, …) et accompagnées d’une légende en dessous d’elles. La figure et sa légende doivent être compréhensibles telles quelles. Dans le texte, vous pourrez alors vous référer à la figure, par exemple: “Tel phénomène est observable (voir Fig. 3)”, ou “La Fig. 4 montre …”. idem pour les tableaux qui sont également numérotés (Tableau 1, Tableau 2, …) et légendés, mais au dessus du tableau. Les règles de lisibilité du tableau + légende et de renvoi vers les tableaux sont identiques que pour les figures. Les équations peuvent aussi être numérotées et des renvois de type (eq. 5) peuvent être alors utilisés. Enfin, toute affirmation doit être soit démontrée dans le rapport, soit complétée d’une citation vers un autre document scientifique qui la démontre. La partie bibliographie regroupe la liste de tous les documents qui sont ainsi cités à la fin du rapport. Veuillez à respecter les notations propres au système métrique international, les abbrévations usuelles dans la discipline, et le droit d’auteur et les licenses si vous voulez citer un passage ou reprendre une illustration provenant d’un autre auteur (sans omettre d’indiquer qui en est l’auteur). Enfin, en vue de rendre le document parfaitement reproductible, vous pouvez indiquer dans les annexes où trouver la source (le document .Rmd) et les données analysées. Vous pouvez également terminer avec un chunk qui renseigne de l’état du système R utilisé, y compris l’ensemble des packages employés. Ce chunk, présenté en annexe, contiendra l’instruction utils::sessionInfo(), ou mieux : xfun::session_info() (version courte) ou devtools::session_info() (version longue). Par exemple : xfun::session_info() R version 3.5.3 (2019-03-11) Platform: x86_64-pc-linux-gnu (64-bit) Running under: Ubuntu 18.04.2 LTS Locale: LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C LC_TIME=en_US.UTF-8 LC_COLLATE=en_US.UTF-8 LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 LC_PAPER=en_US.UTF-8 LC_NAME=C LC_ADDRESS=C LC_TELEPHONE=C LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C Package version: acepack_1.4.1 anytime_0.3.3 askpass_1.1 assertthat_0.2.1 backports_1.1.3 base64enc_0.1-3 BH_1.69.0.1 bookdown_0.9 broom_0.5.2 callr_3.2.0 cellranger_1.1.0 chart_1.3.0 checkmate_1.9.1 cli_1.1.0 clipr_0.5.0 cluster_2.0.8 codetools_0.2-16 colorspace_1.4-1 compiler_3.5.3 cowplot_0.9.4 crayon_1.3.4 curl_3.3 data.io_1.2.2 data.table_1.12.2 datasets_3.5.3 DBI_1.0.0 dbplyr_1.3.0 digest_0.6.18 dplyr_0.8.0.1 ellipse_0.4.1 ellipsis_0.1.0 evaluate_0.13 fansi_0.4.0 farver_1.1.0 flow_1.0.0 forcats_0.4.0 foreign_0.8-71 Formula_1.2-3 fs_1.2.7 generics_0.0.2 gganimate_1.0.3 ggplot2_3.1.1 ggplotify_0.0.3 ggpubr_0.2 ggrepel_0.8.0 ggsci_2.9 ggsignif_0.5.0 glue_1.3.1 graphics_3.5.3 grDevices_3.5.3 grid_3.5.3 gridExtra_2.3 gridGraphics_0.3-0 gtable_0.3.0 haven_2.1.0 highr_0.8 Hmisc_4.2-0 hms_0.4.2 htmlTable_1.13.1 htmltools_0.3.6 htmlwidgets_1.3 httr_1.4.0 igraph_1.2.4 inline_0.3.15 jsonlite_1.6 knitr_1.22 labeling_0.3 lattice_0.20-38 latticeExtra_0.6-28 lazyeval_0.2.2 lubridate_1.7.4 magick_2.0 magrittr_1.5 markdown_0.9 MASS_7.3-51.3 Matrix_1.2-17 methods_3.5.3 mgcv_1.8.28 mime_0.6 modelr_0.1.4 munsell_0.5.0 nlme_3.1-138 nnet_7.3-12 nycflights13_1.0.0 openssl_1.3 pillar_1.3.1 pkgconfig_2.0.2 plogr_0.2.0 plyr_1.8.4 polynom_1.4.0 prettyunits_1.0.2 processx_3.3.0 progress_1.2.0 proto_1.0.0 pryr_0.1.4 ps_1.3.0 purrr_0.3.2 R6_2.4.0 RApiDatetime_0.0.4 RColorBrewer_1.1-2 Rcpp_1.0.1 readr_1.3.1 readxl_1.3.1 rematch_1.0.1 reprex_0.2.1 reshape2_1.4.3 rlang_0.3.4 rmarkdown_1.12 rpart_4.1-13 rstudioapi_0.10 rvcheck_0.1.3 rvest_0.3.2 scales_1.0.0 SciViews_1.1.0 selectr_0.4.1 splines_3.5.3 stats_3.5.3 stringi_1.4.3 stringr_1.4.0 survival_2.44-1.1 svMisc_1.1.0 sys_3.1 tibble_2.1.1 tidyr_0.8.3 tidyselect_0.2.5 tidyverse_1.2.1 tinytex_0.11 tools_3.5.3 tsibble_0.7.0 tweenr_1.0.1 utf8_1.1.4 utils_3.5.3 viridis_0.5.1 viridisLite_0.3.0 whisker_0.3.2 withr_2.1.2 xfun_0.6 xml2_1.2.0 yaml_2.2.0 D.2.1 Table des matières La table des matières est d’une importance capitale pour un long document (mais facultative pour un plus court rapport) afin de présenter la structure de votre oeuvre aux lecteurs. Heureusement, il n’est pas nécessaire de l’écrire manuellement. La table des matières est générée automatiquement dans un rapport R Markdown. L’instruction à ajouter dans le préambule du document R Notebook afin d’obtenir une table des matières est toc: yes (ne l’encodez pas directement, mais sélectionnez l’option Include table of contents dans les options de formattage du document accessibles à partir du bouton engrenage à droite de Preview ou Knit -&gt; Output Options...). Lorsque vous fermerez cette boite de dialogue de configuration, l’entrée ad hoc sera ajoutée pour vous dans le préambule. Vous pouvez aussi choisir de numéroter vos titres automatiquement. L’instruction à ajouter en plus de toc: yes dans le préambule du document R Notebook afin d’obtenir une table des matières avec des titres numéroté est number_sections: yes. Encore une fois, passez par la boite de dialogue de configuration, et cochez-y l’entrée Number section headings. Voyez l’animation ci-dessous pour accéder à la boite de dialogue de configuration du document R Markdown/R Notebook. D.2.2 Introduction L’introduction d’un rapport (ou d’un mémoire) a pour principal objectif de replacer l’étude scientifique réalisée dans son contexte. La règle la plus importante est qu’un lecteur n’ayant jamais entendu parler de cette étude doit comprendre l’intégralité du rapport. L’introduction doit donc permettre de : Remettre l’expérience dans son contexte, Décrire l’organisme étudié + caractéristiques générales de l’organisme, distribution géographique, biotope,… Notez que l’ajout d’images ou d’une carte de distribution est un plus dans l’introduction. D.2.3 But Le but permet de synthétiser la question posée dans cette étude en fonction du contexte de l’expérience expliqué dans l’introduction. D.2.4 Matériel &amp; méthodes La section matériel &amp; méthodes permet de décrire les aspects techniques de l’étude comme le matériel employé et les méthodes mises en oeuvre (protocoles des manipulations et des mesures effectuées) afin d’acquérir les données. Cette section est également le lieu de description des techniques statistiques utilisées pour analyser les données, des programmes informatiques employés, … D.2.5 Résultats Les résultats vont généralement contenir deux parties : La description des données, via l’exploration des données récoltées (avec graphiques et/ou estimateurs statistiques) L’application des outils statistiques pertinents pour répondre à la question posée D.2.6 Discussion Cette section comprend l’interprétation biologique des résultats et la remise dans un contexte plus général, notamment en les comparant à des observations connexes réalisées par d’autres auteurs scientifiques. Il est d’une importance capitale d’avoir un regard critique sur les résultats obtenus. Cette mise en contexte aide en ce sens. D.2.7 Conclusion(s) Cette section va résumer les principales implications à retenir de notre étude et, éventuellement, proposer des perspectives afin de poursuivre la recherche dans cette thématique. D.2.8 Bibliographie (ou références) La rédaction de travaux s’appuye toujours sur une recherche bibliographique au préalable. CIl faut documenter convenablement les sources bibliographiques au sein de cette section afin d’éviter le plagiat volontaire ou involontaire. Une multitude de programmes existent pour faciliter la gestion de votre base de données bibliographique comme Mendeley, Zotero ou encore Endnote. Pour générer correctement ses références bibliographiques dans un document R Markdown/R Notebook, consulter ceci. Il s’agit d’un manuel en anglais de RStudio qui explique comment faire dans le détail. "],
["nom-des-especes.html", "D.3 Nom des espèces", " D.3 Nom des espèces Le nom complet d’une espèce en biologie suit une convention particulière, propre à la nomenclature binomiale de Linné que vous devez utiliser dans tous vos travaux. Partons de l’exemple de l’oursin violet. Il s’agit ici du nom vernaculaire en français. Mais ce nom n’est pas assez précis pour être utilisé seul dans un travail scientifique. En effet, le nom vernaculaire d’une espèce change d’une langue à l’autre. Il peut aussi varier d’une région géographique à l’autre, ou pire, il peut désigner des espèces différentes selon les endroits. Seul le nom latin fait référence ! Une espèce est classée de la manière suivante (les niveaux de classification les plus importants sont mis en gras) : Règne : Animalia Embranchement : Echinodermata Sous-Embranchement : Echinozoa Classe : Echinoidea Sous-classe : Euechinoidea Super-ordre : Echinacea Ordre : Camarodonta Infra-ordre : Echinidae Famille : Parachinidae Genre : Paracentrotus Espèce : lividus Afin de former le nom binomial de l’oursin violet, on utilise le genre et l’espèce de la classification ci-dessus : Paracentrotus lividus En toute rigueur, il faut aussi associer le nom du naturaliste qui a nommé et décrit l’espèce et l’année de la publication de la description (on parle de diagnose en biologie), et ce, uniquement la première fois qu’on cite cette espèce dans notre rapport. Paracentrotus lividus Lamarck 1816 Lors de la première citation d’une espèce, et certainement dans le titre ou le résumé, il est indispensable de spécifier le nom latin complet de l’espèce (genre espèce) qui pourra être éventuellement abbrégé par la suite en indiquant la première lettre du genre. Dans l’exemple cité, on pourra écrire ensuite P. lividus plus loin dans le texte (pour autant que cela ne prête pas à confusion, bien sûr). "],
["references.html", "Références", " Références "]
]
